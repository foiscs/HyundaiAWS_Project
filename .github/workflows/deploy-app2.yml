# .github/workflows/deploy-app2.yml
# Java Spring Boot 애플리케이션 배포 워크플로우 (server2 폴더 변경시만 실행)

name: Deploy Java Spring Boot Application

on:
  push:
    branches: [ main ]
    paths:
      - 'WALB/server2/**'
      - '.github/workflows/deploy-app2.yml'

  
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    # 워킹 디렉토리를 WALB로 설정
    defaults:
      run:
        working-directory: ./WALB
    
    permissions:
      id-token: write
      contents: read
    
    steps:
    # ===============================================
    # 소스코드 체크아웃
    # ===============================================
    - name: Checkout code
      uses: actions/checkout@v4
    
    # ===============================================
    # app2-config.yml에서 설정값 로드
    # ===============================================
    - name: Load config from app2-config.yml
      run: |
        echo "📋 app2-config.yml에서 설정값 로드 중..."
        
        # yq 설치
        sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
        sudo chmod +x /usr/local/bin/yq
        
        # app2-config.yml에서 값 읽기
        PROJECT_NAME=$(yq eval '.app2_config.project_name' ../.github/workflows/config/app2-config.yml)
        DB_NAME=$(yq eval '.app2_config.database.name' ../.github/workflows/config/app2-config.yml)
        DB_USER=$(yq eval '.app2_config.database.user' ../.github/workflows/config/app2-config.yml)
        DB_PORT=$(yq eval '.app2_config.database.port' ../.github/workflows/config/app2-config.yml)
        DB_RDS_IDENTIFIER=$(yq eval '.app2_config.database.rds_identifier' ../.github/workflows/config/app2-config.yml)
        EKS_CLUSTER_NAME=$(yq eval '.app2_config.eks.cluster_name' ../.github/workflows/config/app2-config.yml)
        S3_BUCKET_NAME=$(yq eval '.app2_config.s3.bucket_name' ../.github/workflows/config/app2-config.yml)
        IAM_ROLE_NAME=$(yq eval '.app2_config.iam.role_name' ../.github/workflows/config/app2-config.yml)
        BASTION_HOST_TAG=$(yq eval '.app2_config.bastion.host_tag_name' ../.github/workflows/config/app2-config.yml)
        DEPLOYMENT_NAME=$(yq eval '.app2_config.application.deployment_name' ../.github/workflows/config/app2-config.yml)
        IMAGE_NAME=$(yq eval '.app2_config.application.image_name' ../.github/workflows/config/app2-config.yml)
        HEALTH_CHECK_PATH=$(yq eval '.app2_config.application.health_check_path' ../.github/workflows/config/app2-config.yml)
        AWS_REGION=$(yq eval '.app2_config.network.region' ../.github/workflows/config/app2-config.yml)
        
        # 환경변수로 설정
        echo "PROJECT_NAME=$PROJECT_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$DB_NAME" >> $GITHUB_ENV
        echo "DB_USER=$DB_USER" >> $GITHUB_ENV
        echo "DB_PORT=$DB_PORT" >> $GITHUB_ENV
        echo "DB_RDS_IDENTIFIER=$DB_RDS_IDENTIFIER" >> $GITHUB_ENV
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
        echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV
        echo "IAM_ROLE_NAME=$IAM_ROLE_NAME" >> $GITHUB_ENV
        echo "BASTION_HOST_TAG=$BASTION_HOST_TAG" >> $GITHUB_ENV
        echo "DEPLOYMENT_NAME=$DEPLOYMENT_NAME" >> $GITHUB_ENV
        echo "IMAGE_NAME=$IMAGE_NAME" >> $GITHUB_ENV
        echo "HEALTH_CHECK_PATH=$HEALTH_CHECK_PATH" >> $GITHUB_ENV
        echo "AWS_REGION=$AWS_REGION" >> $GITHUB_ENV
        
        echo "✅ 설정값 로드 완료"
        echo "PROJECT_NAME: $PROJECT_NAME"
        echo "DB_NAME: $DB_NAME"
        echo "AWS_REGION: $AWS_REGION"
    
    # ===============================================
    # Java 및 Maven 환경 설정
    # ===============================================
    - name: Set up JDK 17
      uses: actions/setup-java@v4
      with:
        java-version: '17'
        distribution: 'temurin'
        cache: maven
    
    - name: Validate and Build Maven Project
      run: |
        echo "🔍 Java Spring Boot 애플리케이션 검증 중..."
        if [ -f "server2/pom.xml" ]; then
          cd server2
          echo "📦 Maven 의존성 검증 및 빌드 중..."
          mvn clean compile test package -DskipTests=false
          echo "✅ Maven 빌드 완료"
          ls -la target/
        else
          echo "❌ pom.xml 파일이 없습니다."
          exit 1
        fi
    
    # ===============================================
    # AWS 인증 (OIDC 방식)
    # ===============================================
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN_APP }}
        aws-region: ${{ secrets.AWS_REGION }}
        role-session-name: GitHubActions-Application-${{ github.run_id }}

    # ===============================================
    # 기존 인프라 정보 조회
    # ===============================================
    - name: Get Infrastructure Resources
      run: |
        echo "🔍 기존 인프라 리소스 정보 조회 중..."
        
        # ECR 리포지토리 URI 조회
        ECR_REPO=$(aws ecr describe-repositories --repository-names ${PROJECT_NAME}-ecr --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
        if [ -z "$ECR_REPO" ]; then
          echo "❌ ECR 리포지토리를 찾을 수 없습니다: ${PROJECT_NAME}-ecr"
          echo "먼저 인프라 배포가 필요합니다."
          exit 1
        fi
        echo "ECR_REPOSITORY=$ECR_REPO" >> $GITHUB_ENV
        echo "✅ ECR Repository: $ECR_REPO"
        
        # EKS 클러스터 이름 조회
        EKS_CLUSTER=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.name' --output text 2>/dev/null || echo "")
        if [ -z "$EKS_CLUSTER" ] || [ "$EKS_CLUSTER" == "None" ]; then
          echo "❌ EKS 클러스터를 찾을 수 없습니다: ${{ env.EKS_CLUSTER_NAME }}"
          echo "먼저 인프라 배포가 필요합니다."
          exit 1
        fi
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER" >> $GITHUB_ENV
        echo "✅ EKS Cluster: $EKS_CLUSTER"
        
        # RDS 엔드포인트 조회 (walb2-app 프로젝트용)
        RDS_ENDPOINT=$(aws rds describe-db-instances \
          --db-instance-identifier ${{ env.DB_RDS_IDENTIFIER }} \
          --query "DBInstances[0].Endpoint.Address" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$RDS_ENDPOINT" ] || [ "$RDS_ENDPOINT" == "None" ]; then
          echo "❌ RDS 인스턴스를 찾을 수 없습니다: ${{ env.DB_RDS_IDENTIFIER }}"
          echo "사용 가능한 RDS 인스턴스:"
          aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
          exit 1
        fi
        echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
        echo "✅ RDS Endpoint: $RDS_ENDPOINT"
        
        # EKS 클러스터 상태 확인
        EKS_STATUS=$(aws eks describe-cluster --name $EKS_CLUSTER --query 'cluster.status' --output text)
        if [ "$EKS_STATUS" != "ACTIVE" ]; then
          echo "❌ EKS 클러스터가 활성 상태가 아닙니다: $EKS_STATUS"
          exit 1
        fi
        echo "✅ EKS Cluster Status: $EKS_STATUS"
    
    # ===============================================
    # ECR 로그인
    # ===============================================
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # ===============================================
    # Docker 이미지 빌드 및 푸시
    # ===============================================
    - name: Build and push Docker image
      id: build-image
      run: |
        echo "🐳 Docker 이미지 빌드 중..."
        
        # Git 커밋 해시를 태그로 사용
        IMAGE_TAG=${{ github.sha }}
        IMAGE_URI=${{ env.ECR_REPOSITORY }}:$IMAGE_TAG
        
        # server2 폴더로 이동해서 Docker 빌드
        cd server2
        docker build -t $IMAGE_URI .
        docker tag $IMAGE_URI ${{ env.ECR_REPOSITORY }}:latest
        
        echo "📤 ECR에 이미지 푸시 중..."
        docker push $IMAGE_URI
        docker push ${{ env.ECR_REPOSITORY }}:latest
        
        echo "✅ 이미지 푸시 완료: $IMAGE_URI"
        echo "image=$IMAGE_URI" >> $GITHUB_OUTPUT

    - name: Test Database Connection via Bastion
      run: |
        # AWS CLI를 사용해서 리소스 정보 직접 조회
        PROJECT_NAME="${{ env.PROJECT_NAME }}"
        
        # RDS 엔드포인트 조회 (태그 기반)
        echo "🔍 RDS 인스턴스 조회 중..."
        DB_HOST=$(aws rds describe-db-instances \
          --query "DBInstances[?contains(keys(TagList[?Key=='Project']), 'Project') && TagList[?Key=='Project'].Value[0]=='${PROJECT_NAME}'].Endpoint.Address" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$DB_HOST" ]; then
          # 태그 조회가 안 되면 직접 식별자로 조회
          DB_HOST=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.DB_RDS_IDENTIFIER }} \
            --query "DBInstances[0].Endpoint.Address" \
            --output text 2>/dev/null || echo "")
        fi
        
        # Bastion Host IP 조회 (태그 기반)
        echo "🔍 Bastion Host 조회 중..."
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${{ env.BASTION_HOST_TAG }}" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          # 태그로 안 되면 보안그룹으로 조회
          BASTION_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text 2>/dev/null || echo "")
        fi
        
        # DB 사용자명과 DB 이름 (환경변수 사용)
        DB_NAME="${{ env.DB_NAME }}"
        DB_USER="${{ env.DB_USER }}"
        
        # Parameter Store에서 DB 패스워드 조회
        echo "🔍 DB 패스워드 조회 중..."
        DB_PASSWORD=$(aws ssm get-parameter \
          --name "/${PROJECT_NAME}/rds/master-password" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text 2>/dev/null || echo "")
        
        # 값 검증
        if [ -z "$DB_HOST" ] || [ "$DB_HOST" == "None" ]; then
          echo "❌ RDS 엔드포인트를 찾을 수 없습니다."
          echo "사용 가능한 RDS 인스턴스:"
          aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
          exit 1
        fi
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "❌ Bastion Host를 찾을 수 없습니다."
          echo "실행 중인 EC2 인스턴스:"
          aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" \
            --query "Reservations[*].Instances[*].[InstanceId,PublicIpAddress,Tags[?Key=='Name'].Value[0]]" \
            --output table
          exit 1
        fi
        
        if [ -z "$DB_PASSWORD" ]; then
          echo "❌ DB 패스워드를 Parameter Store에서 찾을 수 없습니다."
          exit 1
        fi
        
        echo "✅ DB Host: '$DB_HOST'"
        echo "✅ Bastion IP: '$BASTION_IP'"
        echo "✅ DB User: '$DB_USER'"
        echo "✅ DB Name: '$DB_NAME'"
        
        # 보안 그룹 자동 설정
        echo "🔧 보안 그룹 규칙 자동 설정 중..."
        
        # Bastion Host 정보 조회
        BASTION_INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=ip-address,Values=$BASTION_IP" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text 2>/dev/null || echo "")
        
        if [ -n "$BASTION_INSTANCE_ID" ] && [ "$BASTION_INSTANCE_ID" != "None" ]; then
          # Bastion Host의 보안 그룹 조회
          BASTION_SG=$(aws ec2 describe-instances \
            --instance-ids $BASTION_INSTANCE_ID \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
            --output text 2>/dev/null || echo "")
          
          echo "Bastion Security Group: $BASTION_SG"
        fi
        
        # RDS 정보 조회
        RDS_INSTANCE_ID=$(aws rds describe-db-instances \
          --query "DBInstances[?Endpoint.Address=='$DB_HOST'].DBInstanceIdentifier" \
          --output text 2>/dev/null || echo "")
        
        if [ -n "$RDS_INSTANCE_ID" ] && [ "$RDS_INSTANCE_ID" != "None" ]; then
          # RDS의 보안 그룹 조회
          RDS_SG=$(aws rds describe-db-instances \
            --db-instance-identifier $RDS_INSTANCE_ID \
            --query "DBInstances[0].VpcSecurityGroups[0].VpcSecurityGroupId" \
            --output text 2>/dev/null || echo "")
          
          echo "RDS Security Group: $RDS_SG"
          
          # RDS 보안 그룹에 Bastion 접근 규칙 추가
          if [ -n "$BASTION_SG" ] && [ -n "$RDS_SG" ]; then
            echo "🔧 RDS 보안 그룹에 Bastion 접근 규칙 추가 중..."
            
            # 기존 규칙 확인
            EXISTING_RULE=$(aws ec2 describe-security-groups \
              --group-ids $RDS_SG \
              --query "SecurityGroups[0].IpPermissions[?FromPort==\`${{ env.DB_PORT }}\` && UserIdGroupPairs[?GroupId==\`$BASTION_SG\`]]" \
              --output text 2>/dev/null || echo "")
            
            if [ -z "$EXISTING_RULE" ]; then
              aws ec2 authorize-security-group-ingress \
                --group-id $RDS_SG \
                --protocol tcp \
                --port ${{ env.DB_PORT }} \
                --source-group $BASTION_SG \
                --no-cli-pager 2>/dev/null && echo "✅ RDS 인바운드 규칙 추가 완료" || echo "⚠️ RDS 규칙 추가 실패 (이미 존재할 수 있음)"
            else
              echo "✅ RDS 인바운드 규칙이 이미 존재합니다"
            fi
          fi
        fi
        
        # Bastion 보안 그룹에 아웃바운드 규칙 확인/추가
        if [ -n "$BASTION_SG" ]; then
          echo "🔧 Bastion 보안 그룹 아웃바운드 규칙 확인 중..."
          
          # MySQL 아웃바운드 규칙 확인
          EXISTING_OUTBOUND=$(aws ec2 describe-security-groups \
            --group-ids $BASTION_SG \
            --query "SecurityGroups[0].IpPermissionsEgress[?FromPort==\`${{ env.DB_PORT }}\` || IpProtocol==\`-1\`]" \
            --output text 2>/dev/null || echo "")
          
          if [ -z "$EXISTING_OUTBOUND" ]; then
            echo "🔧 Bastion 아웃바운드 규칙 추가 중..."
            aws ec2 authorize-security-group-egress \
              --group-id $BASTION_SG \
              --protocol tcp \
              --port ${{ env.DB_PORT }} \
              --cidr 0.0.0.0/0 \
              --no-cli-pager 2>/dev/null && echo "✅ Bastion 아웃바운드 규칙 추가 완료" || echo "⚠️ Bastion 규칙 추가 실패 (이미 존재할 수 있음)"
          else
            echo "✅ Bastion 아웃바운드 규칙이 이미 존재합니다"
          fi
        fi
        
        # SSH 키를 Parameter Store에서 가져오기
        echo "🔑 SSH 키 조회 중..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        echo "🔍 SSH 연결 테스트 중..."
        # SSH 연결 테스트 (타임아웃 설정)
        if ! ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP "echo 'SSH connection successful'" 2>/dev/null; then
          echo "❌ SSH 연결 실패. Bastion Host 상태 확인:"
          aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" \
            --query "Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress,PrivateIpAddress]" \
            --output table
          exit 1
        fi
        
        echo "🔗 SSH 터널을 통한 MySQL 연결 테스트 중..."
        
        # MySQL 클라이언트 설치 (먼저 설치)
        if ! command -v mysql &> /dev/null; then
          echo "📦 MySQL 클라이언트 설치 중..."
          sudo apt-get update -qq && sudo apt-get install -y mysql-client
        fi
        
        # 로컬 포트가 사용 중인지 확인
        if netstat -tuln | grep -q ":${{ env.DB_PORT }} "; then
          echo "⚠️ 포트 ${{ env.DB_PORT }}이 이미 사용 중입니다. 다른 포트를 사용합니다."
          LOCAL_PORT=$(({{ env.DB_PORT }} + 1))
        else
          LOCAL_PORT=${{ env.DB_PORT }}
        fi
        
        echo "🔗 SSH 터널 생성 중... (로컬 포트: $LOCAL_PORT)"
        # SSH 터널 생성 with 더 많은 옵션
        ssh -i bastion_key.pem \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ExitOnForwardFailure=yes \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=3 \
            -o ConnectTimeout=30 \
            -L $LOCAL_PORT:$DB_HOST:${{ env.DB_PORT }} \
            ec2-user@$BASTION_IP \
            -N &
        SSH_PID=$!
        
        # SSH 터널이 정상적으로 시작되었는지 확인
        echo "⏳ SSH 터널 설정 확인 중..."
        sleep 5
        
        # SSH 프로세스가 아직 실행 중인지 확인
        if ! kill -0 $SSH_PID 2>/dev/null; then
          echo "❌ SSH 터널 생성 실패"
          echo "SSH 연결 로그 확인:"
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "echo 'SSH 연결 테스트 성공'" || echo "SSH 기본 연결 실패"
          rm -f bastion_key.pem
          exit 1
        fi
        
        # 터널 포트가 열렸는지 확인
        echo "🔍 터널 포트 확인 중..."
        for i in {1..30}; do
          if netstat -tuln | grep -q ":$LOCAL_PORT "; then
            echo "✅ SSH 터널이 성공적으로 설정되었습니다 (포트: $LOCAL_PORT)"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "❌ SSH 터널 포트 설정 시간 초과"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi
          sleep 2
        done
        
        # Bastion Host에서 직접 DB 연결 테스트
        echo "🔍 Bastion Host에서 DB 직접 연결 테스트 중..."
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "
          # MySQL 클라이언트 설치 확인
          if ! command -v mysql &> /dev/null; then
            echo '📦 Bastion에 MySQL 클라이언트 설치 중...'
            sudo yum update -y
            sudo yum install -y mysql
          fi
          
          # DB 연결 테스트
          echo '🔍 Bastion에서 RDS 직접 연결 테스트...'
          if mysql -h $DB_HOST -P ${{ env.DB_PORT }} -u $DB_USER -p'$DB_PASSWORD' $DB_NAME -e 'SELECT VERSION();' 2>/dev/null; then
            echo '✅ Bastion에서 RDS 직접 연결 성공'
          else
            echo '❌ Bastion에서 RDS 직접 연결 실패'
            echo '🔍 네트워크 연결 확인:'
            telnet $DB_HOST ${{ env.DB_PORT }} < /dev/null 2>&1 | head -5
            echo '🔍 보안 그룹 확인이 필요합니다.'
            exit 1
          fi
        "
        
        # 터널을 통한 연결 테스트 (재시도 로직 추가)
        echo "🔍 SSH 터널을 통한 데이터베이스 연결 테스트 중..."
        DB_CONNECTED=false
        for i in {1..5}; do
          if mysql --protocol=TCP -h 127.0.0.1 -P $LOCAL_PORT -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "SELECT 1;" 2>/dev/null; then
            echo "✅ 데이터베이스 연결 성공 (시도 $i/5)"
            DB_CONNECTED=true
            break
          else
            echo "⚠️ 데이터베이스 연결 실패 (시도 $i/5)"
            # 자세한 오류 정보 출력
            echo "🔍 상세 오류 정보:"
            mysql --protocol=TCP -h 127.0.0.1 -P $LOCAL_PORT -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "SELECT 1;" 2>&1 || true
            
            if [ $i -lt 5 ]; then
              echo "3초 후 재시도..."
              sleep 3
            fi
          fi
        done
        
        if [ "$DB_CONNECTED" != "true" ]; then
          echo "❌ 데이터베이스 연결 최종 실패"
          echo "🔍 연결 정보 확인:"
          echo "  로컬 포트: $LOCAL_PORT"
          echo "  DB 호스트: $DB_HOST"
          echo "  DB 사용자: $DB_USER"
          echo "  DB 이름: $DB_NAME"
          echo "🔍 네트워크 상태:"
          netstat -tuln | grep ":$LOCAL_PORT"
          echo "🔍 SSH 터널 상태:"
          ps aux | grep ssh | grep $BASTION_IP || echo "SSH 프로세스 없음"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        # SSH 터널 종료 및 정리
        echo "🧹 SSH 터널 정리 중..."
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem
        
        echo "✅ 데이터베이스 연결 테스트 완료"
      env:
        PROJECT_NAME: ${{ env.PROJECT_NAME }}
        
    # ===============================================
    # MySQL 클라이언트 설치 및 스키마 적용
    # ===============================================
    - name: Apply Database Schema
      if: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
        echo "🗄️ 데이터베이스 스키마 적용 중..."
        
        # MySQL 클라이언트 설치
        sudo apt-get update && sudo apt-get install -y mysql-client
        
        # 환경 변수 설정
        PROJECT_NAME="${{ env.PROJECT_NAME }}"
        DB_HOST="${{ env.RDS_ENDPOINT }}"
        DB_NAME="${{ env.DB_NAME }}"
        DB_USER="${{ env.DB_USER }}"
        
        # Bastion Host IP 조회
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${{ env.BASTION_HOST_TAG }}" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "❌ Bastion Host를 찾을 수 없습니다."
          exit 1
        fi
        
        echo "🔍 연결 정보:"
        echo "  RDS 엔드포인트: $DB_HOST"
        echo "  Bastion IP: $BASTION_IP"
        echo "  DB 이름: $DB_NAME"
        echo "  DB 사용자: $DB_USER"
        
        # 보안 그룹 자동 설정
        echo "🔧 보안 그룹 규칙 자동 설정 중..."
        
        # Bastion Host 정보 조회
        BASTION_INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=ip-address,Values=$BASTION_IP" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text 2>/dev/null || echo "")
        
        if [ -n "$BASTION_INSTANCE_ID" ] && [ "$BASTION_INSTANCE_ID" != "None" ]; then
          # Bastion Host의 보안 그룹 조회
          BASTION_SG=$(aws ec2 describe-instances \
            --instance-ids $BASTION_INSTANCE_ID \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
            --output text 2>/dev/null || echo "")
          
          echo "Bastion Security Group: $BASTION_SG"
        fi
        
        # RDS 정보 조회
        RDS_INSTANCE_ID=$(aws rds describe-db-instances \
          --query "DBInstances[?Endpoint.Address=='$DB_HOST'].DBInstanceIdentifier" \
          --output text 2>/dev/null || echo "")
        
        if [ -n "$RDS_INSTANCE_ID" ] && [ "$RDS_INSTANCE_ID" != "None" ]; then
          # RDS의 보안 그룹 조회
          RDS_SG=$(aws rds describe-db-instances \
            --db-instance-identifier $RDS_INSTANCE_ID \
            --query "DBInstances[0].VpcSecurityGroups[0].VpcSecurityGroupId" \
            --output text 2>/dev/null || echo "")
          
          echo "RDS Security Group: $RDS_SG"
          
          # RDS 보안 그룹에 Bastion 접근 규칙 추가
          if [ -n "$BASTION_SG" ] && [ -n "$RDS_SG" ]; then
            echo "🔧 RDS 보안 그룹에 Bastion 접근 규칙 추가 중..."
            
            if ! aws ec2 describe-security-groups --group-ids $RDS_SG --query "SecurityGroups[0].IpPermissions[?FromPort==\`${{ env.DB_PORT }}\` && ToPort==\`${{ env.DB_PORT }}\` && UserIdGroupPairs[?GroupId==\`$BASTION_SG\`]]" --output text | grep -q "${{ env.DB_PORT }}"; then
              aws ec2 authorize-security-group-ingress \
                --group-id $RDS_SG \
                --protocol tcp \
                --port ${{ env.DB_PORT }} \
                --source-group $BASTION_SG \
                --no-cli-pager && echo "✅ RDS 인바운드 규칙 추가 완료"
            else
              echo "✅ RDS 인바운드 규칙이 이미 존재합니다"
            fi
          fi
        fi
        
        # Bastion 보안 그룹에 아웃바운드 규칙 추가
        if [ -n "$BASTION_SG" ]; then
          echo "🔧 Bastion 보안 그룹 아웃바운드 규칙 추가 중..."
          
          if ! aws ec2 describe-security-groups --group-ids $BASTION_SG --query "SecurityGroups[0].IpPermissionsEgress[?FromPort==\`${{ env.DB_PORT }}\` && ToPort==\`${{ env.DB_PORT }}\` && IpRanges[?CidrIp==\`0.0.0.0/0\`]]" --output text | grep -q "${{ env.DB_PORT }}"; then
            aws ec2 authorize-security-group-egress \
              --group-id $BASTION_SG \
              --protocol tcp \
              --port ${{ env.DB_PORT }} \
              --cidr 0.0.0.0/0 \
              --no-cli-pager && echo "✅ Bastion 아웃바운드 규칙 추가 완료"
          else
            echo "✅ Bastion 아웃바운드 규칙이 이미 존재합니다"
          fi
        fi
        
        # SSH 키를 Parameter Store에서 가져오기
        echo "🔑 SSH 키 가져오는 중..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        # 로컬 포트가 사용 중인지 확인
        if netstat -tuln | grep -q ":${{ env.DB_PORT }} "; then
          echo "⚠️ 포트 ${{ env.DB_PORT }}이 이미 사용 중입니다. 다른 포트를 사용합니다."
          LOCAL_PORT=$(({{ env.DB_PORT }} + 1))
        else
          LOCAL_PORT=${{ env.DB_PORT }}
        fi
        
        # SSH 터널 생성 (백그라운드에서 실행)
        echo "🔗 SSH 터널 생성 중... (로컬 포트: $LOCAL_PORT)"
        ssh -i bastion_key.pem \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ExitOnForwardFailure=yes \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=3 \
            -o ConnectTimeout=30 \
            -L $LOCAL_PORT:$DB_HOST:${{ env.DB_PORT }} \
            ec2-user@$BASTION_IP \
            -N &
        SSH_PID=$!
        
        # 터널 설정 대기 및 확인
        echo "⏳ SSH 터널 설정 확인 중..."
        sleep 5
        
        # SSH 터널 상태 확인
        if ! kill -0 $SSH_PID 2>/dev/null; then
          echo "❌ SSH 터널 생성 실패"
          echo "SSH 연결 로그 확인:"
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "echo 'SSH 연결 테스트 성공'" || echo "SSH 기본 연결 실패"
          rm -f bastion_key.pem
          exit 1
        fi
        
        # 터널 포트가 열렸는지 확인
        echo "🔍 터널 포트 확인 중..."
        for i in {1..30}; do
          if netstat -tuln | grep -q ":$LOCAL_PORT "; then
            echo "✅ SSH 터널이 성공적으로 설정되었습니다 (포트: $LOCAL_PORT)"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "❌ SSH 터널 포트 설정 시간 초과"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi
          sleep 2
        done
        
        echo "✅ SSH 터널 생성 완료"
        
        # Bastion Host에서 직접 DB 연결 테스트
        echo "🔍 Bastion Host에서 DB 직접 연결 테스트 중..."
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "
          # MySQL 클라이언트 설치 확인
          if ! command -v mysql &> /dev/null; then
            echo '📦 Bastion에 MySQL 클라이언트 설치 중...'
            sudo yum update -y
            sudo yum install -y mysql
          fi
          
          # DB 연결 테스트
          echo '🔍 Bastion에서 RDS 직접 연결 테스트...'
          if mysql -h $DB_HOST -P ${{ env.DB_PORT }} -u $DB_USER -p'${{ secrets.DB_PASSWORD }}' $DB_NAME -e 'SELECT VERSION();' 2>/dev/null; then
            echo '✅ Bastion에서 RDS 직접 연결 성공'
          else
            echo '❌ Bastion에서 RDS 직접 연결 실패'
            echo '🔍 네트워크 연결 확인:'
            nc -zv $DB_HOST ${{ env.DB_PORT }} 2>&1 || telnet $DB_HOST ${{ env.DB_PORT }} < /dev/null 2>&1 | head -5
            echo '🔍 보안 그룹 확인이 필요합니다.'
            exit 1
          fi
        "
        
        # 로컬 포트를 통해 RDS 연결 테스트 (재시도 로직)
        echo "🔍 SSH 터널을 통한 데이터베이스 연결 테스트 중..."
        DB_CONNECTED=false
        for i in {1..5}; do
          if mysql \
            -h localhost \
            -P $LOCAL_PORT \
            -u "$DB_USER" \
            -p"${{ secrets.DB_PASSWORD }}" \
            "$DB_NAME" \
            -e "SELECT VERSION();" 2>/dev/null; then
            echo "✅ 데이터베이스 연결 성공 (시도 $i/5)"
            DB_CONNECTED=true
            break
          else
            echo "⚠️ 데이터베이스 연결 실패 (시도 $i/5)"
            # 자세한 오류 정보 출력
            echo "🔍 상세 오류 정보:"
            mysql \
              -h localhost \
              -P $LOCAL_PORT \
              -u "$DB_USER" \
              -p"${{ secrets.DB_PASSWORD }}" \
              "$DB_NAME" \
              -e "SELECT VERSION();" 2>&1 || true
            
            if [ $i -lt 5 ]; then
              echo "3초 후 재시도..."
              sleep 3
            fi
          fi
        done
        
        if [ "$DB_CONNECTED" != "true" ]; then
          echo "❌ 데이터베이스 연결 최종 실패"
          echo "🔍 연결 정보:"
          echo "  로컬 포트: $LOCAL_PORT"
          echo "  DB 호스트: $DB_HOST"
          echo "  DB 사용자: $DB_USER"
          echo "  DB 이름: $DB_NAME"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        # 기본 연결 테스트만 수행 (Java 애플리케이션은 자체적으로 DB 연결 관리)
        echo "🔍 데이터베이스 연결 확인 완료"
        echo "ℹ️ Java Spring Boot 애플리케이션은 자체적으로 데이터베이스 연결을 설정합니다."

        # 정리 작업
        echo "🧹 정리 작업 중..."
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem

        echo "✅ 데이터베이스 스키마 작업 완료"

    # ===============================================
    # kubectl 및 Helm 설치
    # ===============================================
    - name: Install kubectl and Helm
      run: |
          echo "🔧 kubectl 및 Helm 설치 중..."

          # kubectl 설치
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Helm 설치
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

          echo "✅ kubectl 및 Helm 설치 완료"
          kubectl version --client
          helm version

    - name: Update kubeconfig for EKS
      run: |
          echo "🔧 EKS 클러스터 kubeconfig 업데이트 중..."

          # 현재 AWS 자격 증명 확인
          echo "🔍 현재 AWS 자격 증명 확인..."
          aws sts get-caller-identity

          # EKS 클러스터 이름 조회 (여러 방법 시도)
          echo "🔍 EKS 클러스터 조회 중..."

          # 방법 1: 클러스터 목록에서 첫 번째 조회
          EKS_CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")

          # 방법 2: 특정 이름으로 조회
          if [ -z "$EKS_CLUSTER_NAME" ] || [ "$EKS_CLUSTER_NAME" == "None" ]; then
            EKS_CLUSTER_NAME="${{ env.EKS_CLUSTER_NAME }}"
            echo "기본 클러스터 이름 사용: $EKS_CLUSTER_NAME"
          fi

          # 클러스터 존재 확인
          if ! aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} >/dev/null 2>&1; then
            echo "❌ EKS 클러스터 '$EKS_CLUSTER_NAME'를 찾을 수 없습니다."
            echo "사용 가능한 클러스터 목록:"
            aws eks list-clusters --region ${{ secrets.AWS_REGION }}
            exit 1
          fi

          echo "✅ EKS 클러스터: $EKS_CLUSTER_NAME"
          echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV

          # 클러스터 상태 확인
          CLUSTER_STATUS=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.status' --output text)
          echo "클러스터 상태: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "❌ 클러스터가 ACTIVE 상태가 아닙니다: $CLUSTER_STATUS"
            exit 1
          fi

          # IAM 역할과 OIDC 공급자 정보 확인
          echo "🔍 IAM 역할과 OIDC 설정 확인 중..."
          CLUSTER_OIDC_ISSUER=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.identity.oidc.issuer' --output text)
          echo "OIDC 발행자: $CLUSTER_OIDC_ISSUER"
          
          # kubeconfig 업데이트 (현재 OIDC 자격 증명 사용)
          echo "🔧 kubeconfig 업데이트 중 (OIDC 자격 증명 사용)..."
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name "$EKS_CLUSTER_NAME" \
            --verbose

          # AWS 자격 증명 확인
          echo "🔍 현재 AWS 자격 증명 확인..."
          aws sts get-caller-identity
          
          # 클러스터 연결 테스트 (자세한 오류 정보 포함)
          echo "🔍 클러스터 연결 테스트..."
          if ! kubectl cluster-info --request-timeout=30s; then
            echo "❌ kubectl cluster-info 실패. 추가 진단 정보:"
            
            # kubectl 설정 확인
            echo "kubectl 설정 확인:"
            kubectl config view --minify
            
            # 현재 컨텍스트 확인
            echo "현재 컨텍스트:"
            kubectl config current-context
            
            # 클러스터 엔드포인트 직접 테스트
            CLUSTER_ENDPOINT=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.endpoint' --output text)
            echo "클러스터 엔드포인트: $CLUSTER_ENDPOINT"
            
            # kubeconfig 다시 설정
            echo "🔧 kubeconfig 재설정..."
            aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name "$EKS_CLUSTER_NAME" --verbose
            
            # aws-auth ConfigMap 상태 확인
            echo "🔍 aws-auth ConfigMap 확인..."
            kubectl get configmap aws-auth -n kube-system -o yaml || echo "aws-auth ConfigMap 없음"
            
            exit 1
          fi

          # aws-auth ConfigMap 상태 확인 (정보성)
          echo "🔍 aws-auth ConfigMap 상태 확인..."
          kubectl get configmap aws-auth -n kube-system -o yaml | head -20 || echo "aws-auth ConfigMap 조회 실패"
          
          echo "✅ EKS 클러스터 연결 성공"

          echo "🔍 노드 상태 확인..."
          kubectl get nodes --show-labels

      # ===============================================
      # Ingress 및 ALB 동기화 상태 체크 및 정리
      # ===============================================
    - name: Check and Clean Ingress Resources
      run: |
          echo "🔍 기존 Ingress 리소스 및 finalizer 상태 체크 중..."
          
          # Ingress finalizer 체크 및 정리 함수
          check_and_clean_ingress() {
            local namespace="$1"
            local ingress_name="$2"
            
            echo "🔍 Ingress '$ingress_name' (네임스페이스: $namespace) 상태 확인..."
            
            # Ingress 존재 확인
            if ! kubectl get ingress "$ingress_name" -n "$namespace" >/dev/null 2>&1; then
              echo "ℹ️ Ingress '$ingress_name'이 존재하지 않습니다."
              return 0
            fi
            
            # finalizer 확인
            local finalizers=$(kubectl get ingress "$ingress_name" -n "$namespace" -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "")
            
            if [[ "$finalizers" == *"ingress.k8s.aws/resources"* ]]; then
              echo "⚠️ Ingress '$ingress_name'에 finalizer가 남아있습니다: $finalizers"
              
              # Ingress 삭제 시도
              echo "🗑️ Ingress 삭제 시도 중..."
              kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s &
              local delete_pid=$!
              
              # 30초 대기
              sleep 30
              
              # 프로세스가 아직 실행 중인지 확인
              if kill -0 $delete_pid 2>/dev/null; then
                echo "⚠️ Ingress 삭제가 finalizer로 인해 멈춤. 강제 정리 시작..."
                kill $delete_pid 2>/dev/null || true
                
                # validating webhook 확인 및 삭제
                echo "🔍 validating webhook 확인 중..."
                local webhook_exists=$(kubectl get validatingwebhookconfigurations aws-load-balancer-webhook 2>/dev/null | wc -l)
                if [ "$webhook_exists" -gt 0 ]; then
                  echo "🗑️ aws-load-balancer-webhook 삭제 중..."
                  kubectl delete validatingwebhookconfigurations aws-load-balancer-webhook --timeout=10s || echo "webhook 삭제 실패 (이미 없을 수 있음)"
                  echo "✅ webhook 삭제 완료"
                else
                  echo "ℹ️ validating webhook이 이미 없습니다."
                fi
                
                # mutating webhook 확인 및 삭제
                local mutating_webhook_exists=$(kubectl get mutatingwebhookconfigurations aws-load-balancer-webhook 2>/dev/null | wc -l)
                if [ "$mutating_webhook_exists" -gt 0 ]; then
                  echo "🗑️ aws-load-balancer-webhook (mutating) 삭제 중..."
                  kubectl delete mutatingwebhookconfigurations aws-load-balancer-webhook --timeout=10s || echo "mutating webhook 삭제 실패 (이미 없을 수 있음)"
                  echo "✅ mutating webhook 삭제 완료"
                fi
                
                # finalizer 강제 제거
                echo "🔧 finalizer 강제 제거 중..."
                if kubectl patch ingress "$ingress_name" -n "$namespace" -p '{"metadata":{"finalizers":[]}}' --type=merge; then
                  echo "✅ finalizer 제거 성공"
                  
                  # 다시 삭제 시도
                  echo "🗑️ Ingress 재삭제 시도..."
                  if kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s; then
                    echo "✅ Ingress '$ingress_name' 삭제 완료"
                  else
                    echo "⚠️ Ingress 재삭제 실패. 수동 확인 필요"
                  fi
                else
                  echo "❌ finalizer 제거 실패"
                fi
              else
                echo "✅ Ingress '$ingress_name' 정상 삭제 완료"
              fi
            else
              echo "✅ Ingress '$ingress_name'에 문제되는 finalizer 없음"
            fi
          }
          
          # 모든 네임스페이스의 Ingress 조회 및 체크
          echo "🔍 모든 네임스페이스의 Ingress 리소스 조회..."
          kubectl get ingress -A --no-headers 2>/dev/null | while read namespace name class hosts address ports age; do
            if [ -n "$namespace" ] && [ -n "$name" ]; then
              check_and_clean_ingress "$namespace" "$name"
            fi
          done || echo "ℹ️ 기존 Ingress 리소스가 없습니다."
          
          echo "✅ Ingress 리소스 정리 완료"

      # ===============================================
      # AWS Load Balancer Controller 확인 및 복구
      # ===============================================
    - name: Check and Recover AWS Load Balancer Controller
      run: |
          echo "🔍 AWS Load Balancer Controller 상태 확인 중..."
          
          # AWS Load Balancer Controller Pod 확인
          if kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller >/dev/null 2>&1; then
            echo "✅ AWS Load Balancer Controller Pod 발견"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            
            # Pod 상태 확인
            running_pods=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --field-selector=status.phase=Running --no-headers | wc -l)
            if [ "$running_pods" -eq 0 ]; then
              echo "⚠️ AWS Load Balancer Controller Pod가 Running 상태가 아닙니다. 재시작 중..."
              kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
              kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=120s
              echo "✅ AWS Load Balancer Controller 재시작 완료"
            fi
          else
            echo "❌ AWS Load Balancer Controller를 찾을 수 없습니다."
            echo "Terraform으로 AWS Load Balancer Controller를 먼저 설치해주세요."
            exit 1
          fi
          
          # IngressClass 확인
          if kubectl get ingressclass alb >/dev/null 2>&1; then
            echo "✅ IngressClass 'alb' 확인됨"
            kubectl get ingressclass alb
          else
            echo "❌ IngressClass 'alb'를 찾을 수 없습니다."
            echo "사용 가능한 IngressClass:"
            kubectl get ingressclass
            exit 1
          fi
          
          # Webhook config 재생성 대기
          echo "⏳ webhook config 재생성 대기 중..."
          sleep 10
          
          # ValidatingAdmissionWebhook 확인
          webhook_ready=false
          for i in {1..6}; do
            if kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)" >/dev/null 2>&1; then
              echo "✅ ValidatingWebhookConfiguration 확인됨"
              kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)"
              webhook_ready=true
              break
            else
              echo "⏳ ValidatingWebhookConfiguration 대기 중... ($i/6)"
              sleep 5
            fi
          done
          
          if [ "$webhook_ready" != "true" ]; then
            echo "⚠️ ValidatingWebhookConfiguration을 찾을 수 없습니다."
            echo "AWS Load Balancer Controller가 아직 완전히 준비되지 않았을 수 있습니다."
            echo "🔧 Controller 강제 재시작 중..."
            kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=120s
            
            # 재시작 후 다시 확인
            echo "⏳ 재시작 후 webhook 재확인..."
            sleep 15
            if kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)" >/dev/null 2>&1; then
              echo "✅ ValidatingWebhookConfiguration 재확인 성공"
            else
              echo "❌ ValidatingWebhookConfiguration 재확인 실패. 수동 점검 필요"
            fi
          fi
          
          echo "✅ AWS Load Balancer Controller 확인 완료 - Ingress 기반 ALB 생성 준비됨"

      # ===============================================
      # 기존 k8s 매니페스트 업데이트 및 사용
      # ===============================================
    - name: Update existing Kubernetes manifests
      run: |
          echo "🔧 기존 k8s 매니페스트 파일 업데이트 중..."
          cd server2
          cd k8s
          
          # namespace.yml 업데이트 (프로젝트명 사용)
          sed -i "s/music1-namespace/${{ env.PROJECT_NAME }}/g" namespace.yml
          
          # deployment.yml 업데이트
          sed -i "s/music1-deployment/${{ env.PROJECT_NAME }}-app/g" deployment.yml
          sed -i "s/music1-namespace/${{ env.PROJECT_NAME }}/g" deployment.yml
          sed -i "s/music1-app/${{ env.PROJECT_NAME }}-app/g" deployment.yml
          sed -i "s/music1-service/${{ env.PROJECT_NAME }}-service/g" deployment.yml
          sed -i "s|YOUR_ECR_URI/${{ env.IMAGE_NAME }}:latest|${{ steps.build-image.outputs.image }}|g" deployment.yml
          
          # Spring Boot Actuator 헬스체크로 변경
          sed -i "s|path: /api/music|path: ${{ env.HEALTH_CHECK_PATH }}|g" deployment.yml
          
          # Service를 ClusterIP로 변경 (Ingress 사용을 위해)
          sed -i "s/type: LoadBalancer/type: ClusterIP/g" deployment.yml
          
          # ServiceAccount 추가
          sed -i "/spec:/a\\      serviceAccountName: ${{ env.PROJECT_NAME }}-service-account" deployment.yml
          
          # envFrom 설정 추가 (환경변수 주입)
          sed -i "/- name: SPRING_PROFILES_ACTIVE/i\\        envFrom:\n        - configMapRef:\n            name: ${{ env.PROJECT_NAME }}-config\n        - secretRef:\n            name: ${{ env.PROJECT_NAME }}-secret" deployment.yml
          
          echo "✅ 기존 k8s 매니페스트 업데이트 완료"
          
          # 업데이트된 파일 확인
          echo "📋 업데이트된 namespace.yml:"
          cat namespace.yml
          echo "📋 업데이트된 deployment.yml:"
          cat deployment.yml

    - name: Get Subnet Information
      run: |
          echo "🔍 서브넷 정보 조회 중..."
          
          # 퍼블릭 서브넷 조회 (ALB용)
          PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
            --filters "Name=tag:Type,Values=public" "Name=state,Values=available" \
            --query "Subnets[*].SubnetId" \
            --output text | tr '\t' ',' | sed 's/,$//')
          
          if [ -z "$PUBLIC_SUBNETS" ]; then
            # 태그로 못 찾으면 기본값 사용
            PUBLIC_SUBNETS="subnet-0c3b7ce8248681101,subnet-057589a6494b65a9c"
            echo "⚠️ 퍼블릭 서브넷 태그를 찾을 수 없어 기본값 사용: $PUBLIC_SUBNETS"
          else
            echo "✅ 퍼블릭 서브넷 발견: $PUBLIC_SUBNETS"
          fi
          
          echo "PUBLIC_SUBNETS=$PUBLIC_SUBNETS" >> $GITHUB_ENV

    - name: Generate additional Kubernetes manifests
      run: |
          echo "📝 추가 Kubernetes 매니페스트 생성 중..."
          
          # ConfigMap 생성 (환경변수)
          echo "apiVersion: v1" > configmap.yaml
          echo "kind: ConfigMap" >> configmap.yaml
          echo "metadata:" >> configmap.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-config" >> configmap.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> configmap.yaml
          echo "data:" >> configmap.yaml
          echo '  DB_HOST: "${{ env.RDS_ENDPOINT }}"' >> configmap.yaml
          echo '  DB_PORT: "${{ env.DB_PORT }}"' >> configmap.yaml
          echo '  DB_NAME: "${{ env.DB_NAME }}"' >> configmap.yaml
          echo '  DB_USER: "${{ env.DB_USER }}"' >> configmap.yaml
          echo '  AWS_REGION: "${{ secrets.AWS_REGION }}"' >> configmap.yaml
          echo '  AWS_S3_BUCKET: "${{ env.S3_BUCKET_NAME }}"' >> configmap.yaml
          echo '  AWS_S3_REGION: "${{ secrets.AWS_REGION }}"' >> configmap.yaml
          echo '  STORAGE_TYPE: "s3"' >> configmap.yaml
          echo '  SPRING_PROFILES_ACTIVE: "production"' >> configmap.yaml
          echo '  JAVA_OPTS: "-Xmx512m -Xms256m"' >> configmap.yaml
          echo '  TZ: "Asia/Seoul"' >> configmap.yaml
          echo '  SERVER_PORT: "8080"' >> configmap.yaml
          echo '  MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: "health,info,prometheus"' >> configmap.yaml
        
          # Secret 생성 (DB 패스워드)
          echo "apiVersion: v1" > secret.yaml
          echo "kind: Secret" >> secret.yaml
          echo "metadata:" >> secret.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-secret" >> secret.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> secret.yaml
          echo "type: Opaque" >> secret.yaml
          echo "data:" >> secret.yaml
          echo "  DB_PASSWORD: $(echo -n '${{ secrets.DB_PASSWORD }}' | base64)" >> secret.yaml

          # Ingress 생성 (AWS Load Balancer Controller 사용)
          echo "apiVersion: networking.k8s.io/v1" > ingress.yaml
          echo "kind: Ingress" >> ingress.yaml
          echo "metadata:" >> ingress.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-ingress" >> ingress.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> ingress.yaml
          echo "  annotations:" >> ingress.yaml
          echo "    # AWS Application Load Balancer 설정" >> ingress.yaml
          echo "    kubernetes.io/ingress.class: alb" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/scheme: internet-facing" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/target-type: ip" >> ingress.yaml
          echo '    alb.ingress.kubernetes.io/listen-ports: '"'"'[{"HTTP": 80}]'"'"'' >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/subnets: ${{ env.PUBLIC_SUBNETS }}" >> ingress.yaml
          echo "    " >> ingress.yaml
          echo "    # Health Check 설정 (Spring Boot 애플리케이션)" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-path: ${{ env.HEALTH_CHECK_PATH }}" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '10'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthy-threshold-count: '2'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/unhealthy-threshold-count: '3'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-port: '8080'" >> ingress.yaml
          echo "    " >> ingress.yaml
          echo "    # Load Balancer 설정" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/load-balancer-name: ${{ env.PROJECT_NAME }}-ingress-alb" >> ingress.yaml
          echo '    alb.ingress.kubernetes.io/target-group-attributes: "stickiness.enabled=false,deregistration_delay.timeout_seconds=60,load_balancing.algorithm.type=round_robin,slow_start.duration_seconds=30"' >> ingress.yaml
          echo "spec:" >> ingress.yaml
          echo "  ingressClassName: alb" >> ingress.yaml
          echo "  rules:" >> ingress.yaml
          echo "  - http:" >> ingress.yaml
          echo "      paths:" >> ingress.yaml
          echo "      - path: /" >> ingress.yaml
          echo "        pathType: Prefix" >> ingress.yaml
          echo "        backend:" >> ingress.yaml
          echo "          service:" >> ingress.yaml
          echo "            name: ${{ env.PROJECT_NAME }}-service" >> ingress.yaml
          echo "            port:" >> ingress.yaml
          echo "              number: 8080" >> ingress.yaml

          echo "✅ Ingress 기반 매니페스트 생성 완료"

          # ServiceAccount 생성 (IRSA용)
          echo "apiVersion: v1" > serviceaccount.yaml
          echo "kind: ServiceAccount" >> serviceaccount.yaml
          echo "metadata:" >> serviceaccount.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-service-account" >> serviceaccount.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> serviceaccount.yaml
          echo "  annotations:" >> serviceaccount.yaml
          echo "    eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ env.IAM_ROLE_NAME }}" >> serviceaccount.yaml

          echo "✅ 매니페스트 파일 생성 완료"

      # ===============================================
      # EKS에 애플리케이션 배포 (main 또는 terraform 브랜치일 때만)
      # ===============================================
    - name: Deploy to EKS
      if: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
          echo "🚀 EKS에 애플리케이션 배포 중..."

          # 기존 k8s 매니페스트 적용
          echo "📦 기존 k8s 리소스 배포 중..."
          kubectl apply -f server2/k8s/namespace.yml
          kubectl apply -f server2/k8s/deployment.yml

          # 추가 리소스 배포  
          kubectl apply -f serviceaccount.yaml
          kubectl apply -f configmap.yaml
          kubectl apply -f secret.yaml

          # Ingress 배포 (AWS Load Balancer Controller로 ALB 생성)
          echo "🔗 Ingress 리소스 배포 중..."
          
          # Ingress 배포
          echo "🚀 Ingress 배포 시작..."
          if kubectl apply -f ingress.yaml; then
            echo "✅ Ingress 리소스 생성 성공"
            
            # Ingress와 ALB 동기화 상태 체크
            echo "🔍 Ingress와 ALB 동기화 상태 체크 중..."
            
            # webhook이 정상 작동하는지 확인
            if ! kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)" >/dev/null 2>&1; then
              echo "⚠️ Webhook이 없습니다. Controller 재시작 중..."
              kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
              kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=120s
              sleep 10
            fi
            
            # Ingress finalizer 상태 확인
            ingress_finalizers=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "")
            if [[ -z "$ingress_finalizers" ]]; then
              echo "⚠️ Ingress finalizer가 설정되지 않았습니다. AWS Load Balancer Controller 상태 확인 중..."
              kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} | grep -A 10 "Events:" || true
              
              # Controller 로그 확인
              echo "🔍 Controller 로그 확인:"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=10 || true
            else
              echo "✅ Ingress finalizer 설정됨: $ingress_finalizers"
            fi
            
            # ALB 생성 대기 (총 20분)
            echo "⏳ AWS Load Balancer Controller에 의한 ALB 생성 대기 중..."
            echo "ℹ️ ALB 생성 및 프로비저닝에는 5-10분 정도 소요될 수 있습니다."
            ALB_CREATED=false
            ALB_PROVISIONING=false
            for i in {1..40}; do
              ALB_DNS=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              
              if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "null" ]; then
                if [ "$ALB_PROVISIONING" != "true" ]; then
                  echo "🎯 ALB DNS 주소 할당 완료: $ALB_DNS"
                  echo "⏳ 이제 ALB 프로비저닝 및 헬스체크 완료를 기다립니다..."
                  ALB_PROVISIONING=true
                fi
                
                # ALB가 실제로 응답하는지 확인
                if curl -s --connect-timeout 5 --max-time 10 -o /dev/null -w "%{http_code}" "http://$ALB_DNS${{ env.HEALTH_CHECK_PATH }}" | grep -E "^(200|404)$" >/dev/null; then
                  echo "✅ ALB 프로비저닝 완료 및 연결 가능: $ALB_DNS"
                  echo "ALB_HOSTNAME=$ALB_DNS" >> $GITHUB_ENV
                  echo "DEPLOYMENT_METHOD=Ingress" >> $GITHUB_ENV
                  ALB_CREATED=true
                  break
                else
                  echo "⏳ ALB는 생성되었지만 아직 프로비저닝 중입니다... ($i/40)"
                fi
              else
                echo "⏳ ALB 생성 대기 중... ($i/40)"
              fi
              
              # 진행 상황 상세 확인 (3번마다)
              if [ $((i % 3)) -eq 0 ]; then
                echo "🔍 Ingress 상태 확인 (${i}/40)..."
                kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} || true
                
                # AWS Load Balancer Controller 상태 확인
                if [ $((i % 6)) -eq 0 ]; then
                  echo "🔍 AWS Load Balancer Controller Pod 상태:"
                  kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
                  
                  # 최근 이벤트 확인
                  echo "🔍 최근 Ingress 이벤트:"
                  kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} | grep -A 10 "Events:" || true
                fi
              fi
              
              sleep 30
            done
            
            if [ "$ALB_CREATED" != "true" ]; then
              echo "⚠️ ALB 생성/프로비저닝 시간 초과 (20분) - 상세 진단 실행"
              
              # 현재 시간 로깅
              echo "🕐 진단 시작 시간: $(date)"
              
              echo "🔍 최종 Ingress 상태:"
              kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o yaml || true
              kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} || true
              
              echo "🔍 Ingress finalizer 상태 재확인:"
              final_finalizers=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "NONE")
              echo "Finalizers: $final_finalizers"
              
              if [[ "$final_finalizers" == *"ingress.k8s.aws/resources"* ]]; then
                echo "✅ Finalizer가 설정되어 있음 - Controller가 인식했음"
              else
                echo "❌ Finalizer가 없음 - Controller가 Ingress를 처리하지 못했음"
              fi
              
              echo "🔍 AWS Load Balancer Controller 상세 상태:"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true
              kubectl describe pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
              
              echo "🔍 AWS Load Balancer Controller 최근 로그 (실패 관련):"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=200 | grep -i "error\|fail\|denied\|invalid\|timeout" || echo "관련 에러 로그 없음"
              
              echo "🔍 AWS Load Balancer Controller 전체 로그 (최근 50줄):"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 || true
              
              echo "🔍 IngressClass 확인:"
              kubectl get ingressclass || true
              kubectl describe ingressclass alb || true
              
              echo "🔍 ValidatingWebhookConfiguration 확인:"
              webhook_status=$(kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)" || echo "웹훅 없음")
              echo "Webhook 상태: $webhook_status"
              
              if [[ "$webhook_status" == "웹훅 없음" ]]; then
                echo "❌ ValidatingWebhookConfiguration이 없습니다!"
                echo "🔧 이는 Ingress 생성이 차단되지 않았지만 Controller가 처리하지 못했음을 의미합니다."
              else
                echo "✅ ValidatingWebhookConfiguration이 있습니다."
              fi
              
              echo "🔍 모든 네임스페이스의 Ingress 확인:"
              kubectl get ingress -A || true
              
              echo "🔍 AWS CLI를 통한 ALB 확인:"
              aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, \`${{ env.PROJECT_NAME }}\`) || contains(LoadBalancerName, \`k8s\`)].{Name:LoadBalancerName,State:State.Code,DNS:DNSName}" --output table 2>/dev/null || echo "AWS CLI로 ALB 조회 실패"
              
              # ALB가 생성되었지만 프로비저닝이 완료되지 않은 경우 환경변수 설정
              if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "null" ]; then
                echo "DEPLOYMENT_METHOD=Ingress-Provisioning" >> $GITHUB_ENV
                echo "ALB_HOSTNAME=$ALB_DNS" >> $GITHUB_ENV
                echo "ℹ️ ALB가 생성되었지만 프로비저닝이 완료되지 않았습니다: $ALB_DNS"
                echo "ℹ️ 수동으로 ALB 상태를 확인하거나 잠시 후 다시 시도해주세요."
              else
                echo "DEPLOYMENT_METHOD=Ingress-Failed" >> $GITHUB_ENV
                echo "ALB_HOSTNAME=failed" >> $GITHUB_ENV
                echo "❌ ALB 생성 자체가 실패했습니다."
              fi
            fi
            
          else
            echo "❌ Ingress 배포 실패"
            echo "🔍 진단 정보:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 || true
            kubectl get validatingwebhookconfigurations | grep -E "(vingress|aws-load-balancer)" || echo "웹훅 없음"
            kubectl get ingressclass || true
            exit 1
          fi
          
          echo "✅ Ingress 기반 ALB 배포 완료"

          echo "⏳ 배포 완료 대기 중..."
          kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s

      # ===============================================
      # 배포 결과 확인
      # ===============================================
    - name: Verify Deployment
      if: ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
          echo "🔍 배포 상태 확인 중..."
          echo "네임스페이스: ${{ env.PROJECT_NAME }}"
          echo "앱 라벨: ${{ env.PROJECT_NAME }}-app"

          # 네임스페이스 존재 확인
          if ! kubectl get namespace ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "❌ 네임스페이스 '${{ env.PROJECT_NAME }}'가 존재하지 않습니다."
            kubectl get namespaces
            exit 1
          fi

          # 기본 리소스 상태 확인
          echo "📋 Pod 상태:"
          kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide || echo "Pod를 찾을 수 없습니다."

          echo "📋 Service 상태:"
          kubectl get services -n ${{ env.PROJECT_NAME }} || echo "Service를 찾을 수 없습니다."

          echo "📋 Deployment 상태:"
          kubectl get deployments -n ${{ env.PROJECT_NAME }} || echo "Deployment를 찾을 수 없습니다."

          echo "📋 Ingress 상태:"
          kubectl get ingress -n ${{ env.PROJECT_NAME }} || echo "Ingress를 찾을 수 없습니다."

          echo "📋 AWS Load Balancer Controller 상태:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "AWS Load Balancer Controller를 찾을 수 없습니다."
          
          echo "🔧 AWS Load Balancer Controller 재시작 중..."
          kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
          kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=120s || true
          
          echo "✅ AWS Load Balancer Controller 재시작 완료"

          # Deployment 존재 확인 후 대기
          if kubectl get deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "⏳ Pod 준비 상태 대기 중..."
            kubectl wait --for=condition=ready pod -l app=${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || echo "Pod 준비 상태 대기 시간 초과"
            
            echo "📝 Deployment 상세 정보:"
            kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }}
            
          else
            echo "❌ Deployment '${{ env.PROJECT_NAME }}-app'를 찾을 수 없습니다."
            echo "사용 가능한 Deployment 목록:"
            kubectl get deployments -n ${{ env.PROJECT_NAME }}
            exit 1
          fi

      # ===============================================
      # 배포 방법별 접속 URL 제공
      # ===============================================
    - name: Get Application URL
      if: success() && ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
          echo "🔗 애플리케이션 접속 정보 확인 중..."
          echo "배포 방법: ${DEPLOYMENT_METHOD:-Unknown}"

          if [ "${DEPLOYMENT_METHOD:-}" = "Ingress" ]; then
            echo "✅ Ingress ALB 배포 완료!"
            echo "🌐 애플리케이션 접속 URL: http://${ALB_HOSTNAME:-확인불가}"
            
            # 실제 연결 테스트
            echo "🔍 ALB 연결 테스트 중..."
            if curl -s --connect-timeout 10 --max-time 15 -o /dev/null -w "%{http_code}" "http://${ALB_HOSTNAME}${{ env.HEALTH_CHECK_PATH }}" | grep -E "^(200|404)$" >/dev/null; then
              echo "✅ ALB 연결 성공 - 애플리케이션 접속 가능"
            else
              echo "⚠️ ALB 연결 실패 - 헬스체크 문제일 수 있습니다"
              echo "🔍 타겟 그룹 헬스 상태 확인을 권장합니다."
            fi
            
            echo "🔍 Ingress 상태:"
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Provisioning" ]; then
            echo "⚠️ ALB가 생성되었지만 아직 프로비저닝 중입니다"
            echo "🌐 ALB DNS: http://${ALB_HOSTNAME:-확인불가}"
            echo "ℹ️ 5-10분 후 접속이 가능할 예정입니다."
            
            echo "🔍 현재 ALB 연결 상태:"
            curl -s --connect-timeout 5 --max-time 10 -w "HTTP 상태코드: %{http_code}, 응답시간: %{time_total}s\n" "http://${ALB_HOSTNAME}${{ env.HEALTH_CHECK_PATH }}" || echo "아직 연결 불가"
            
            echo "🔍 Ingress 상태:"
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Failed" ]; then
            echo "❌ ALB 생성에 실패했습니다"
            echo "🔍 현재 Ingress 상태:"
            kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
            echo "🔧 문제 해결을 위한 체크리스트:"
            echo "1. AWS Load Balancer Controller가 정상 실행 중인지 확인"
            echo "2. IngressClass 'alb'가 존재하는지 확인"
            echo "3. AWS IAM 권한이 올바른지 확인"
            echo "4. 서브넷 태그가 올바른지 확인"
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Pending" ]; then
            echo "⚠️ ALB 생성이 아직 시작되지 않았습니다"
            echo "🔍 현재 Ingress 상태:"
            kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
            echo "🔧 AWS Load Balancer Controller 재시작 중..."
            kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
            kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=60s || true
            
            echo "🔍 AWS Load Balancer Controller 로그:"
            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20 || true
            
            echo "잠시 후 ALB가 생성되면 다음 명령어로 URL을 확인하세요:"
            echo "kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}"
            
          else
            echo "❓ 배포 방법을 확인할 수 없습니다."
            echo "배포 방법: ${DEPLOYMENT_METHOD:-Unknown}"
            echo "ALB 호스트명: ${ALB_HOSTNAME:-Unknown}"
            echo "사용 가능한 서비스 확인:"
            kubectl get services -n ${{ env.PROJECT_NAME }}
            echo "사용 가능한 Ingress 확인:"
            kubectl get ingress -n ${{ env.PROJECT_NAME }}
          fi

          # 공통 디버깅 정보
          echo ""
          echo "📋 전체 리소스 상태:"
          kubectl get all -n ${{ env.PROJECT_NAME }}

      # ===============================================
      # 배포 완료 알림
      # ===============================================
    - name: Application Deployment Notification
      if: success() && ${{ github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
          echo "🎉 Java Spring Boot 애플리케이션 배포 완료!"
          echo "프로젝트: ${{ env.PROJECT_NAME }}"
          echo "이미지: ${{ steps.build-image.outputs.image }}"
          echo "클러스터: ${{ env.EKS_CLUSTER_NAME }}"
          echo "네임스페이스: ${{ env.PROJECT_NAME }}"
          echo "데이터베이스: ${{ env.RDS_ENDPOINT }}"
          echo "커밋: ${{ github.sha }}"
          echo "배포 시간: $(date)"

          # 배포된 서비스 정보 출력
          echo "📋 배포된 리소스 목록:"
          kubectl get all -n ${{ env.PROJECT_NAME }} || echo "리소스 조회 실패"

      # ===============================================
      # 배포 실패 시 롤백 및 정리
      # ===============================================
    - name: Rollback on failure
      if: ${{ failure() && github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/terraform') }}
      run: |
          echo "❌ 배포 실패 - 롤백 및 정리 시작..."
          
          # Ingress finalizer 체크 및 정리 함수 재정의
          check_and_clean_ingress() {
            local namespace="$1"
            local ingress_name="$2"
            
            echo "🔍 실패한 Ingress '$ingress_name' (네임스페이스: $namespace) 정리 중..."
            
            # Ingress 존재 확인
            if ! kubectl get ingress "$ingress_name" -n "$namespace" >/dev/null 2>&1; then
              echo "ℹ️ Ingress '$ingress_name'이 존재하지 않습니다."
              return 0
            fi
            
            # finalizer 확인
            local finalizers=$(kubectl get ingress "$ingress_name" -n "$namespace" -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "")
            
            if [[ "$finalizers" == *"ingress.k8s.aws/resources"* ]]; then
              echo "⚠️ Ingress '$ingress_name'에 finalizer가 남아있습니다. 강제 정리 시작..."
              
              # validating webhook 삭제
              kubectl delete validatingwebhookconfigurations aws-load-balancer-webhook --timeout=10s 2>/dev/null || echo "validating webhook 없음"
              
              # mutating webhook 삭제
              kubectl delete mutatingwebhookconfigurations aws-load-balancer-webhook --timeout=10s 2>/dev/null || echo "mutating webhook 없음"
              
              # finalizer 강제 제거
              echo "🔧 finalizer 강제 제거 중..."
              if kubectl patch ingress "$ingress_name" -n "$namespace" -p '{"metadata":{"finalizers":[]}}' --type=merge; then
                echo "✅ finalizer 제거 성공"
                
                # Ingress 삭제
                kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s || echo "Ingress 삭제 실패"
                echo "✅ Ingress '$ingress_name' 정리 완료"
              else
                echo "⚠️ finalizer 제거 실패. 수동 확인 필요"
              fi
            else
              echo "✅ Ingress '$ingress_name'에 문제되는 finalizer 없음"
              # 정상 삭제 시도
              kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s || echo "Ingress 삭제 실패"
            fi
          }
          
          # 실패한 Ingress 정리
          if kubectl get ingress -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "🗑️ 실패한 Ingress 리소스 정리 중..."
            check_and_clean_ingress "${{ env.PROJECT_NAME }}" "${{ env.PROJECT_NAME }}-ingress"
          fi
          
          # 애플리케이션 롤백
          echo "🔄 애플리케이션 롤백 중..."
          kubectl rollout undo deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} || true
          kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || true
          
          # AWS Load Balancer Controller 복구
          echo "🔧 AWS Load Balancer Controller 복구 중..."
          kubectl rollout restart deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
          kubectl rollout status deployment -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --timeout=120s || true
          
          echo "✅ 롤백 및 정리 완료"