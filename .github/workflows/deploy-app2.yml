# .github/workflows/deploy-app2.yml
# Java Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì›Œí¬í”Œë¡œìš° (server2 í´ë” ë³€ê²½ì‹œë§Œ ì‹¤í–‰)

name: Deploy Java Spring Boot Application

on:
  push:
    branches: [ main ]
    paths:
      - 'WALB/server2/**'
      - '.github/workflows/deploy-app2.yml'

  
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    # ì›Œí‚¹ ë””ë ‰í† ë¦¬ë¥¼ WALBë¡œ ì„¤ì •
    defaults:
      run:
        working-directory: ./WALB
    
    permissions:
      id-token: write
      contents: read
    
    steps:
    # ===============================================
    # ì†ŒìŠ¤ì½”ë“œ ì²´í¬ì•„ì›ƒ
    # ===============================================
    - name: Checkout code
      uses: actions/checkout@v4
    
    # ===============================================
    # app2-config.ymlì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
    # ===============================================
    - name: Load config from app2-config.yml
      run: |
        echo "ğŸ“‹ app2-config.ymlì—ì„œ ì„¤ì •ê°’ ë¡œë“œ ì¤‘..."
        
        # yq ì„¤ì¹˜
        sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
        sudo chmod +x /usr/local/bin/yq
        
        # app2-config.ymlì—ì„œ ê°’ ì½ê¸°
        PROJECT_NAME=$(yq eval '.app2_config.project_name' ../.github/workflows/config/app2-config.yml)
        DB_NAME=$(yq eval '.app2_config.database.name' ../.github/workflows/config/app2-config.yml)
        DB_USER=$(yq eval '.app2_config.database.user' ../.github/workflows/config/app2-config.yml)
        DB_PORT=$(yq eval '.app2_config.database.port' ../.github/workflows/config/app2-config.yml)
        DB_RDS_IDENTIFIER=$(yq eval '.app2_config.database.rds_identifier' ../.github/workflows/config/app2-config.yml)
        EKS_CLUSTER_NAME=$(yq eval '.app2_config.eks.cluster_name' ../.github/workflows/config/app2-config.yml)
        S3_BUCKET_NAME=$(yq eval '.app2_config.s3.bucket_name' ../.github/workflows/config/app2-config.yml)
        IAM_ROLE_NAME=$(yq eval '.app2_config.iam.role_name' ../.github/workflows/config/app2-config.yml)
        BASTION_HOST_TAG=$(yq eval '.app2_config.bastion.host_tag_name' ../.github/workflows/config/app2-config.yml)
        DEPLOYMENT_NAME=$(yq eval '.app2_config.application.deployment_name' ../.github/workflows/config/app2-config.yml)
        IMAGE_NAME=$(yq eval '.app2_config.application.image_name' ../.github/workflows/config/app2-config.yml)
        HEALTH_CHECK_PATH=$(yq eval '.app2_config.application.health_check_path' ../.github/workflows/config/app2-config.yml)
        AWS_REGION=$(yq eval '.app2_config.network.region' ../.github/workflows/config/app2-config.yml)
        
        # í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
        echo "PROJECT_NAME=$PROJECT_NAME" >> $GITHUB_ENV
        echo "DB_NAME=$DB_NAME" >> $GITHUB_ENV
        echo "DB_USER=$DB_USER" >> $GITHUB_ENV
        echo "DB_PORT=$DB_PORT" >> $GITHUB_ENV
        echo "DB_RDS_IDENTIFIER=$DB_RDS_IDENTIFIER" >> $GITHUB_ENV
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
        echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV
        echo "IAM_ROLE_NAME=$IAM_ROLE_NAME" >> $GITHUB_ENV
        echo "BASTION_HOST_TAG=$BASTION_HOST_TAG" >> $GITHUB_ENV
        echo "DEPLOYMENT_NAME=$DEPLOYMENT_NAME" >> $GITHUB_ENV
        echo "IMAGE_NAME=$IMAGE_NAME" >> $GITHUB_ENV
        echo "HEALTH_CHECK_PATH=$HEALTH_CHECK_PATH" >> $GITHUB_ENV
        echo "AWS_REGION=$AWS_REGION" >> $GITHUB_ENV
        
        echo "âœ… ì„¤ì •ê°’ ë¡œë“œ ì™„ë£Œ"
        echo "PROJECT_NAME: $PROJECT_NAME"
        echo "DB_NAME: $DB_NAME"
        echo "AWS_REGION: $AWS_REGION"
    
    # ===============================================
    # Java ë° Maven í™˜ê²½ ì„¤ì •
    # ===============================================
    - name: Set up JDK 17
      uses: actions/setup-java@v4
      with:
        java-version: '17'
        distribution: 'temurin'
        cache: maven
    
    - name: Validate and Build Maven Project
      run: |
        echo "ğŸ” Java Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ ê²€ì¦ ì¤‘.."
        if [ -f "server2/pom.xml" ]; then
          cd server2
          echo "ğŸ“¦ Maven ì˜ì¡´ì„± ê²€ì¦ ë° ë¹Œë“œ ì¤‘..."
          mvn clean compile test package -DskipTests=false
          echo "âœ… Maven ë¹Œë“œ ì™„ë£Œ"
          ls -la target/
        else
          echo "âŒ pom.xml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
          exit 1
        fi
    
    # ===============================================
    # AWS ì¸ì¦ (OIDC ë°©ì‹)
    # ===============================================
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN_APP2 }}
        aws-region: ${{ secrets.AWS_REGION }}
        role-session-name: GitHubActions-Application-${{ github.run_id }}

    # ===============================================
    # ê¸°ì¡´ ì¸í”„ë¼ ì •ë³´ ì¡°íšŒ
    # ===============================================
    - name: Get Infrastructure Resources
      run: |
        echo "ğŸ” ê¸°ì¡´ ì¸í”„ë¼ ë¦¬ì†ŒìŠ¤ ì •ë³´ ì¡°íšŒ ì¤‘..."
        
        # ECR ë¦¬í¬ì§€í† ë¦¬ URI ì¡°íšŒ
        ECR_REPO=$(aws ecr describe-repositories --repository-names ${PROJECT_NAME}-ecr --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
        if [ -z "$ECR_REPO" ]; then
          echo "âŒ ECR ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${PROJECT_NAME}-ecr"
          echo "ë¨¼ì € ì¸í”„ë¼ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
          exit 1
        fi
        echo "ECR_REPOSITORY=$ECR_REPO" >> $GITHUB_ENV
        echo "âœ… ECR Repository: $ECR_REPO"
        
        # EKS í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì¡°íšŒ
        EKS_CLUSTER=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.name' --output text 2>/dev/null || echo "")
        if [ -z "$EKS_CLUSTER" ] || [ "$EKS_CLUSTER" == "None" ]; then
          echo "âŒ EKS í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${{ env.EKS_CLUSTER_NAME }}"
          echo "ë¨¼ì € ì¸í”„ë¼ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
          exit 1
        fi
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER" >> $GITHUB_ENV
        echo "âœ… EKS Cluster: $EKS_CLUSTER"
        
        # RDS ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ (walb2-app í”„ë¡œì íŠ¸ìš©)
        RDS_ENDPOINT=$(aws rds describe-db-instances \
          --db-instance-identifier ${{ env.DB_RDS_IDENTIFIER }} \
          --query "DBInstances[0].Endpoint.Address" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$RDS_ENDPOINT" ] || [ "$RDS_ENDPOINT" == "None" ]; then
          echo "âŒ RDS ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${{ env.DB_RDS_IDENTIFIER }}"
          echo "ì‚¬ìš© ê°€ëŠ¥í•œ RDS ì¸ìŠ¤í„´ìŠ¤:"
          aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
          exit 1
        fi
        echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
        echo "âœ… RDS Endpoint: $RDS_ENDPOINT"
        
        # EKS í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
        EKS_STATUS=$(aws eks describe-cluster --name $EKS_CLUSTER --query 'cluster.status' --output text)
        if [ "$EKS_STATUS" != "ACTIVE" ]; then
          echo "âŒ EKS í´ëŸ¬ìŠ¤í„°ê°€ í™œì„± ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: $EKS_STATUS"
          exit 1
        fi
        echo "âœ… EKS Cluster Status: $EKS_STATUS"

    # ===============================================
    # EKS ë³´ì•ˆê·¸ë£¹ ê·œì¹™ í™•ì¸ ë° ìë™ ìˆ˜ì • (kubelet í†µì‹ ìš©)
    # ===============================================
    - name: Check and fix EKS security group rules
      run: |
        echo "ğŸ” EKS ë³´ì•ˆê·¸ë£¹ 10250 í¬íŠ¸ ê·œì¹™ í™•ì¸ ì¤‘..."
        
        # ì›Œì»¤ë…¸ë“œ ë³´ì•ˆê·¸ë£¹ ID ì¡°íšŒ
        NODE_SG_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:kubernetes.io/cluster/${{ env.EKS_CLUSTER_NAME }},Values=owned" \
          --query "Reservations[*].Instances[*].SecurityGroups[*].GroupId" \
          --output text | tr '\t' '\n' | sort | uniq | head -1)
        
        # í´ëŸ¬ìŠ¤í„° ë³´ì•ˆê·¸ë£¹ ID ì¡°íšŒ  
        CLUSTER_SG_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} \
          --query "cluster.resourcesVpcConfig.securityGroupIds[0]" --output text)
        
        if [ -z "$NODE_SG_ID" ] || [ -z "$CLUSTER_SG_ID" ]; then
          echo "âš ï¸ ë³´ì•ˆê·¸ë£¹ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤."
          echo "NODE_SG_ID: $NODE_SG_ID"
          echo "CLUSTER_SG_ID: $CLUSTER_SG_ID"
        else
          echo "ğŸ“‹ ë³´ì•ˆê·¸ë£¹ ì •ë³´:"
          echo "  Worker Node SG: $NODE_SG_ID"
          echo "  Cluster SG: $CLUSTER_SG_ID"
          
          # 10250 í¬íŠ¸ ê·œì¹™ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
          RULE_EXISTS=$(aws ec2 describe-security-groups --group-ids $NODE_SG_ID \
            --query "SecurityGroups[0].GroupRules[?FromPort==\`10250\` && ToPort==\`10250\` && IsEgress==\`false\`]" \
            --output text)
          
          if [ -z "$RULE_EXISTS" ]; then
            echo "âš ï¸ kubelet í†µì‹ ìš© 10250 í¬íŠ¸ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ì¤‘..."
            
            # 10250 í¬íŠ¸ ì¸ë°”ìš´ë“œ ê·œì¹™ ì¶”ê°€ (í´ëŸ¬ìŠ¤í„°ì—ì„œ ë…¸ë“œë¡œ)
            aws ec2 authorize-security-group-ingress \
              --group-id $NODE_SG_ID \
              --protocol tcp \
              --port 10250 \
              --source-group $CLUSTER_SG_ID \
              --description "Allow kubelet communication from control plane (auto-added by GitHub Actions)" \
              2>/dev/null && echo "âœ… 10250 í¬íŠ¸ ê·œì¹™ ì¶”ê°€ ì™„ë£Œ" || echo "âš ï¸ ê·œì¹™ ì¶”ê°€ ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ)"
            
          else
            echo "âœ… kubelet í†µì‹ ìš© 10250 í¬íŠ¸ ê·œì¹™ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤"
          fi
        fi
    
    # ===============================================
    # ECR ë¡œê·¸ì¸
    # ===============================================
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # ===============================================
    # Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ
    # ===============================================
    - name: Build and push Docker image
      id: build-image
      run: |
        echo "ğŸ³ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
        
        # Git ì»¤ë°‹ í•´ì‹œë¥¼ íƒœê·¸ë¡œ ì‚¬ìš©
        IMAGE_TAG=${{ github.sha }}
        IMAGE_URI=${{ env.ECR_REPOSITORY }}:$IMAGE_TAG
        
        # server2 í´ë”ë¡œ ì´ë™í•´ì„œ Docker ë¹Œë“œ
        cd server2
        docker build -t $IMAGE_URI .
        docker tag $IMAGE_URI ${{ env.ECR_REPOSITORY }}:latest
        
        echo "ğŸ“¤ ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ ì¤‘..."
        docker push $IMAGE_URI
        docker push ${{ env.ECR_REPOSITORY }}:latest
        
        echo "âœ… ì´ë¯¸ì§€ í‘¸ì‹œ ì™„ë£Œ: $IMAGE_URI"
        echo "image=$IMAGE_URI" >> $GITHUB_OUTPUT

    - name: Test Database Connection via Bastion
      run: |
        # AWS CLIë¥¼ ì‚¬ìš©í•´ì„œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ì§ì ‘ ì¡°íšŒ
        PROJECT_NAME="${{ env.PROJECT_NAME }}"
        
        # RDS ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ (íƒœê·¸ ê¸°ë°˜)
        echo "ğŸ” RDS ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ ì¤‘..."
        DB_HOST=$(aws rds describe-db-instances \
          --query "DBInstances[?contains(keys(TagList[?Key=='Project']), 'Project') && TagList[?Key=='Project'].Value[0]=='${PROJECT_NAME}'].Endpoint.Address" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$DB_HOST" ]; then
          # íƒœê·¸ ì¡°íšŒê°€ ì•ˆ ë˜ë©´ ì§ì ‘ ì‹ë³„ìë¡œ ì¡°íšŒ
          DB_HOST=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.DB_RDS_IDENTIFIER }} \
            --query "DBInstances[0].Endpoint.Address" \
            --output text 2>/dev/null || echo "")
        fi
        
        # Bastion Host IP ì¡°íšŒ (íƒœê·¸ ê¸°ë°˜)
        echo "ğŸ” Bastion Host ì¡°íšŒ ì¤‘..."
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${{ env.BASTION_HOST_TAG }}" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          # íƒœê·¸ë¡œ ì•ˆ ë˜ë©´ ë³´ì•ˆê·¸ë£¹ìœ¼ë¡œ ì¡°íšŒ
          BASTION_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text 2>/dev/null || echo "")
        fi
        
        # DB ì‚¬ìš©ìëª…ê³¼ DB ì´ë¦„ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
        DB_NAME="${{ env.DB_NAME }}"
        DB_USER="${{ env.DB_USER }}"
        
        # Parameter Storeì—ì„œ DB íŒ¨ìŠ¤ì›Œë“œ ì¡°íšŒ
        echo "ğŸ” DB íŒ¨ìŠ¤ì›Œë“œ ì¡°íšŒ ì¤‘..."
        DB_PASSWORD=$(aws ssm get-parameter \
          --name "/${PROJECT_NAME}/rds/master-password" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text 2>/dev/null || echo "")
        
        # ê°’ ê²€ì¦
        if [ -z "$DB_HOST" ] || [ "$DB_HOST" == "None" ]; then
          echo "âŒ RDS ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‚¬ìš© ê°€ëŠ¥í•œ RDS ì¸ìŠ¤í„´ìŠ¤:"
          aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
          exit 1
        fi
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "âŒ Bastion Hostë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‹¤í–‰ ì¤‘ì¸ EC2 ì¸ìŠ¤í„´ìŠ¤:"
          aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" \
            --query "Reservations[*].Instances[*].[InstanceId,PublicIpAddress,Tags[?Key=='Name'].Value[0]]" \
            --output table
          exit 1
        fi
        
        if [ -z "$DB_PASSWORD" ]; then
          echo "âŒ DB íŒ¨ìŠ¤ì›Œë“œë¥¼ Parameter Storeì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          exit 1
        fi
        
        echo "âœ… DB Host: '$DB_HOST'"
        echo "âœ… Bastion IP: '$BASTION_IP'"
        echo "âœ… DB User: '$DB_USER'"
        echo "âœ… DB Name: '$DB_NAME'"
        
        # SSH í‚¤ë¥¼ Parameter Storeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        echo "ğŸ”‘ SSH í‚¤ ì¡°íšŒ ì¤‘..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        echo "ğŸ” SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        # SSH ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        if ! ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP "echo 'SSH connection successful'" 2>/dev/null; then
          echo "âŒ SSH ì—°ê²° ì‹¤íŒ¨. Bastion Host ìƒíƒœ í™•ì¸:"
          aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" \
            --query "Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress,PrivateIpAddress]" \
            --output table
          exit 1
        fi
        
        echo "ğŸ”— SSH í„°ë„ì„ í†µí•œ MySQL ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        
        # MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ (ë¨¼ì € ì„¤ì¹˜)
        if ! command -v mysql &> /dev/null; then
          echo "ğŸ“¦ MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ì¤‘..."
          sudo apt-get update -qq && sudo apt-get install -y mysql-client
        fi
        
        # ë¡œì»¬ í¬íŠ¸ê°€ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
        if netstat -tuln | grep -q ":${{ env.DB_PORT }} "; then
          echo "âš ï¸ í¬íŠ¸ ${{ env.DB_PORT }}ì´ ì´ë¯¸ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
          LOCAL_PORT=$((${{ env.DB_PORT }} + 1))
        else
          LOCAL_PORT=${{ env.DB_PORT }}
        fi
        
        echo "ğŸ”— SSH í„°ë„ ìƒì„± ì¤‘... (ë¡œì»¬ í¬íŠ¸: $LOCAL_PORT)"
        # SSH í„°ë„ ìƒì„± with ë” ë§ì€ ì˜µì…˜
        ssh -i bastion_key.pem \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ExitOnForwardFailure=yes \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=3 \
            -o ConnectTimeout=30 \
            -L $LOCAL_PORT:$DB_HOST:${{ env.DB_PORT }} \
            ec2-user@$BASTION_IP \
            -N &
        SSH_PID=$!
        
        # SSH í„°ë„ì´ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
        echo "â³ SSH í„°ë„ ì„¤ì • í™•ì¸ ì¤‘..."
        sleep 5
        
        # SSH í”„ë¡œì„¸ìŠ¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if ! kill -0 $SSH_PID 2>/dev/null; then
          echo "âŒ SSH í„°ë„ ìƒì„± ì‹¤íŒ¨"
          echo "SSH ì—°ê²° ë¡œê·¸ í™•ì¸:"
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "echo 'SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ'" || echo "SSH ê¸°ë³¸ ì—°ê²° ì‹¤íŒ¨"
          rm -f bastion_key.pem
          exit 1
        fi
        
        # í„°ë„ í¬íŠ¸ê°€ ì—´ë ¸ëŠ”ì§€ í™•ì¸
        echo "ğŸ” í„°ë„ í¬íŠ¸ í™•ì¸ ì¤‘..."
        for i in {1..30}; do
          if netstat -tuln | grep -q ":$LOCAL_PORT "; then
            echo "âœ… SSH í„°ë„ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤ (í¬íŠ¸: $LOCAL_PORT)"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "âŒ SSH í„°ë„ í¬íŠ¸ ì„¤ì • ì‹œê°„ ì´ˆê³¼"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi
          sleep 2
        done
        
        # Bastion Hostì—ì„œ ì§ì ‘ DB ì—°ê²° í…ŒìŠ¤íŠ¸
        echo "ğŸ” Bastion Hostì—ì„œ DB ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "
          # MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í™•ì¸
          if ! command -v mysql &> /dev/null; then
            echo 'ğŸ“¦ Bastionì— MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ì¤‘...'
            sudo yum update -y
            sudo yum install -y mysql
          fi
          
          # DB ì—°ê²° í…ŒìŠ¤íŠ¸
          echo 'ğŸ” Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸...'
          if mysql -h $DB_HOST -P ${{ env.DB_PORT }} -u $DB_USER -p'$DB_PASSWORD' $DB_NAME -e 'SELECT VERSION();' 2>/dev/null; then
            echo 'âœ… Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° ì„±ê³µ'
          else
            echo 'âŒ Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° ì‹¤íŒ¨'
            echo 'ğŸ” ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸:'
            telnet $DB_HOST ${{ env.DB_PORT }} < /dev/null 2>&1 | head -5
            echo 'ğŸ” ë³´ì•ˆ ê·¸ë£¹ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.'
            exit 1
          fi
        "
        
        # í„°ë„ì„ í†µí•œ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
        echo "ğŸ” SSH í„°ë„ì„ í†µí•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        DB_CONNECTED=false
        for i in {1..5}; do
          if mysql --protocol=TCP -h 127.0.0.1 -P $LOCAL_PORT -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "SELECT 1;" 2>/dev/null; then
            echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ (ì‹œë„ $i/5)"
            DB_CONNECTED=true
            break
          else
            echo "âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ $i/5)"
            # ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
            echo "ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:"
            mysql --protocol=TCP -h 127.0.0.1 -P $LOCAL_PORT -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "SELECT 1;" 2>&1 || true
            
            if [ $i -lt 5 ]; then
              echo "3ì´ˆ í›„ ì¬ì‹œë„..."
              sleep 3
            fi
          fi
        done
        
        if [ "$DB_CONNECTED" != "true" ]; then
          echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì¢… ì‹¤íŒ¨"
          echo "ğŸ” ì—°ê²° ì •ë³´ í™•ì¸:"
          echo "  ë¡œì»¬ í¬íŠ¸: $LOCAL_PORT"
          echo "  DB í˜¸ìŠ¤íŠ¸: $DB_HOST"
          echo "  DB ì‚¬ìš©ì: $DB_USER"
          echo "  DB ì´ë¦„: $DB_NAME"
          echo "ğŸ” ë„¤íŠ¸ì›Œí¬ ìƒíƒœ:"
          netstat -tuln | grep ":$LOCAL_PORT"
          echo "ğŸ” SSH í„°ë„ ìƒíƒœ:"
          ps aux | grep ssh | grep $BASTION_IP || echo "SSH í”„ë¡œì„¸ìŠ¤ ì—†ìŒ"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        # SSH í„°ë„ ì¢…ë£Œ ë° ì •ë¦¬
        echo "ğŸ§¹ SSH í„°ë„ ì •ë¦¬ ì¤‘..."
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem
        
        echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
      env:
        PROJECT_NAME: ${{ env.PROJECT_NAME }}
        
    # ===============================================
    # MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ë° ìŠ¤í‚¤ë§ˆ ì ìš©
    # ===============================================
    - name: Apply Database Schema
      if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
        echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì ìš© ì¤‘..."
        
        # MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜
        sudo apt-get update && sudo apt-get install -y mysql-client
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        PROJECT_NAME="${{ env.PROJECT_NAME }}"
        DB_HOST="${{ env.RDS_ENDPOINT }}"
        DB_NAME="${{ env.DB_NAME }}"
        DB_USER="${{ env.DB_USER }}"
        
        # Bastion Host IP ì¡°íšŒ
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${{ env.BASTION_HOST_TAG }}" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "âŒ Bastion Hostë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          exit 1
        fi
        
        echo "ğŸ” ì—°ê²° ì •ë³´:"
        echo "  RDS ì—”ë“œí¬ì¸íŠ¸: $DB_HOST"
        echo "  Bastion IP: $BASTION_IP"
        echo "  DB ì´ë¦„: $DB_NAME"
        echo "  DB ì‚¬ìš©ì: $DB_USER"
        
        # SSH í‚¤ë¥¼ Parameter Storeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        echo "ğŸ”‘ SSH í‚¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        # ë¡œì»¬ í¬íŠ¸ê°€ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
        if netstat -tuln | grep -q ":${{ env.DB_PORT }} "; then
          echo "âš ï¸ í¬íŠ¸ ${{ env.DB_PORT }}ì´ ì´ë¯¸ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
          LOCAL_PORT=$((${{ env.DB_PORT }} + 1))
        else
          LOCAL_PORT=${{ env.DB_PORT }}
        fi
        
        # SSH í„°ë„ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
        echo "ğŸ”— SSH í„°ë„ ìƒì„± ì¤‘... (ë¡œì»¬ í¬íŠ¸: $LOCAL_PORT)"
        ssh -i bastion_key.pem \
            -o StrictHostKeyChecking=no \
            -o UserKnownHostsFile=/dev/null \
            -o ExitOnForwardFailure=yes \
            -o ServerAliveInterval=60 \
            -o ServerAliveCountMax=3 \
            -o ConnectTimeout=30 \
            -L $LOCAL_PORT:$DB_HOST:${{ env.DB_PORT }} \
            ec2-user@$BASTION_IP \
            -N &
        SSH_PID=$!
        
        # í„°ë„ ì„¤ì • ëŒ€ê¸° ë° í™•ì¸
        echo "â³ SSH í„°ë„ ì„¤ì • í™•ì¸ ì¤‘..."
        sleep 5
        
        # SSH í„°ë„ ìƒíƒœ í™•ì¸
        if ! kill -0 $SSH_PID 2>/dev/null; then
          echo "âŒ SSH í„°ë„ ìƒì„± ì‹¤íŒ¨"
          echo "SSH ì—°ê²° ë¡œê·¸ í™•ì¸:"
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "echo 'SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ'" || echo "SSH ê¸°ë³¸ ì—°ê²° ì‹¤íŒ¨"
          rm -f bastion_key.pem
          exit 1
        fi
        
        # í„°ë„ í¬íŠ¸ê°€ ì—´ë ¸ëŠ”ì§€ í™•ì¸
        echo "ğŸ” í„°ë„ í¬íŠ¸ í™•ì¸ ì¤‘..."
        for i in {1..30}; do
          if netstat -tuln | grep -q ":$LOCAL_PORT "; then
            echo "âœ… SSH í„°ë„ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤ (í¬íŠ¸: $LOCAL_PORT)"
            break
          fi
          if [ $i -eq 30 ]; then
            echo "âŒ SSH í„°ë„ í¬íŠ¸ ì„¤ì • ì‹œê°„ ì´ˆê³¼"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi
          sleep 2
        done
        
        echo "âœ… SSH í„°ë„ ìƒì„± ì™„ë£Œ"
        
        # Bastion Hostì—ì„œ ì§ì ‘ DB ì—°ê²° í…ŒìŠ¤íŠ¸
        echo "ğŸ” Bastion Hostì—ì„œ DB ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no ec2-user@$BASTION_IP "
          # MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í™•ì¸
          if ! command -v mysql &> /dev/null; then
            echo 'ğŸ“¦ Bastionì— MySQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ì¤‘...'
            sudo yum update -y
            sudo yum install -y mysql
          fi
          
          # DB ì—°ê²° í…ŒìŠ¤íŠ¸
          echo 'ğŸ” Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸...'
          if mysql -h $DB_HOST -P ${{ env.DB_PORT }} -u $DB_USER -p'${{ secrets.DB_PASSWORD }}' $DB_NAME -e 'SELECT VERSION();' 2>/dev/null; then
            echo 'âœ… Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° ì„±ê³µ'
          else
            echo 'âŒ Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° ì‹¤íŒ¨'
            echo 'ğŸ” ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸:'
            nc -zv $DB_HOST ${{ env.DB_PORT }} 2>&1 || telnet $DB_HOST ${{ env.DB_PORT }} < /dev/null 2>&1 | head -5
            echo 'ğŸ” ë³´ì•ˆ ê·¸ë£¹ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.'
            exit 1
          fi
        "
        
        # ë¡œì»¬ í¬íŠ¸ë¥¼ í†µí•´ RDS ì—°ê²° í…ŒìŠ¤íŠ¸ (ì¬ì‹œë„ ë¡œì§)
        echo "ğŸ” SSH í„°ë„ì„ í†µí•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        DB_CONNECTED=false
        for i in {1..5}; do
          if mysql \
            --protocol=TCP \
            -h localhost \
            -P $LOCAL_PORT \
            -u "$DB_USER" \
            -p"${{ secrets.DB_PASSWORD }}" \
            "$DB_NAME" \
            -e "SELECT VERSION();" 2>/dev/null; then
            echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ (ì‹œë„ $i/5)"
            DB_CONNECTED=true
            break
          else
            echo "âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ $i/5)"
            # ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
            echo "ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:"
            mysql \
              --protocol=TCP \
              -h localhost \
              -P $LOCAL_PORT \
              -u "$DB_USER" \
              -p"${{ secrets.DB_PASSWORD }}" \
              "$DB_NAME" \
              -e "SELECT VERSION();" 2>&1 || true
            
            if [ $i -lt 5 ]; then
              echo "3ì´ˆ í›„ ì¬ì‹œë„..."
              sleep 3
            fi
          fi
        done
        
        if [ "$DB_CONNECTED" != "true" ]; then
          echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì¢… ì‹¤íŒ¨"
          echo "ğŸ” ì—°ê²° ì •ë³´:"
          echo "  ë¡œì»¬ í¬íŠ¸: $LOCAL_PORT"
          echo "  DB í˜¸ìŠ¤íŠ¸: $DB_HOST"
          echo "  DB ì‚¬ìš©ì: $DB_USER"
          echo "  DB ì´ë¦„: $DB_NAME"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰ (Java ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ìì²´ì ìœ¼ë¡œ DB ì—°ê²° ê´€ë¦¬)
        echo "ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸ ì™„ë£Œ"
        echo "â„¹ï¸ Java Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ìì²´ì ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤."

        # ì •ë¦¬ ì‘ì—…
        echo "ğŸ§¹ ì •ë¦¬ ì‘ì—… ì¤‘..."
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem

        echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì‘ì—… ì™„ë£Œ"

    # ===============================================
    # kubectl ë° Helm ì„¤ì¹˜
    # ===============================================
    - name: Install kubectl and Helm
      run: |
          echo "ğŸ”§ kubectl ë° Helm ì„¤ì¹˜ ì¤‘..."

          # kubectl ì„¤ì¹˜
          curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Helm ì„¤ì¹˜
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

          echo "âœ… kubectl ë° Helm ì„¤ì¹˜ ì™„ë£Œ"
          kubectl version --client
          helm version

    - name: Update kubeconfig for EKS
      run: |
          echo "ğŸ”§ EKS í´ëŸ¬ìŠ¤í„° kubeconfig ì—…ë°ì´íŠ¸ ì¤‘..."

          # í˜„ì¬ AWS ìê²© ì¦ëª… í™•ì¸
          echo "ğŸ” í˜„ì¬ AWS ìê²© ì¦ëª… í™•ì¸..."
          aws sts get-caller-identity

          # EKS í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì¡°íšŒ (configì—ì„œ ì½ì€ ê°’ ì‚¬ìš©)
          echo "ğŸ” EKS í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì¤‘..."
          
          # configì—ì„œ ì½ì€ í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì‚¬ìš©
          EKS_CLUSTER_NAME="${{ env.EKS_CLUSTER_NAME }}"
          echo "configì—ì„œ ì½ì€ í´ëŸ¬ìŠ¤í„° ì´ë¦„: $EKS_CLUSTER_NAME"

          # í´ëŸ¬ìŠ¤í„° ì¡´ì¬ í™•ì¸
          if ! aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} >/dev/null 2>&1; then
            echo "âŒ EKS í´ëŸ¬ìŠ¤í„° '$EKS_CLUSTER_NAME'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            echo "ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„° ëª©ë¡:"
            aws eks list-clusters --region ${{ secrets.AWS_REGION }}
            exit 1
          fi

          echo "âœ… EKS í´ëŸ¬ìŠ¤í„°: $EKS_CLUSTER_NAME"
          echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV

          # í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
          CLUSTER_STATUS=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.status' --output text)
          echo "í´ëŸ¬ìŠ¤í„° ìƒíƒœ: $CLUSTER_STATUS"

          if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
            echo "âŒ í´ëŸ¬ìŠ¤í„°ê°€ ACTIVE ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: $CLUSTER_STATUS"
            exit 1
          fi

          # IAM ì—­í• ê³¼ OIDC ê³µê¸‰ì ì •ë³´ í™•ì¸
          echo "ğŸ” IAM ì—­í• ê³¼ OIDC ì„¤ì • í™•ì¸ ì¤‘..."
          CLUSTER_OIDC_ISSUER=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.identity.oidc.issuer' --output text)
          echo "OIDC ë°œí–‰ì: $CLUSTER_OIDC_ISSUER"
          
          # kubeconfig ì—…ë°ì´íŠ¸ (í˜„ì¬ OIDC ìê²© ì¦ëª… ì‚¬ìš©)
          echo "ğŸ”§ kubeconfig ì—…ë°ì´íŠ¸ ì¤‘ (OIDC ìê²© ì¦ëª… ì‚¬ìš©)..."
          aws eks update-kubeconfig \
            --region ${{ secrets.AWS_REGION }} \
            --name "$EKS_CLUSTER_NAME" \
            --verbose

          # AWS ìê²© ì¦ëª… í™•ì¸
          echo "ğŸ” í˜„ì¬ AWS ìê²© ì¦ëª… í™•ì¸..."
          aws sts get-caller-identity
          
          # í´ëŸ¬ìŠ¤í„° ì—°ê²° í…ŒìŠ¤íŠ¸ (ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ í¬í•¨)
          echo "ğŸ” í´ëŸ¬ìŠ¤í„° ì—°ê²° í…ŒìŠ¤íŠ¸..."
          if ! kubectl cluster-info --request-timeout=30s; then
            echo "âŒ kubectl cluster-info ì‹¤íŒ¨. ì¶”ê°€ ì§„ë‹¨ ì •ë³´:"
            
            # kubectl ì„¤ì • í™•ì¸
            echo "kubectl ì„¤ì • í™•ì¸:"
            kubectl config view --minify
            
            # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
            echo "í˜„ì¬ ì»¨í…ìŠ¤íŠ¸:"
            kubectl config current-context
            
            # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í…ŒìŠ¤íŠ¸
            CLUSTER_ENDPOINT=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.endpoint' --output text)
            echo "í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸: $CLUSTER_ENDPOINT"
            
            # kubeconfig ë‹¤ì‹œ ì„¤ì •
            echo "ğŸ”§ kubeconfig ì¬ì„¤ì •..."
            aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name "$EKS_CLUSTER_NAME" --verbose
            
            # aws-auth ConfigMap ìƒíƒœ í™•ì¸
            echo "ğŸ” aws-auth ConfigMap í™•ì¸..."
            kubectl get configmap aws-auth -n kube-system -o yaml || echo "aws-auth ConfigMap ì—†ìŒ"
            
            exit 1
          fi

          # aws-auth ConfigMap ìƒíƒœ í™•ì¸ (ì •ë³´ì„±)
          echo "ğŸ” aws-auth ConfigMap ìƒíƒœ í™•ì¸..."
          kubectl get configmap aws-auth -n kube-system -o yaml | head -20 || echo "aws-auth ConfigMap ì¡°íšŒ ì‹¤íŒ¨"
          
          echo "âœ… EKS í´ëŸ¬ìŠ¤í„° ì—°ê²° ì„±ê³µ"

          echo "ğŸ” ë…¸ë“œ ìƒíƒœ í™•ì¸..."
          kubectl get nodes --show-labels

      # ===============================================
      # ê¸°ì¡´ k8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ë° ì‚¬ìš©
      # ===============================================
    - name: Update existing Kubernetes manifests
      run: |
          echo "ğŸ”§ ê¸°ì¡´ k8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘..."
          cd server2
          cd k8s
          
          # namespace.yml ì—…ë°ì´íŠ¸ (í”„ë¡œì íŠ¸ëª… ì‚¬ìš©)
          sed -i "s/music1-namespace/${{ env.PROJECT_NAME }}/g" namespace.yml
          
          # deployment.yml ì—…ë°ì´íŠ¸ (ì‹¤ì œ íŒŒì¼ ë‚´ìš©ì— ë§ê²Œ ìˆ˜ì •)
          sed -i "s|\${ECR_REGISTRY}/walb2-app-ecr:\${IMAGE_TAG}|${{ steps.build-image.outputs.image }}|g" deployment.yml
          
          # Spring Boot Actuator í—¬ìŠ¤ì²´í¬ë¡œ ë³€ê²½
          sed -i "s|path: /api/music|path: ${{ env.HEALTH_CHECK_PATH }}|g" deployment.yml
          
          # Serviceë¥¼ ClusterIPë¡œ ë³€ê²½ (Ingress ì‚¬ìš©ì„ ìœ„í•´)
          sed -i "s/type: LoadBalancer/type: ClusterIP/g" deployment.yml
          
          # ServiceAccount ì¶”ê°€ (containers spec í•˜ìœ„ì—)
          sed -i "/containers:/i\\      serviceAccountName: ${{ env.PROJECT_NAME }}-service-account" deployment.yml
          
          # envFrom ì„¤ì • ì¶”ê°€ (í™˜ê²½ë³€ìˆ˜ ì£¼ì…) - ports ì„¹ì…˜ ë’¤ì— ì¶”ê°€
          sed -i "/          name: http/a\\        envFrom:" deployment.yml
          sed -i "/        envFrom:/a\\        - configMapRef:" deployment.yml
          sed -i "/        - configMapRef:/a\\            name: ${{ env.PROJECT_NAME }}-config" deployment.yml
          sed -i "/            name: ${{ env.PROJECT_NAME }}-config/a\\        - secretRef:" deployment.yml
          sed -i "/        - secretRef:/a\\            name: ${{ env.PROJECT_NAME }}-secret" deployment.yml
          
          echo "âœ… ê¸°ì¡´ k8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ"
          
          # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ í™•ì¸
          echo "ğŸ“‹ ì—…ë°ì´íŠ¸ëœ namespace.yml:"
          cat namespace.yml
          echo "ğŸ“‹ ì—…ë°ì´íŠ¸ëœ deployment.yml:"
          cat deployment.yml

    - name: Get Subnet Information
      run: |
          echo "ğŸ” ì„œë¸Œë„· ì •ë³´ ì¡°íšŒ ì¤‘..."
          
          # í¼ë¸”ë¦­ ì„œë¸Œë„· ì¡°íšŒ (ALBìš©) - ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
          echo "ğŸ” í¼ë¸”ë¦­ ì„œë¸Œë„· ì¡°íšŒ ì¤‘..."
          
          # EKS í´ëŸ¬ìŠ¤í„°ì˜ VPC ì¡°íšŒ
          echo "ğŸ” EKS í´ëŸ¬ìŠ¤í„° VPC ì¡°íšŒ ì¤‘..."
          EKS_VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text 2>/dev/null || echo "")
          
          if [ -z "$EKS_VPC_ID" ] || [ "$EKS_VPC_ID" == "None" ]; then
            echo "âš ï¸ EKS í´ëŸ¬ìŠ¤í„° VPCë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ê³„ì •ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
            VPC_FILTER=""
          else
            echo "âœ… EKS VPC ë°œê²¬: $EKS_VPC_ID"
            VPC_FILTER="Name=vpc-id,Values=$EKS_VPC_ID"
          fi
          
          # ë°©ë²• 1: Type=public íƒœê·¸ë¡œ ì¡°íšŒ (EKS VPC ë‚´ì—ì„œ, ìµœëŒ€ 3ê°œ)
          if [ -n "$VPC_FILTER" ]; then
            PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=tag:Type,Values=public" "Name=state,Values=available" "$VPC_FILTER" \
              --query "Subnets[:3].SubnetId" \
              --output text | tr '\t' ',' | sed 's/,$//')
          else
            PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=tag:Type,Values=public" "Name=state,Values=available" \
              --query "Subnets[:3].SubnetId" \
              --output text | tr '\t' ',' | sed 's/,$//')
          fi
          
          if [ -z "$PUBLIC_SUBNETS" ]; then
            # ë°©ë²• 2: kubernetes.io/role/elb íƒœê·¸ë¡œ ì¡°íšŒ (EKS VPC ë‚´ì—ì„œ)
            echo "âš ï¸ Type=public íƒœê·¸ë¡œ ì°¾ì§€ ëª»í•¨. kubernetes.io/role/elb íƒœê·¸ë¡œ ì¬ì‹œë„..."
            if [ -n "$VPC_FILTER" ]; then
              PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
                --filters "Name=tag:kubernetes.io/role/elb,Values=1" "Name=state,Values=available" "$VPC_FILTER" \
                --query "Subnets[:3].SubnetId" \
                --output text | tr '\t' ',' | sed 's/,$//')
            else
              PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
                --filters "Name=tag:kubernetes.io/role/elb,Values=1" "Name=state,Values=available" \
                --query "Subnets[:3].SubnetId" \
                --output text | tr '\t' ',' | sed 's/,$//')
            fi
          fi
          
          if [ -z "$PUBLIC_SUBNETS" ]; then
            # ë°©ë²• 3: ë¼ìš°íŠ¸ í…Œì´ë¸”ì„ í†µí•´ ì¸í„°ë„· ê²Œì´íŠ¸ì›¨ì´ê°€ ìˆëŠ” ì„œë¸Œë„· ì°¾ê¸° (EKS VPC ë‚´ì—ì„œ)
            echo "âš ï¸ íƒœê·¸ë¡œ ì°¾ì§€ ëª»í•¨. ë¼ìš°íŠ¸ í…Œì´ë¸” ë¶„ì„ìœ¼ë¡œ í¼ë¸”ë¦­ ì„œë¸Œë„· ì°¾ëŠ” ì¤‘..."
            if [ -n "$EKS_VPC_ID" ]; then
              PUBLIC_SUBNETS_RAW=$(aws ec2 describe-route-tables \
                --filters "Name=route.gateway-id,Values=igw-*" "Name=vpc-id,Values=$EKS_VPC_ID" \
                --query "RouteTables[*].Associations[?SubnetId!=null].SubnetId" \
                --output text | tr '\n' ' ' | tr '\t' ' ')
            else
              PUBLIC_SUBNETS_RAW=$(aws ec2 describe-route-tables \
                --filters "Name=route.gateway-id,Values=igw-*" \
                --query "RouteTables[*].Associations[?SubnetId!=null].SubnetId" \
                --output text | tr '\n' ' ' | tr '\t' ' ')
            fi
            
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œë¡œ ì œí•œ
            PUBLIC_SUBNETS=$(echo "$PUBLIC_SUBNETS_RAW" | tr ' ' '\n' | grep -v '^$' | sort | uniq | head -3 | tr '\n' ',' | sed 's/,$//')
          fi
          
          if [ -z "$PUBLIC_SUBNETS" ]; then
            # ë°©ë²• 4: ëª¨ë“  í¼ë¸”ë¦­ ì„œë¸Œë„·ì—ì„œ ê°€ìš©ì˜ì—­ë³„ë¡œ 1ê°œì”© ì„ íƒ
            echo "âš ï¸ íƒœê·¸ ê¸°ë°˜ ì¡°íšŒ ì‹¤íŒ¨. ëª¨ë“  í¼ë¸”ë¦­ ì„œë¸Œë„·ì—ì„œ ê°€ìš©ì˜ì—­ë³„ë¡œ ì„ íƒ ì¤‘..."
            
            # EKS VPC ë‚´ì—ì„œ ìš°ì„  ì¡°íšŒ, ì‹¤íŒ¨í•˜ë©´ ì „ì²´ ê³„ì •ì—ì„œ ì¡°íšŒ
            VPC_FILTER_OPTION=""
            if [ -n "$EKS_VPC_ID" ] && [ "$EKS_VPC_ID" != "None" ]; then
              VPC_FILTER_OPTION="Name=vpc-id,Values=$EKS_VPC_ID"
              echo "ğŸ” EKS VPC($EKS_VPC_ID) ë‚´ì—ì„œ í¼ë¸”ë¦­ ì„œë¸Œë„· ì¡°íšŒ ì¤‘..."
            else
              echo "ğŸ” ì „ì²´ ê³„ì •ì—ì„œ í¼ë¸”ë¦­ ì„œë¸Œë„· ì¡°íšŒ ì¤‘..."
            fi
            
            # ëª¨ë“  ì„œë¸Œë„· ì¡°íšŒ (State=availableë§Œ)
            ALL_SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=state,Values=available" $VPC_FILTER_OPTION \
              --query "Subnets[*].[SubnetId,AvailabilityZone,MapPublicIpOnLaunch]" \
              --output text)
            
            if [ -n "$ALL_SUBNETS" ]; then
              UNIQUE_AZ_SUBNETS=""
              USED_AZS=""
              
              echo "$ALL_SUBNETS" | while read subnet_id az map_public; do
                # MapPublicIpOnLaunchê°€ trueì¸ ì„œë¸Œë„·ë§Œ (í¼ë¸”ë¦­ ì„œë¸Œë„·)
                if [ "$map_public" = "True" ] && [[ ! "$USED_AZS" == *"$az"* ]]; then
                  UNIQUE_AZ_SUBNETS="$UNIQUE_AZ_SUBNETS $subnet_id"
                  USED_AZS="$USED_AZS $az"
                  echo "âœ… ê°€ìš©ì˜ì—­ $azì—ì„œ ì„œë¸Œë„· $subnet_id ì„ íƒ"
                  
                  # ìµœëŒ€ 3ê°œ AZê¹Œì§€ë§Œ
                  if [ $(echo $UNIQUE_AZ_SUBNETS | wc -w) -ge 3 ]; then
                    break
                  fi
                fi
              done
              
              if [ -n "$UNIQUE_AZ_SUBNETS" ]; then
                PUBLIC_SUBNETS=$(echo "$UNIQUE_AZ_SUBNETS" | tr ' ' ',' | sed 's/^,//' | sed 's/,$//')
                echo "âœ… ìë™ ì„ íƒëœ í¼ë¸”ë¦­ ì„œë¸Œë„·: $PUBLIC_SUBNETS"
              fi
            fi
            
            if [ -z "$PUBLIC_SUBNETS" ]; then
              echo "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í¼ë¸”ë¦­ ì„œë¸Œë„·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
              echo "ğŸ” VPCì— í¼ë¸”ë¦­ ì„œë¸Œë„·ì´ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
              echo "ğŸ” ë˜ëŠ” ì„œë¸Œë„·ì— ë‹¤ìŒ íƒœê·¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:"
              echo "   - Type=public ë˜ëŠ”"
              echo "   - kubernetes.io/role/elb=1"
              exit 1
            fi
          else
            echo "âœ… í¼ë¸”ë¦­ ì„œë¸Œë„· ë°œê²¬ (VPC: ${EKS_VPC_ID:-ì „ì²´}): $PUBLIC_SUBNETS"
          fi
          
          # í™˜ê²½ë³€ìˆ˜ ì•ˆì „í•˜ê²Œ ì €ì¥
          if [ -n "$PUBLIC_SUBNETS" ] && [ "$PUBLIC_SUBNETS" != "null" ]; then
            # ì„œë¸Œë„· ê°œìˆ˜ í™•ì¸
            SUBNET_COUNT=$(echo "$PUBLIC_SUBNETS" | tr ',' '\n' | wc -l)
            echo "ğŸ“Š ì„ íƒëœ ì„œë¸Œë„· ê°œìˆ˜: $SUBNET_COUNTê°œ"
            echo "ğŸ“‹ ì„œë¸Œë„· ëª©ë¡: $PUBLIC_SUBNETS"
            
            # GITHUB_ENVì— ì•ˆì „í•˜ê²Œ ì €ì¥
            echo "PUBLIC_SUBNETS=$PUBLIC_SUBNETS" >> "$GITHUB_ENV"
          else
            echo "âŒ ìœ íš¨í•œ í¼ë¸”ë¦­ ì„œë¸Œë„·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            exit 1
          fi

    - name: Generate additional Kubernetes manifests
      run: |
          echo "ğŸ“ ì¶”ê°€ Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì¤‘..."
          
          # ConfigMap ìƒì„± (í™˜ê²½ë³€ìˆ˜)
          echo "apiVersion: v1" > configmap.yaml
          echo "kind: ConfigMap" >> configmap.yaml
          echo "metadata:" >> configmap.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-config" >> configmap.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> configmap.yaml
          echo "data:" >> configmap.yaml
          echo '  DB_HOST: "${{ env.RDS_ENDPOINT }}"' >> configmap.yaml
          echo '  DB_PORT: "${{ env.DB_PORT }}"' >> configmap.yaml
          echo '  DB_NAME: "${{ env.DB_NAME }}"' >> configmap.yaml
          echo '  DB_USER: "${{ env.DB_USER }}"' >> configmap.yaml
          echo '  AWS_REGION: "${{ secrets.AWS_REGION }}"' >> configmap.yaml
          echo '  AWS_S3_BUCKET: "${{ env.S3_BUCKET_NAME }}"' >> configmap.yaml
          echo '  AWS_S3_REGION: "${{ secrets.AWS_REGION }}"' >> configmap.yaml
          echo '  STORAGE_TYPE: "s3"' >> configmap.yaml
          echo '  SPRING_PROFILES_ACTIVE: "production"' >> configmap.yaml
          echo '  JAVA_OPTS: "-Xmx512m -Xms256m"' >> configmap.yaml
          echo '  TZ: "Asia/Seoul"' >> configmap.yaml
          echo '  SERVER_PORT: "8080"' >> configmap.yaml
          echo '  MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: "health,info,prometheus"' >> configmap.yaml
        
          # Secret ìƒì„± (DB íŒ¨ìŠ¤ì›Œë“œ)
          echo "apiVersion: v1" > secret.yaml
          echo "kind: Secret" >> secret.yaml
          echo "metadata:" >> secret.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-secret" >> secret.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> secret.yaml
          echo "type: Opaque" >> secret.yaml
          echo "data:" >> secret.yaml
          echo "  DB_PASSWORD: $(echo -n '${{ secrets.DB_PASSWORD }}' | base64)" >> secret.yaml

          # Ingress ìƒì„± (AWS Load Balancer Controller ì‚¬ìš©)
          echo "apiVersion: networking.k8s.io/v1" > ingress.yaml
          echo "kind: Ingress" >> ingress.yaml
          echo "metadata:" >> ingress.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-ingress" >> ingress.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> ingress.yaml
          echo "  annotations:" >> ingress.yaml
          echo "    # AWS Application Load Balancer ì„¤ì •" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/scheme: internet-facing" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/target-type: ip" >> ingress.yaml
          echo '    alb.ingress.kubernetes.io/listen-ports: '"'"'[{"HTTP": 80}]'"'"'' >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/subnets: ${{ env.PUBLIC_SUBNETS }}" >> ingress.yaml
          echo "    " >> ingress.yaml
          echo "    # Health Check ì„¤ì • (Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜)" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-path: ${{ env.HEALTH_CHECK_PATH }}" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '10'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthy-threshold-count: '2'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/unhealthy-threshold-count: '3'" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/healthcheck-port: '80'" >> ingress.yaml
          echo "    " >> ingress.yaml
          echo "    # Load Balancer ì„¤ì •" >> ingress.yaml
          echo "    alb.ingress.kubernetes.io/load-balancer-name: ${{ env.PROJECT_NAME }}-ingress-alb" >> ingress.yaml
          echo '    alb.ingress.kubernetes.io/target-group-attributes: "stickiness.enabled=false,deregistration_delay.timeout_seconds=60,load_balancing.algorithm.type=round_robin,slow_start.duration_seconds=30"' >> ingress.yaml
          echo "spec:" >> ingress.yaml
          echo "  ingressClassName: alb" >> ingress.yaml
          echo "  rules:" >> ingress.yaml
          echo "  - http:" >> ingress.yaml
          echo "      paths:" >> ingress.yaml
          echo "      - path: /" >> ingress.yaml
          echo "        pathType: Prefix" >> ingress.yaml
          echo "        backend:" >> ingress.yaml
          echo "          service:" >> ingress.yaml
          echo "            name: ${{ env.PROJECT_NAME }}-service" >> ingress.yaml
          echo "            port:" >> ingress.yaml
          echo "              number: 80" >> ingress.yaml

          echo "âœ… Ingress ê¸°ë°˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ"

          # ServiceAccount ìƒì„± (IRSAìš©)
          echo "apiVersion: v1" > serviceaccount.yaml
          echo "kind: ServiceAccount" >> serviceaccount.yaml
          echo "metadata:" >> serviceaccount.yaml
          echo "  name: ${{ env.PROJECT_NAME }}-service-account" >> serviceaccount.yaml
          echo "  namespace: ${{ env.PROJECT_NAME }}" >> serviceaccount.yaml
          echo "  annotations:" >> serviceaccount.yaml
          echo "    eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ env.IAM_ROLE_NAME }}" >> serviceaccount.yaml

          echo "âœ… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ"

      # ===============================================
      # EKSì— ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ (main ë¸Œëœì¹˜ì¼ ë•Œë§Œ)
      # ===============================================
    - name: Deploy to EKS
      if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
          echo "ğŸš€ EKSì— ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì¤‘..."

          # k8s ë¦¬ì†ŒìŠ¤ ì˜ì¡´ì„± ìˆœì„œëŒ€ë¡œ ë°°í¬
          echo "ğŸ“¦ k8s ë¦¬ì†ŒìŠ¤ ë°°í¬ ì¤‘..."
          
          # 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¨¼ì € ìƒì„±
          kubectl apply -f server2/k8s/namespace.yml
          
          # 2. ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ë“¤ ìƒì„± (deploymentê°€ ì°¸ì¡°í•˜ëŠ” ë¦¬ì†ŒìŠ¤ë“¤)
          kubectl apply -f serviceaccount.yaml
          kubectl apply -f configmap.yaml
          kubectl apply -f secret.yaml
          
          # 3. ë§ˆì§€ë§‰ì— deployment ìƒì„± (ëª¨ë“  ì˜ì¡´ì„± ë¦¬ì†ŒìŠ¤ ì¤€ë¹„ ì™„ë£Œ í›„)
          kubectl apply -f server2/k8s/deployment.yml

          # Ingress ë°°í¬ (AWS Load Balancer Controllerë¡œ ALB ìƒì„±)
          echo "ğŸ”— Ingress ë¦¬ì†ŒìŠ¤ ë°°í¬ ì¤‘..."
          
          # Ingress ë°°í¬
          echo "ğŸš€ Ingress ë°°í¬ ì‹œì‘..."
          if kubectl apply -f ingress.yaml; then
            echo "âœ… Ingress ë¦¬ì†ŒìŠ¤ ìƒì„± ì„±ê³µ"
            
            # Ingressì™€ ALB ë™ê¸°í™” ìƒíƒœ ì²´í¬
            echo "ğŸ” Ingressì™€ ALB ë™ê¸°í™” ìƒíƒœ ì²´í¬ ì¤‘..."
            
            # Ingress finalizer ìƒíƒœ í™•ì¸
            ingress_finalizers=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "")
            if [[ -z "$ingress_finalizers" ]]; then
              echo "âš ï¸ Ingress finalizerê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AWS Load Balancer Controller ìƒíƒœ í™•ì¸ ì¤‘..."
              kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} | grep -A 10 "Events:" || true
              
              # Controller ë¡œê·¸ í™•ì¸
              echo "ğŸ” Controller ë¡œê·¸ í™•ì¸:"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=10 || true
            else
              echo "âœ… Ingress finalizer ì„¤ì •ë¨: $ingress_finalizers"
            fi
            
            # ALB ìƒì„± ëŒ€ê¸° (ì´ 20ë¶„)
            echo "â³ AWS Load Balancer Controllerì— ì˜í•œ ALB ìƒì„± ëŒ€ê¸° ì¤‘..."
            echo "â„¹ï¸ ALB ìƒì„± ë° í”„ë¡œë¹„ì €ë‹ì—ëŠ” 5-10ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ALB_CREATED=false
            ALB_PROVISIONING=false
            for i in {1..40}; do
              ALB_DNS=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              
              if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "null" ]; then
                if [ "$ALB_PROVISIONING" != "true" ]; then
                  echo "ğŸ¯ ALB DNS ì£¼ì†Œ í• ë‹¹ ì™„ë£Œ: $ALB_DNS"
                  echo "â³ ì´ì œ ALB í”„ë¡œë¹„ì €ë‹ ë° í—¬ìŠ¤ì²´í¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤..."
                  ALB_PROVISIONING=true
                fi
                
                # ALBê°€ ì‹¤ì œë¡œ ì‘ë‹µí•˜ëŠ”ì§€ í™•ì¸
                if curl -s --connect-timeout 5 --max-time 10 -o /dev/null -w "%{http_code}" "http://$ALB_DNS${{ env.HEALTH_CHECK_PATH }}" | grep -E "^(200|404)$" >/dev/null; then
                  echo "âœ… ALB í”„ë¡œë¹„ì €ë‹ ì™„ë£Œ ë° ì—°ê²° ê°€ëŠ¥: $ALB_DNS"
                  echo "ALB_HOSTNAME=$ALB_DNS" >> $GITHUB_ENV
                  echo "DEPLOYMENT_METHOD=Ingress" >> $GITHUB_ENV
                  ALB_CREATED=true
                  break
                else
                  echo "â³ ALBëŠ” ìƒì„±ë˜ì—ˆì§€ë§Œ ì•„ì§ í”„ë¡œë¹„ì €ë‹ ì¤‘ì…ë‹ˆë‹¤... ($i/40)"
                fi
              else
                echo "â³ ALB ìƒì„± ëŒ€ê¸° ì¤‘... ($i/40)"
              fi
              
              # ì§„í–‰ ìƒí™© ìƒì„¸ í™•ì¸ (3ë²ˆë§ˆë‹¤)
              if [ $((i % 3)) -eq 0 ]; then
                echo "ğŸ” Ingress ìƒíƒœ í™•ì¸ (${i}/40)..."
                kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} || true
                
                # AWS Load Balancer Controller ìƒíƒœ í™•ì¸
                if [ $((i % 6)) -eq 0 ]; then
                  echo "ğŸ” AWS Load Balancer Controller Pod ìƒíƒœ:"
                  kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
                  
                  # ìµœê·¼ ì´ë²¤íŠ¸ í™•ì¸
                  echo "ğŸ” ìµœê·¼ Ingress ì´ë²¤íŠ¸:"
                  kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} | grep -A 10 "Events:" || true
                fi
              fi
              
              sleep 30
            done
            
            if [ "$ALB_CREATED" != "true" ]; then
              echo "âš ï¸ ALB ìƒì„±/í”„ë¡œë¹„ì €ë‹ ì‹œê°„ ì´ˆê³¼ (20ë¶„) - ìƒì„¸ ì§„ë‹¨ ì‹¤í–‰"
              
              # í˜„ì¬ ì‹œê°„ ë¡œê¹…
              echo "ğŸ• ì§„ë‹¨ ì‹œì‘ ì‹œê°„: $(date)"
              
              echo "ğŸ” ìµœì¢… Ingress ìƒíƒœ:"
              kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o yaml || true
              kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} || true
              
              echo "ğŸ” Ingress finalizer ìƒíƒœ ì¬í™•ì¸:"
              final_finalizers=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "NONE")
              echo "Finalizers: $final_finalizers"
              
              if [[ "$final_finalizers" == *"ingress.k8s.aws/resources"* ]]; then
                echo "âœ… Finalizerê°€ ì„¤ì •ë˜ì–´ ìˆìŒ - Controllerê°€ ì¸ì‹í–ˆìŒ"
              else
                echo "âŒ Finalizerê°€ ì—†ìŒ - Controllerê°€ Ingressë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŒ"
              fi
              
              echo "ğŸ” AWS Load Balancer Controller ìƒì„¸ ìƒíƒœ:"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true
              kubectl describe pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
              
              echo "ğŸ” AWS Load Balancer Controller App2 ìµœê·¼ ë¡œê·¸ (ì‹¤íŒ¨ ê´€ë ¨):"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=200 | grep -i "error\|fail\|denied\|invalid\|timeout" || echo "ê´€ë ¨ ì—ëŸ¬ ë¡œê·¸ ì—†ìŒ"
              
              echo "ğŸ” AWS Load Balancer Controller App2 ì „ì²´ ë¡œê·¸ (ìµœê·¼ 50ì¤„):"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 || true
              
              echo "ğŸ” IngressClass í™•ì¸:"
              kubectl get ingressclass || true
              kubectl describe ingressclass alb || true
              
              echo "ğŸ” ValidatingWebhookConfiguration í™•ì¸:"
              webhook_status=$(kubectl get validatingwebhookconfigurations | grep -E "(ingress|aws-load-balancer)" || echo "ì›¹í›… ì—†ìŒ")
              echo "Webhook ìƒíƒœ: $webhook_status"
              
              if [[ "$webhook_status" == "ì›¹í›… ì—†ìŒ" ]]; then
                echo "âŒ ValidatingWebhookConfigurationì´ ì—†ìŠµë‹ˆë‹¤!"
                echo "ğŸ”§ ì´ëŠ” Ingress ìƒì„±ì´ ì°¨ë‹¨ë˜ì§€ ì•Šì•˜ì§€ë§Œ Controllerê°€ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤."
              else
                echo "âœ… ValidatingWebhookConfigurationì´ ìˆìŠµë‹ˆë‹¤."
              fi
              
              echo "ğŸ” ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ Ingress í™•ì¸:"
              kubectl get ingress -A || true
              
              echo "ğŸ” AWS CLIë¥¼ í†µí•œ ALB í™•ì¸:"
              aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, \`${{ env.PROJECT_NAME }}\`) || contains(LoadBalancerName, \`k8s\`)].{Name:LoadBalancerName,State:State.Code,DNS:DNSName}" --output table 2>/dev/null || echo "AWS CLIë¡œ ALB ì¡°íšŒ ì‹¤íŒ¨"
              
              # ALBê°€ ìƒì„±ë˜ì—ˆì§€ë§Œ í”„ë¡œë¹„ì €ë‹ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
              if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "null" ]; then
                echo "DEPLOYMENT_METHOD=Ingress-Provisioning" >> $GITHUB_ENV
                echo "ALB_HOSTNAME=$ALB_DNS" >> $GITHUB_ENV
                echo "â„¹ï¸ ALBê°€ ìƒì„±ë˜ì—ˆì§€ë§Œ í”„ë¡œë¹„ì €ë‹ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: $ALB_DNS"
                echo "â„¹ï¸ ìˆ˜ë™ìœ¼ë¡œ ALB ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
              else
                echo "DEPLOYMENT_METHOD=Ingress-Failed" >> $GITHUB_ENV
                echo "ALB_HOSTNAME=failed" >> $GITHUB_ENV
                echo "âŒ ALB ìƒì„± ìì²´ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
              fi
            fi
            
          else
            echo "âŒ Ingress ë°°í¬ ì‹¤íŒ¨"
            echo "ğŸ” ì§„ë‹¨ ì •ë³´:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
            kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 || true
            kubectl get validatingwebhookconfigurations | grep -E "(ingress|aws-load-balancer)" || echo "ì›¹í›… ì—†ìŒ"
            kubectl get ingressclass || true
            exit 1
          fi
          
          echo "âœ… Ingress ê¸°ë°˜ ALB ë°°í¬ ì™„ë£Œ"

          echo "â³ ë°°í¬ ì™„ë£Œ ëŒ€ê¸° ì¤‘..."
          echo "ğŸ” ë°°í¬ ì „ ìƒíƒœ í™•ì¸:"
          kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide || true
          kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} || true
          
          # ë” ê¸´ timeoutìœ¼ë¡œ ë°°í¬ ëŒ€ê¸°
          if ! kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=600s; then
            echo "âŒ ë°°í¬ timeout ë°œìƒ. ìƒì„¸ ì§„ë‹¨ ì‹œì‘..."
            
            echo "ğŸ” Pod ìƒíƒœ í™•ì¸:"
            kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide
            
            echo "ğŸ” ì‹¤íŒ¨í•œ Pod ë¡œê·¸ í™•ì¸:"
            failed_pods=$(kubectl get pods -n ${{ env.PROJECT_NAME }} --field-selector=status.phase!=Running --no-headers -o custom-columns=":metadata.name" 2>/dev/null || echo "")
            if [ -n "$failed_pods" ]; then
              for pod in $failed_pods; do
                echo "=== Pod $pod ë¡œê·¸ ==="
                kubectl logs $pod -n ${{ env.PROJECT_NAME }} --tail=50 || true
                echo "=== Pod $pod ìƒì„¸ ì •ë³´ ==="
                kubectl describe pod $pod -n ${{ env.PROJECT_NAME }} || true
              done
            fi
            
            echo "ğŸ” Deployment ì´ë²¤íŠ¸ í™•ì¸:"
            kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }}
            
            echo "ğŸ” ConfigMap ë° Secret í™•ì¸:"
            kubectl get configmap -n ${{ env.PROJECT_NAME }} || true
            kubectl get secret -n ${{ env.PROJECT_NAME }} || true
            
            exit 1
          fi

      # ===============================================
      # ë°°í¬ ê²°ê³¼ í™•ì¸
      # ===============================================
    - name: Verify Deployment
      if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
          echo "ğŸ” ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘..."
          echo "ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ${{ env.PROJECT_NAME }}"
          echo "ì•± ë¼ë²¨: ${{ env.PROJECT_NAME }}-app"

          # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ í™•ì¸
          if ! kubectl get namespace ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "âŒ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '${{ env.PROJECT_NAME }}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            kubectl get namespaces
            exit 1
          fi

          # ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
          echo "ğŸ“‹ Pod ìƒíƒœ:"
          kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide || echo "Podë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

          echo "ğŸ“‹ Service ìƒíƒœ:"
          kubectl get services -n ${{ env.PROJECT_NAME }} || echo "Serviceë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

          echo "ğŸ“‹ Deployment ìƒíƒœ:"
          kubectl get deployments -n ${{ env.PROJECT_NAME }} || echo "Deploymentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

          echo "ğŸ“‹ Ingress ìƒíƒœ:"
          kubectl get ingress -n ${{ env.PROJECT_NAME }} || echo "Ingressë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

          echo "ğŸ“‹ AWS Load Balancer Controller ìƒíƒœ:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "AWS Load Balancer Controller App2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

          # Deployment ì¡´ì¬ í™•ì¸ í›„ ëŒ€ê¸°
          if kubectl get deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "â³ Pod ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° ì¤‘..."
            kubectl wait --for=condition=ready pod -l app=${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || echo "Pod ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼"
            
            echo "ğŸ“ Deployment ìƒì„¸ ì •ë³´:"
            kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }}
            
          else
            echo "âŒ Deployment '${{ env.PROJECT_NAME }}-app'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            echo "ì‚¬ìš© ê°€ëŠ¥í•œ Deployment ëª©ë¡:"
            kubectl get deployments -n ${{ env.PROJECT_NAME }}
            exit 1
          fi

      # ===============================================
      # ë°°í¬ ë°©ë²•ë³„ ì ‘ì† URL ì œê³µ
      # ===============================================
    - name: Get Application URL
      if: success() && ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
          echo "ğŸ”— ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† ì •ë³´ í™•ì¸ ì¤‘..."
          echo "ë°°í¬ ë°©ë²•: ${DEPLOYMENT_METHOD:-Unknown}"

          if [ "${DEPLOYMENT_METHOD:-}" = "Ingress" ]; then
            echo "âœ… Ingress ALB ë°°í¬ ì™„ë£Œ!"
            echo "ğŸŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† URL: http://${ALB_HOSTNAME:-í™•ì¸ë¶ˆê°€}"
            
            # ì‹¤ì œ ì—°ê²° í…ŒìŠ¤íŠ¸
            echo "ğŸ” ALB ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
            if curl -s --connect-timeout 10 --max-time 15 -o /dev/null -w "%{http_code}" "http://${ALB_HOSTNAME}${{ env.HEALTH_CHECK_PATH }}" | grep -E "^(200|404)$" >/dev/null; then
              echo "âœ… ALB ì—°ê²° ì„±ê³µ - ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† ê°€ëŠ¥"
            else
              echo "âš ï¸ ALB ì—°ê²° ì‹¤íŒ¨ - í—¬ìŠ¤ì²´í¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
              echo "ğŸ” íƒ€ê²Ÿ ê·¸ë£¹ í—¬ìŠ¤ ìƒíƒœ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            fi
            
            echo "ğŸ” Ingress ìƒíƒœ:"
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Provisioning" ]; then
            echo "âš ï¸ ALBê°€ ìƒì„±ë˜ì—ˆì§€ë§Œ ì•„ì§ í”„ë¡œë¹„ì €ë‹ ì¤‘ì…ë‹ˆë‹¤"
            echo "ğŸŒ ALB DNS: http://${ALB_HOSTNAME:-í™•ì¸ë¶ˆê°€}"
            echo "â„¹ï¸ 5-10ë¶„ í›„ ì ‘ì†ì´ ê°€ëŠ¥í•  ì˜ˆì •ì…ë‹ˆë‹¤."
            
            echo "ğŸ” í˜„ì¬ ALB ì—°ê²° ìƒíƒœ:"
            curl -s --connect-timeout 5 --max-time 10 -w "HTTP ìƒíƒœì½”ë“œ: %{http_code}, ì‘ë‹µì‹œê°„: %{time_total}s\n" "http://${ALB_HOSTNAME}${{ env.HEALTH_CHECK_PATH }}" || echo "ì•„ì§ ì—°ê²° ë¶ˆê°€"
            
            echo "ğŸ” Ingress ìƒíƒœ:"
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Failed" ]; then
            echo "âŒ ALB ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
            echo "ğŸ” í˜„ì¬ Ingress ìƒíƒœ:"
            kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
            echo "ğŸ”§ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸:"
            echo "1. AWS Load Balancer Controllerê°€ ì •ìƒ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"
            echo "2. IngressClass 'alb'ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"
            echo "3. AWS IAM ê¶Œí•œì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸"
            echo "4. ì„œë¸Œë„· íƒœê·¸ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸"
            
          elif [ "${DEPLOYMENT_METHOD:-}" = "Ingress-Pending" ]; then
            echo "âš ï¸ ALB ìƒì„±ì´ ì•„ì§ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            echo "ğŸ” í˜„ì¬ Ingress ìƒíƒœ:"
            kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
            
            echo "ì ì‹œ í›„ ALBê°€ ìƒì„±ë˜ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ URLì„ í™•ì¸í•˜ì„¸ìš”:"
            echo "kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}"
            
          else
            echo "â“ ë°°í¬ ë°©ë²•ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            echo "ë°°í¬ ë°©ë²•: ${DEPLOYMENT_METHOD:-Unknown}"
            echo "ALB í˜¸ìŠ¤íŠ¸ëª…: ${ALB_HOSTNAME:-Unknown}"
            echo "ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ í™•ì¸:"
            kubectl get services -n ${{ env.PROJECT_NAME }}
            echo "ì‚¬ìš© ê°€ëŠ¥í•œ Ingress í™•ì¸:"
            kubectl get ingress -n ${{ env.PROJECT_NAME }}
          fi

          # ê³µí†µ ë””ë²„ê¹… ì •ë³´
          echo ""
          echo "ğŸ“‹ ì „ì²´ ë¦¬ì†ŒìŠ¤ ìƒíƒœ:"
          kubectl get all -n ${{ env.PROJECT_NAME }}

      # ===============================================
      # ë°°í¬ ì™„ë£Œ ì•Œë¦¼
      # ===============================================
    - name: Application Deployment Notification
      if: success() && ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
          echo "ğŸ‰ Java Spring Boot ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì™„ë£Œ!"
          echo "í”„ë¡œì íŠ¸: ${{ env.PROJECT_NAME }}"
          echo "ì´ë¯¸ì§€: ${{ steps.build-image.outputs.image }}"
          echo "í´ëŸ¬ìŠ¤í„°: ${{ env.EKS_CLUSTER_NAME }}"
          echo "ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ${{ env.PROJECT_NAME }}"
          echo "ë°ì´í„°ë² ì´ìŠ¤: ${{ env.RDS_ENDPOINT }}"
          echo "ì»¤ë°‹: ${{ github.sha }}"
          echo "ë°°í¬ ì‹œê°„: $(date)"

          # ë°°í¬ëœ ì„œë¹„ìŠ¤ ì •ë³´ ì¶œë ¥
          echo "ğŸ“‹ ë°°í¬ëœ ë¦¬ì†ŒìŠ¤ ëª©ë¡:"
          kubectl get all -n ${{ env.PROJECT_NAME }} || echo "ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"

      # ===============================================
      # ë°°í¬ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ë° ì •ë¦¬
      # ===============================================
    - name: Rollback on failure
      if: ${{ failure() && github.event_name == 'push' && github.ref == 'refs/heads/main' }}
      run: |
          echo "âŒ ë°°í¬ ì‹¤íŒ¨ - ë¡¤ë°± ë° ì •ë¦¬ ì‹œì‘..."
          
          # Ingress finalizer ì²´í¬ ë° ì •ë¦¬ í•¨ìˆ˜ ì¬ì •ì˜
          check_and_clean_ingress() {
            local namespace="$1"
            local ingress_name="$2"
            
            echo "ğŸ” ì‹¤íŒ¨í•œ Ingress '$ingress_name' (ë„¤ì„ìŠ¤í˜ì´ìŠ¤: $namespace) ì •ë¦¬ ì¤‘..."
            
            # Ingress ì¡´ì¬ í™•ì¸
            if ! kubectl get ingress "$ingress_name" -n "$namespace" >/dev/null 2>&1; then
              echo "â„¹ï¸ Ingress '$ingress_name'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
              return 0
            fi
            
            # finalizer í™•ì¸
            local finalizers=$(kubectl get ingress "$ingress_name" -n "$namespace" -o jsonpath='{.metadata.finalizers}' 2>/dev/null || echo "")
            
            if [[ "$finalizers" == *"ingress.k8s.aws/resources"* ]]; then
              echo "âš ï¸ Ingress '$ingress_name'ì— finalizerê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ê°•ì œ ì •ë¦¬ ì‹œì‘..."
              
              # ìì‹ ì˜ ì•± ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‚´ì˜ Ingressë§Œ ì •ìƒ ì‚­ì œ ì‹œë„
              echo "ğŸ—‘ï¸ ìê¸° ì•±ì˜ Ingressë§Œ ì •ìƒ ì‚­ì œ ì‹œë„..."
              kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s || echo "Ingress ì‚­ì œ ì‹¤íŒ¨"
            else
              echo "âœ… Ingress '$ingress_name'ì— ë¬¸ì œë˜ëŠ” finalizer ì—†ìŒ"
              # ì •ìƒ ì‚­ì œ ì‹œë„
              kubectl delete ingress "$ingress_name" -n "$namespace" --timeout=30s || echo "Ingress ì‚­ì œ ì‹¤íŒ¨"
            fi
          }
          
          # ì‹¤íŒ¨í•œ Ingress ì •ë¦¬
          if kubectl get ingress -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
            echo "ğŸ—‘ï¸ ì‹¤íŒ¨í•œ Ingress ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘..."
            check_and_clean_ingress "${{ env.PROJECT_NAME }}" "${{ env.PROJECT_NAME }}-ingress"
          fi
          
          # ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡¤ë°±
          echo "ğŸ”„ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡¤ë°± ì¤‘..."
          kubectl rollout undo deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} || true
          kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || true
          
          echo "âœ… ë¡¤ë°± ë° ì •ë¦¬ ì™„ë£Œ"