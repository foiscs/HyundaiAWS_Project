# .github/workflows/deploy-app.yml
# PHP ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì›Œí¬í”Œë¡œìš° (server í´ë” ë³€ê²½ì‹œë§Œ ì‹¤í–‰)

name: Deploy PHP Application

on:
  push:
    branches: [ main ]
    paths:
      - 'WALB/server/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'WALB/server/**'

env:
  PROJECT_NAME: "walb-app"
  
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    # ì›Œí‚¹ ë””ë ‰í† ë¦¬ë¥¼ WALBë¡œ ì„¤ì •
    defaults:
      run:
        working-directory: ./WALB
    
    permissions:
      id-token: write
      contents: read
    
    steps:
    # ===============================================
    # ì†ŒìŠ¤ì½”ë“œ ì²´í¬ì•„ì›ƒ
    # ===============================================
    - name: Checkout code
      uses: actions/checkout@v4
    
    # ===============================================
    # PHP ë° Composer í™˜ê²½ ì„¤ì •
    # ===============================================
    - name: Set up PHP
      uses: shivammathur/setup-php@v2
      with:
        php-version: '8.1'
        extensions: pdo, pdo_pgsql, mbstring, xml, zip, gd
        coverage: none
    
    - name: Validate Composer
      run: |
        echo "ğŸ” PHP ì• í”Œë¦¬ì¼€ì´ì…˜ ê²€ì¦ ì¤‘..."
        if [ -f "server/composer.json" ]; then
          cd server
          composer validate --no-check-publish
          composer install --no-dev --optimize-autoloader --no-interaction
          echo "âœ… Composer ê²€ì¦ ì™„ë£Œ"
        else
          echo "â„¹ï¸ Composer íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Docker ë¹Œë“œë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤."
        fi
    
    # ===============================================
    # AWS ì¸ì¦ (OIDC ë°©ì‹)
    # ===============================================
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN_APP }}
        aws-region: ${{ secrets.AWS_REGION }}
        role-session-name: GitHubActions-Application-${{ github.run_id }}

    # ===============================================
    # ê¸°ì¡´ ì¸í”„ë¼ ì •ë³´ ì¡°íšŒ
    # ===============================================
    - name: Get Infrastructure Resources
      run: |
        echo "ğŸ” ê¸°ì¡´ ì¸í”„ë¼ ë¦¬ì†ŒìŠ¤ ì •ë³´ ì¡°íšŒ ì¤‘..."
        
        # ECR ë¦¬í¬ì§€í† ë¦¬ URI ì¡°íšŒ
        ECR_REPO=$(aws ecr describe-repositories --repository-names ${PROJECT_NAME}-ecr --query 'repositories[0].repositoryUri' --output text 2>/dev/null || echo "")
        if [ -z "$ECR_REPO" ]; then
          echo "âŒ ECR ë¦¬í¬ì§€í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${PROJECT_NAME}-ecr"
          echo "ë¨¼ì € ì¸í”„ë¼ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
          exit 1
        fi
        echo "ECR_REPOSITORY=$ECR_REPO" >> $GITHUB_ENV
        echo "âœ… ECR Repository: $ECR_REPO"
        
        # EKS í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì¡°íšŒ
        EKS_CLUSTER=$(aws eks describe-cluster --name walb-eks-cluster --query 'cluster.name' --output text 2>/dev/null || echo "")
        if [ -z "$EKS_CLUSTER" ] || [ "$EKS_CLUSTER" == "None" ]; then
          echo "âŒ EKS í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${PROJECT_NAME}-eks"
          echo "ë¨¼ì € ì¸í”„ë¼ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
          exit 1
        fi
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER" >> $GITHUB_ENV
        echo "âœ… EKS Cluster: $EKS_CLUSTER"
        
        # RDS ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ
        RDS_ENDPOINT=$(aws rds describe-db-instances --query 'DBInstances[?DBName==`mydb`].Endpoint.Address' --output text 2>/dev/null || echo "")
        if [ -z "$RDS_ENDPOINT" ] || [ "$RDS_ENDPOINT" == "None" ]; then
          echo "âŒ RDS ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
          echo "ë¨¼ì € ì¸í”„ë¼ ë°°í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
          exit 1
        fi
        echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
        echo "âœ… RDS Endpoint: $RDS_ENDPOINT"
        
        # EKS í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
        EKS_STATUS=$(aws eks describe-cluster --name $EKS_CLUSTER --query 'cluster.status' --output text)
        if [ "$EKS_STATUS" != "ACTIVE" ]; then
          echo "âŒ EKS í´ëŸ¬ìŠ¤í„°ê°€ í™œì„± ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: $EKS_STATUS"
          exit 1
        fi
        echo "âœ… EKS Cluster Status: $EKS_STATUS"
    
    # ===============================================
    # ECR ë¡œê·¸ì¸
    # ===============================================
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # ===============================================
    # Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ
    # ===============================================
    - name: Build and push Docker image
      id: build-image
      run: |
        echo "ğŸ³ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
        
        # Git ì»¤ë°‹ í•´ì‹œë¥¼ íƒœê·¸ë¡œ ì‚¬ìš©
        IMAGE_TAG=${{ github.sha }}
        IMAGE_URI=${{ env.ECR_REPOSITORY }}:$IMAGE_TAG
        
        # server í´ë”ë¡œ ì´ë™í•´ì„œ Docker ë¹Œë“œ
        cd server
        docker build -t $IMAGE_URI .
        docker tag $IMAGE_URI ${{ env.ECR_REPOSITORY }}:latest
        
        echo "ğŸ“¤ ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ ì¤‘..."
        docker push $IMAGE_URI
        docker push ${{ env.ECR_REPOSITORY }}:latest
        
        echo "âœ… ì´ë¯¸ì§€ í‘¸ì‹œ ì™„ë£Œ: $IMAGE_URI"
        echo "image=$IMAGE_URI" >> $GITHUB_OUTPUT

    - name: Database Connection and Schema Setup
      run: |
        # AWS CLIë¥¼ ì‚¬ìš©í•´ì„œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ì§ì ‘ ì¡°íšŒ
        PROJECT_NAME="walb-app"
        
        # RDS ì—”ë“œí¬ì¸íŠ¸ ì¡°íšŒ (íƒœê·¸ ê¸°ë°˜)
        echo "ğŸ” RDS ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ ì¤‘..."
        DB_HOST=$(aws rds describe-db-instances \
          --query "DBInstances[?contains(keys(TagList[?Key=='Project']), 'Project') && TagList[?Key=='Project'].Value[0]=='${PROJECT_NAME}'].Endpoint.Address" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$DB_HOST" ]; then
          # íƒœê·¸ ì¡°íšŒê°€ ì•ˆ ë˜ë©´ DB ì´ë¦„ìœ¼ë¡œ ì¡°íšŒ
          DB_HOST=$(aws rds describe-db-instances \
            --query "DBInstances[?DBName=='mydb'].Endpoint.Address" \
            --output text 2>/dev/null || echo "")
        fi
        
        # Bastion Host IP ì¡°íšŒ (íƒœê·¸ ê¸°ë°˜)
        echo "ğŸ” Bastion Host ì¡°íšŒ ì¤‘..."
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          # íƒœê·¸ë¡œ ì•ˆ ë˜ë©´ ë³´ì•ˆê·¸ë£¹ìœ¼ë¡œ ì¡°íšŒ
          BASTION_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text 2>/dev/null || echo "")
        fi
        
        # DB ì‚¬ìš©ìëª…ê³¼ DB ì´ë¦„ (í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©)
        DB_NAME="mydb"
        DB_USER="dbadmin"
        
        # Parameter Storeì—ì„œ DB íŒ¨ìŠ¤ì›Œë“œ ì¡°íšŒ
        echo "ğŸ” DB íŒ¨ìŠ¤ì›Œë“œ ì¡°íšŒ ì¤‘..."
        DB_PASSWORD=$(aws ssm get-parameter \
          --name "/${PROJECT_NAME}/rds/master-password" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text 2>/dev/null || echo "")
        
        # ê°’ ê²€ì¦
        if [ -z "$DB_HOST" ] || [ "$DB_HOST" == "None" ]; then
          echo "âŒ RDS ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‚¬ìš© ê°€ëŠ¥í•œ RDS ì¸ìŠ¤í„´ìŠ¤:"
          aws rds describe-db-instances --query "DBInstances[*].[DBInstanceIdentifier,Endpoint.Address,DBName]" --output table
          exit 1
        fi
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "âŒ Bastion Hostë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‹¤í–‰ ì¤‘ì¸ EC2 ì¸ìŠ¤í„´ìŠ¤:"
          aws ec2 describe-instances \
            --filters "Name=instance-state-name,Values=running" \
            --query "Reservations[*].Instances[*].[InstanceId,PublicIpAddress,Tags[?Key=='Name'].Value[0]]" \
            --output table
          exit 1
        fi
        
        if [ -z "$DB_PASSWORD" ]; then
          echo "âŒ DB íŒ¨ìŠ¤ì›Œë“œë¥¼ Parameter Storeì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          exit 1
        fi
        
        echo "âœ… DB Host: '$DB_HOST'"
        echo "âœ… Bastion IP: '$BASTION_IP'"
        echo "âœ… DB User: '$DB_USER'"
        echo "âœ… DB Name: '$DB_NAME'"
        
        # SSH í‚¤ë¥¼ Parameter Storeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        echo "ğŸ”‘ SSH í‚¤ ì¡°íšŒ ì¤‘..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        echo "ğŸ” SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        # SSH ì—°ê²° í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        if ! ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP "echo 'SSH connection successful'" 2>/dev/null; then
          echo "âŒ SSH ì—°ê²° ì‹¤íŒ¨. Bastion Host ìƒíƒœ í™•ì¸:"
          aws ec2 describe-instances \
            --filters "Name=tag:Component,Values=Bastion" \
            --query "Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress,PrivateIpAddress]" \
            --output table
          exit 1
        fi
        
        echo "ğŸ”— SSH í„°ë„ì„ í†µí•œ PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        
        # Bastion Hostì—ì„œ RDS ì—°ê²° í…ŒìŠ¤íŠ¸ ë¨¼ì € ìˆ˜í–‰
        echo "ğŸ” Bastion Hostì—ì„œ RDS ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        echo "  DB Host: $DB_HOST"
        echo "  DB Port: 5432"
        
        # netcat ì„¤ì¹˜ ë° ì—°ê²° í…ŒìŠ¤íŠ¸
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP \
          "command -v nc >/dev/null 2>&1 || sudo yum install -y nc; echo 'Testing connection...'; timeout 10 nc -zv $DB_HOST 5432" || {
          echo "âš ï¸ Bastionì—ì„œ RDS ì§ì ‘ ì—°ê²° ì‹¤íŒ¨"
          echo "ëŒ€ì²´ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œë„ ì¤‘..."
          
          # telnet ëŒ€ì²´ í…ŒìŠ¤íŠ¸
          ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o BatchMode=yes ec2-user@$BASTION_IP \
            "timeout 10 bash -c 'exec 3<>/dev/tcp/$DB_HOST/5432' && echo 'Raw socket connection successful' || echo 'Raw socket connection failed'"
        }
        
        # ë³´ì•ˆ ê·¸ë£¹ ì •ë³´ í™•ì¸
        echo "ğŸ” ë³´ì•ˆ ê·¸ë£¹ ì •ë³´ í™•ì¸ ì¤‘..."
        BASTION_SG=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" --output text)
        
        RDS_SG=$(aws rds describe-db-instances \
          --query "DBInstances[?DBName=='mydb'].VpcSecurityGroups[0].VpcSecurityGroupId" --output text)
        
        echo "Bastion Security Group: $BASTION_SG"
        echo "RDS Security Group: $RDS_SG"
        
        # ë³´ì•ˆ ê·¸ë£¹ ê·œì¹™ í™•ì¸ (ê¶Œí•œì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if [ -n "$RDS_SG" ]; then
          echo "ğŸ” RDS ë³´ì•ˆ ê·¸ë£¹ ì¸ë°”ìš´ë“œ ê·œì¹™ í™•ì¸ ì‹œë„..."
          if aws ec2 describe-security-groups --group-ids "$RDS_SG" \
            --query "SecurityGroups[0].IpPermissions[?FromPort==\`5432\`]" --output table 2>/dev/null; then
            echo "âœ… ë³´ì•ˆ ê·¸ë£¹ ê·œì¹™ ì¡°íšŒ ì„±ê³µ"
          else
            echo "âš ï¸ ë³´ì•ˆ ê·¸ë£¹ ê·œì¹™ ì¡°íšŒ ê¶Œí•œ ì—†ìŒ (ì •ìƒ - ë³´ì•ˆìƒ ì œí•œ)"
            echo "RDS Security Group ID: $RDS_SG"
            echo "Bastion Security Group ID: $BASTION_SG"
          fi
        fi
        
        # SSH í„°ë„ì„ í†µí•œ PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸
        echo "ğŸ”— SSH í„°ë„ ìƒì„± ì¤‘..."
        ssh -i bastion_key.pem -o StrictHostKeyChecking=no -o ExitOnForwardFailure=yes -L 5432:$DB_HOST:5432 ec2-user@$BASTION_IP -N &
        SSH_PID=$!
        
        # í„°ë„ ì„¤ì • ëŒ€ê¸°
        sleep 15
        
        # PostgreSQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš°)
        if ! command -v psql &> /dev/null; then
          echo "ğŸ“¦ PostgreSQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ì¤‘..."
          sudo apt-get update && sudo apt-get install -y postgresql-client
        fi
        
        # ë¡œì»¬ í¬íŠ¸ë¥¼ í†µí•´ RDS ì—°ê²° í…ŒìŠ¤íŠ¸
        if PGPASSWORD=$DB_PASSWORD psql -h localhost -p 5432 -U $DB_USER -d $DB_NAME -c "SELECT 1;" 2>/dev/null; then
          echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ"
        else
          echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        # SSH í„°ë„ ì¢…ë£Œ ë° ì •ë¦¬
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem
        
        echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
      env:
        PROJECT_NAME: "walb-app"
        
    # ===============================================
    # PostgreSQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ ë° ìŠ¤í‚¤ë§ˆ ì ìš©
    # ===============================================
    - name: Apply Database Schema
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì ìš© ì¤‘..."
        
        # PostgreSQL í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜
        sudo apt-get update && sudo apt-get install -y postgresql-client
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        PROJECT_NAME="walb-app"
        DB_HOST="${{ env.RDS_ENDPOINT }}"
        DB_NAME="mydb"
        DB_USER="dbadmin"
        
        # Bastion Host IP ì¡°íšŒ
        BASTION_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=${PROJECT_NAME}-bastion-host" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].PublicIpAddress" \
          --output text 2>/dev/null || echo "")
        
        if [ -z "$BASTION_IP" ] || [ "$BASTION_IP" == "None" ]; then
          echo "âŒ Bastion Hostë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          exit 1
        fi
        
        echo "ğŸ” ì—°ê²° ì •ë³´:"
        echo "  RDS ì—”ë“œí¬ì¸íŠ¸: $DB_HOST"
        echo "  Bastion IP: $BASTION_IP"
        echo "  DB ì´ë¦„: $DB_NAME"
        echo "  DB ì‚¬ìš©ì: $DB_USER"
        
        # SSH í‚¤ë¥¼ Parameter Storeì—ì„œ ê°€ì ¸ì˜¤ê¸°
        echo "ğŸ”‘ SSH í‚¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."
        aws ssm get-parameter \
          --name "/${PROJECT_NAME}/bastion/ssh-private-key" \
          --with-decryption \
          --query 'Parameter.Value' \
          --output text > bastion_key.pem
        chmod 600 bastion_key.pem
        
        # SSH í„°ë„ ìƒì„± (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
        echo "ğŸ”— SSH í„°ë„ ìƒì„± ì¤‘..."
        ssh -i bastion_key.pem \
            -o StrictHostKeyChecking=no \
            -o ExitOnForwardFailure=yes \
            -L 5432:$DB_HOST:5432 \
            ec2-user@$BASTION_IP \
            -N &
        SSH_PID=$!
        
        # í„°ë„ ì„¤ì • ëŒ€ê¸°
        echo "â³ SSH í„°ë„ ì„¤ì • ëŒ€ê¸° ì¤‘..."
        sleep 15
        
        # SSH í„°ë„ ìƒíƒœ í™•ì¸
        if ! kill -0 $SSH_PID 2>/dev/null; then
          echo "âŒ SSH í„°ë„ ìƒì„± ì‹¤íŒ¨"
          rm -f bastion_key.pem
          exit 1
        fi
        
        echo "âœ… SSH í„°ë„ ìƒì„± ì™„ë£Œ"
        
        # ë¡œì»¬ í¬íŠ¸ë¥¼ í†µí•´ RDS ì—°ê²° í…ŒìŠ¤íŠ¸
        echo "ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."
        if ! PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
          -h localhost \
          -p 5432 \
          -U "$DB_USER" \
          -d "$DB_NAME" \
          -c "SELECT version();" \
          -v ON_ERROR_STOP=1 >/dev/null 2>&1; then
          echo "âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
          kill $SSH_PID 2>/dev/null
          rm -f bastion_key.pem
          exit 1
        fi
        
        echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ"
        
        # ê¸°ì¡´ í…Œì´ë¸” í™•ì¸
        echo "ğŸ” ê¸°ì¡´ í…Œì´ë¸” í™•ì¸ ì¤‘..."
        EXISTING_TABLES=$(PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
          -h localhost \
          -p 5432 \
          -U "$DB_USER" \
          -d "$DB_NAME" \
          -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('users', 'posts', 'images', 'files');" \
          -v ON_ERROR_STOP=1 | tr -d ' ')
        
        if [ "$EXISTING_TABLES" -eq "0" ]; then
          echo "ğŸ“ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ì ìš© ì¤‘..."
          if PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
            -h localhost \
            -p 5432 \
            -U "$DB_USER" \
            -d "$DB_NAME" \
            -f server/files/schema.sql \
            -v ON_ERROR_STOP=1; then
            echo "âœ… ìŠ¤í‚¤ë§ˆ ì ìš© ì™„ë£Œ"
          else
            echo "âŒ ìŠ¤í‚¤ë§ˆ ì ìš© ì‹¤íŒ¨"
            kill $SSH_PID 2>/dev/null
            rm -f bastion_key.pem
            exit 1
          fi
        else
          echo "â„¹ï¸ í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆ ì ìš©ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        fi
        
        # ì •ë¦¬ ì‘ì—…
        echo "ğŸ§¹ ì •ë¦¬ ì‘ì—… ì¤‘..."
        kill $SSH_PID 2>/dev/null
        rm -f bastion_key.pem
        
        echo "âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì‘ì—… ì™„ë£Œ"
    
    # ===============================================
    # kubectl ë° Helm ì„¤ì¹˜
    # ===============================================
    - name: Install kubectl and Helm
      run: |
        echo "ğŸ”§ kubectl ë° Helm ì„¤ì¹˜ ì¤‘..."
        
        # kubectl ì„¤ì¹˜
        curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
        # Helm ì„¤ì¹˜
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        
        echo "âœ… kubectl ë° Helm ì„¤ì¹˜ ì™„ë£Œ"
        kubectl version --client
        helm version
    
    - name: Update kubeconfig for EKS
      run: |
        echo "ğŸ”§ EKS í´ëŸ¬ìŠ¤í„° kubeconfig ì—…ë°ì´íŠ¸ ì¤‘..."
        
        # í˜„ì¬ AWS ìê²© ì¦ëª… í™•ì¸
        echo "ğŸ” í˜„ì¬ AWS ìê²© ì¦ëª… í™•ì¸..."
        aws sts get-caller-identity
        
        # EKS í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì¡°íšŒ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        echo "ğŸ” EKS í´ëŸ¬ìŠ¤í„° ì¡°íšŒ ì¤‘..."
        
        # ë°©ë²• 1: í´ëŸ¬ìŠ¤í„° ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ ì¡°íšŒ
        EKS_CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
        
        # ë°©ë²• 2: íŠ¹ì • ì´ë¦„ìœ¼ë¡œ ì¡°íšŒ
        if [ -z "$EKS_CLUSTER_NAME" ] || [ "$EKS_CLUSTER_NAME" == "None" ]; then
          EKS_CLUSTER_NAME="walb-eks-cluster"
          echo "ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ì´ë¦„ ì‚¬ìš©: $EKS_CLUSTER_NAME"
        fi
        
        # í´ëŸ¬ìŠ¤í„° ì¡´ì¬ í™•ì¸
        if ! aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} >/dev/null 2>&1; then
          echo "âŒ EKS í´ëŸ¬ìŠ¤í„° '$EKS_CLUSTER_NAME'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„° ëª©ë¡:"
          aws eks list-clusters --region ${{ secrets.AWS_REGION }}
          exit 1
        fi
        
        echo "âœ… EKS í´ëŸ¬ìŠ¤í„°: $EKS_CLUSTER_NAME"
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> $GITHUB_ENV
        
        # í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
        CLUSTER_STATUS=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.status' --output text)
        echo "í´ëŸ¬ìŠ¤í„° ìƒíƒœ: $CLUSTER_STATUS"
        
        if [ "$CLUSTER_STATUS" != "ACTIVE" ]; then
          echo "âŒ í´ëŸ¬ìŠ¤í„°ê°€ ACTIVE ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤: $CLUSTER_STATUS"
          exit 1
        fi
        
        # kubeconfig ì—…ë°ì´íŠ¸ (ìƒì„¸ ë¡œê·¸ í¬í•¨)
        echo "ğŸ”§ kubeconfig ì—…ë°ì´íŠ¸ ì¤‘..."
        aws eks update-kubeconfig --region ${{ secrets.AWS_REGION }} --name "$EKS_CLUSTER_NAME" --verbose
        
        # í´ëŸ¬ìŠ¤í„° ì—°ê²° í…ŒìŠ¤íŠ¸ (ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ í¬í•¨)
        echo "ğŸ” í´ëŸ¬ìŠ¤í„° ì—°ê²° í…ŒìŠ¤íŠ¸..."
        if ! kubectl cluster-info --request-timeout=30s; then
          echo "âŒ kubectl cluster-info ì‹¤íŒ¨. ì¶”ê°€ ì§„ë‹¨ ì •ë³´:"
          
          # kubectl ì„¤ì • í™•ì¸
          echo "kubectl ì„¤ì • í™•ì¸:"
          kubectl config view
          
          # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
          echo "í˜„ì¬ ì»¨í…ìŠ¤íŠ¸:"
          kubectl config current-context
          
          # í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸ ì§ì ‘ í…ŒìŠ¤íŠ¸
          CLUSTER_ENDPOINT=$(aws eks describe-cluster --name "$EKS_CLUSTER_NAME" --region ${{ secrets.AWS_REGION }} --query 'cluster.endpoint' --output text)
          echo "í´ëŸ¬ìŠ¤í„° ì—”ë“œí¬ì¸íŠ¸: $CLUSTER_ENDPOINT"
          
          # IAM ì—­í• ê³¼ RBAC ë§¤í•‘ í™•ì¸
          echo "EKS í´ëŸ¬ìŠ¤í„°ì˜ aws-auth ConfigMap í™•ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
          exit 1
        fi
        
        echo "ğŸ” ë…¸ë“œ ìƒíƒœ í™•ì¸..."
        kubectl get nodes --show-labels
    
    # ===============================================
    # cert-manager ì„¤ì¹˜ (webhook TLS ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°)
    # ===============================================
    - name: Install cert-manager
      run: |
        echo "ğŸ”§ cert-manager ì„¤ì¹˜ ì¤‘..."
        
        # ê¸°ì¡´ AWS Load Balancer Controller webhook ì •ë¦¬ (cert-manager ì„¤ì¹˜ ê°„ì„­ ë°©ì§€)
        echo "ğŸ§¹ ê¸°ì¡´ AWS Load Balancer Controller webhook ì •ë¦¬ ì¤‘..."
        kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || echo "ValidatingAdmissionWebhook ì—†ìŒ"
        kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || echo "MutatingAdmissionWebhook ì—†ìŒ"
        kubectl delete service aws-load-balancer-webhook-service -n kube-system 2>/dev/null || echo "ì›¹í›… ì„œë¹„ìŠ¤ ì—†ìŒ"
        
        # AWS Load Balancer Controller ì œê±° (ì¬ì„¤ì¹˜ë¥¼ ìœ„í•´)
        if helm list -n kube-system | grep -q aws-load-balancer-controller; then
          echo "ê¸°ì¡´ AWS Load Balancer Controller ì œê±° ì¤‘..."
          helm uninstall aws-load-balancer-controller -n kube-system || true
          sleep 10
        fi
        
        echo "âœ… webhook ì •ë¦¬ ì™„ë£Œ"
        
        # cert-manager ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
        kubectl create namespace cert-manager --dry-run=client -o yaml | kubectl apply -f -
        
        # cert-manager Helm ë¦¬í¬ì§€í† ë¦¬ ì¶”ê°€
        helm repo add jetstack https://charts.jetstack.io
        helm repo update
        
        # ê¸°ì¡´ cert-manager ì™„ì „ ì œê±°
        if helm list -n cert-manager | grep -q cert-manager; then
          echo "ê¸°ì¡´ cert-manager ì œê±° ì¤‘..."
          helm uninstall cert-manager -n cert-manager || true
        fi
        
        # cert-manager CRDs ì œê±° (ì™„ì „ ì´ˆê¸°í™”)
        kubectl delete crd certificaterequests.cert-manager.io 2>/dev/null || true
        kubectl delete crd certificates.cert-manager.io 2>/dev/null || true
        kubectl delete crd challenges.acme.cert-manager.io 2>/dev/null || true
        kubectl delete crd clusterissuers.cert-manager.io 2>/dev/null || true
        kubectl delete crd issuers.cert-manager.io 2>/dev/null || true
        kubectl delete crd orders.acme.cert-manager.io 2>/dev/null || true
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì™„ì „ ì •ë¦¬
        kubectl delete namespace cert-manager 2>/dev/null || true
        sleep 20
        
        # cert-manager ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¬ìƒì„±
        kubectl create namespace cert-manager
        
        # cert-manager kubectl ì§ì ‘ ì„¤ì¹˜ (ë” ì•ˆì •ì )
        echo "ğŸ“¦ cert-manager kubectl ì§ì ‘ ì„¤ì¹˜ ì¤‘..."
        kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml
        
        echo "âœ… cert-manager ì„¤ì¹˜ ì™„ë£Œ"
        
        # cert-manager Pod ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° (ì¶©ë¶„í•œ ì‹œê°„)
        echo "â³ cert-manager Pod ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
        kubectl wait --for=condition=ready pod -l app=cert-manager -n cert-manager --timeout=600s
        kubectl wait --for=condition=ready pod -l app=webhook -n cert-manager --timeout=600s  
        kubectl wait --for=condition=ready pod -l app=cainjector -n cert-manager --timeout=600s
        
        # cert-manager ì™„ì „ ì•ˆì •í™” ëŒ€ê¸°
        echo "â³ cert-manager ì™„ì „ ì•ˆì •í™” ëŒ€ê¸° (60ì´ˆ)..."
        sleep 60
        
        # cert-manager webhook ì„œë¹„ìŠ¤ í™•ì¸
        echo "ğŸ” cert-manager webhook ì„œë¹„ìŠ¤ í™•ì¸..."
        kubectl get service cert-manager-webhook -n cert-manager
        kubectl get endpoints cert-manager-webhook -n cert-manager
        
        # ValidatingAdmissionWebhook í™•ì¸
        echo "ğŸ” ValidatingAdmissionWebhook í™•ì¸..."
        kubectl get validatingadmissionwebhook cert-manager-webhook || echo "webhook ì•„ì§ ì¤€ë¹„ ì¤‘..."
        
        # webhook ì—°ê²°ì„± í…ŒìŠ¤íŠ¸
        echo "ğŸ” webhook ì—°ê²°ì„± í…ŒìŠ¤íŠ¸..."
        for i in {1..10}; do
          echo "webhook í…ŒìŠ¤íŠ¸ ì‹œë„ ($i/10)..."
          if kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager-webhook:https:webhook/proxy/readyz 2>/dev/null; then
            echo "âœ… webhook ì—°ê²°ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ"
            break
          else
            echo "âš ï¸ webhook ì—°ê²°ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, 10ì´ˆ í›„ ì¬ì‹œë„..."
            sleep 10
          fi
        done
        
        # Self-signed issuer ìƒì„± (webhook ì¤€ë¹„ í›„)
        echo "ğŸ” Self-signed issuer ìƒì„± ì¤‘..."
        for i in {1..5}; do
          echo "ClusterIssuer ìƒì„± ì‹œë„ ($i/5)..."
          if cat <<'EOF' | kubectl apply -f -
        apiVersion: cert-manager.io/v1
        kind: ClusterIssuer
        metadata:
          name: selfsigned-issuer
        spec:
          selfSigned: {}
        EOF
          then
            echo "âœ… ClusterIssuer ìƒì„± ì„±ê³µ"
            break
          else
            echo "âš ï¸ ClusterIssuer ìƒì„± ì‹¤íŒ¨, 30ì´ˆ í›„ ì¬ì‹œë„..."
            sleep 30
          fi
        done
        
        # ClusterIssuer ì¤€ë¹„ ëŒ€ê¸°
        echo "â³ ClusterIssuer ì¤€ë¹„ ëŒ€ê¸°..."
        kubectl wait --for=condition=ready clusterissuer selfsigned-issuer --timeout=300s || echo "ClusterIssuer ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼"
        
        echo "âœ… cert-manager ì„¤ì¹˜ ì™„ë£Œ"
        kubectl get pods -n cert-manager
        kubectl get certificates -n cert-manager
    
    # ===============================================
    # AWS Load Balancer Controller ì„¤ì¹˜
    # ===============================================
    - name: Install AWS Load Balancer Controller
      run: |
        echo "ğŸ”§ AWS Load Balancer Controller ì„¤ì¹˜ ì¤‘..."
        
        # IAM Roleê³¼ ServiceAccount ì¡´ì¬ í™•ì¸
        echo "ğŸ” IAM Role í™•ì¸ ì¤‘..."
        
        # EKS í´ëŸ¬ìŠ¤í„° ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ Role ì´ë¦„ êµ¬ì„±
        PRIMARY_ROLE_NAME="${{ env.EKS_CLUSTER_NAME }}-aws-load-balancer-controller"
        
        echo "ğŸ” ì˜ˆìƒë˜ëŠ” IAM Role: $PRIMARY_ROLE_NAME"
        
        # ë¨¼ì € ì˜ˆìƒë˜ëŠ” Role ì´ë¦„ìœ¼ë¡œ ì§ì ‘ í™•ì¸
        if aws iam get-role --role-name "$PRIMARY_ROLE_NAME" >/dev/null 2>&1; then
          echo "âœ… IAM Role ë°œê²¬: $PRIMARY_ROLE_NAME"
          IAM_ROLE_ARN="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/$PRIMARY_ROLE_NAME"
          FOUND_ROLE="$PRIMARY_ROLE_NAME"
        else
          echo "âš ï¸ ì˜ˆìƒ Role ì—†ìŒ, ë‹¤ë¥¸ íŒ¨í„´ í™•ì¸ ì¤‘..."
          
          # ë°±ì—… íŒ¨í„´ë“¤ í™•ì¸
          BACKUP_PATTERNS=(
            "walb-eks-cluster-aws-load-balancer-controller"
            "walb-app-aws-load-balancer-controller"
          )
          
          FOUND_ROLE=""
          for ROLE_NAME in "${BACKUP_PATTERNS[@]}"; do
            echo "ğŸ” ë°±ì—… íŒ¨í„´ í™•ì¸: $ROLE_NAME"
            if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
              echo "âœ… IAM Role ë°œê²¬: $ROLE_NAME"
              IAM_ROLE_ARN="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/$ROLE_NAME"
              FOUND_ROLE="$ROLE_NAME"
              break
            fi
          done
          
          # ëª¨ë“  íŒ¨í„´ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°
          if [ -z "$FOUND_ROLE" ]; then
            echo "âŒ AWS Load Balancer Controller IAM Roleì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            echo ""
            echo "ğŸ” Load Balancer ê´€ë ¨ IAM Role ê²€ìƒ‰ ì¤‘..."
            if aws iam list-roles --query 'Roles[?contains(RoleName, `load-balancer`) || contains(RoleName, `alb`)].{RoleName:RoleName,CreateDate:CreateDate}' --output table 2>/dev/null; then
              echo "ìœ„ Roleë“¤ì„ í™•ì¸í–ˆì§€ë§Œ ì˜ˆìƒ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            else
              echo "IAM Role ëª©ë¡ ì¡°íšŒ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
            fi
            echo ""
            echo "í•´ê²° ë°©ë²•:"
            echo "1. Terraformì—ì„œ enable_load_balancer = trueë¡œ ì„¤ì •"
            echo "2. terraform applyë¡œ ì¸í”„ë¼ ì¬ë°°í¬"
            echo "3. ì˜ˆìƒ Role ì´ë¦„: $PRIMARY_ROLE_NAME"
            exit 1
          fi
        fi
        
        echo "âœ… ì‚¬ìš©í•  IAM Role: $IAM_ROLE_ARN"
        
        # ê¸°ì¡´ ServiceAccount í™•ì¸ ë° ì‚­ì œ (ì¬ìƒì„±ì„ ìœ„í•´)
        if kubectl get serviceaccount aws-load-balancer-controller -n kube-system >/dev/null 2>&1; then
          echo "ê¸°ì¡´ ServiceAccount ì‚­ì œ ì¤‘..."
          kubectl delete serviceaccount aws-load-balancer-controller -n kube-system || true
        fi
        
        # AWS Load Balancer Controller ServiceAccount ìƒì„±
        echo "ğŸ”§ ServiceAccount ìƒì„± ì¤‘..."
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          annotations:
            eks.amazonaws.com/role-arn: $IAM_ROLE_ARN
        EOF
        
        # VPC ID ì¡°íšŒ
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text)
        echo "VPC ID: $VPC_ID"
        
        # AWS Load Balancer Controller Helm chart ì„¤ì¹˜
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # ê¸°ì¡´ ì„¤ì¹˜ í™•ì¸ ë° ì²˜ë¦¬
        if helm list -n kube-system | grep -q aws-load-balancer-controller; then
          echo "ê¸°ì¡´ AWS Load Balancer Controller ë°œê²¬ - ìƒíƒœ í™•ì¸ ì¤‘..."
          
          # ê¸°ì¡´ ì„¤ì¹˜ ìƒíƒœ í™•ì¸
          EXISTING_STATUS=$(helm status aws-load-balancer-controller -n kube-system -o json 2>/dev/null | jq -r '.info.status' || echo "unknown")
          echo "ê¸°ì¡´ ì„¤ì¹˜ ìƒíƒœ: $EXISTING_STATUS"
          
          if [ "$EXISTING_STATUS" != "deployed" ]; then
            echo "ê¸°ì¡´ ì„¤ì¹˜ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ì œê±° í›„ ì¬ì„¤ì¹˜..."
            helm uninstall aws-load-balancer-controller -n kube-system || true
            
            # ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
            kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
            
            # ì •ë¦¬ ëŒ€ê¸°
            sleep 30
          else
            echo "ê¸°ì¡´ ì„¤ì¹˜ê°€ ì •ìƒì…ë‹ˆë‹¤. ì—…ê·¸ë ˆì´ë“œ ì‹œë„..."
            if helm upgrade aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set region=${{ secrets.AWS_REGION }} \
              --set vpcId=$VPC_ID \
              --set enableShield=false \
              --set enableWaf=false \
              --set enableWafv2=false \
              --timeout=600s \
              --wait; then
              echo "âœ… ì—…ê·¸ë ˆì´ë“œ ì„±ê³µ"
              INSTALL_NEEDED=false
            else
              echo "âŒ ì—…ê·¸ë ˆì´ë“œ ì‹¤íŒ¨ - ì œê±° í›„ ì¬ì„¤ì¹˜..."
              helm uninstall aws-load-balancer-controller -n kube-system || true
              kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
              kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
              sleep 30
              INSTALL_NEEDED=true
            fi
          fi
        else
          INSTALL_NEEDED=true
        fi
        
        # ìƒˆë¡œìš´ ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš°)
        if [ "${INSTALL_NEEDED:-true}" = "true" ]; then
          echo "ìƒˆë¡œìš´ AWS Load Balancer Controller ì„¤ì¹˜ ì¤‘..."
          
          # cert-manager ì™„ì „ ì¤€ë¹„ ëŒ€ê¸° (webhook í¬í•¨)
          echo "â³ cert-manager ì™„ì „ ì¤€ë¹„ ëŒ€ê¸° ì¤‘..."
          for i in {1..15}; do
            CERT_MANAGER_READY=false
            WEBHOOK_READY=false
            
            # cert-manager Pod ìƒíƒœ í™•ì¸
            if kubectl get pods -n cert-manager -l app=cert-manager --field-selector=status.phase=Running | grep -q cert-manager; then
              CERT_MANAGER_READY=true
            fi
            
            # cert-manager webhook ìƒíƒœ í™•ì¸
            if kubectl get validatingadmissionwebhook cert-manager-webhook >/dev/null 2>&1; then
              WEBHOOK_READY=true
            fi
            
            if [ "$CERT_MANAGER_READY" = "true" ] && [ "$WEBHOOK_READY" = "true" ]; then
              echo "âœ… cert-managerì™€ webhook ëª¨ë‘ ì¤€ë¹„ ì™„ë£Œ"
              break
            fi
            
            echo "cert-manager ëŒ€ê¸° ì¤‘... ($i/15) [cert-manager: $CERT_MANAGER_READY, webhook: $WEBHOOK_READY]"
            sleep 20
          done
          
          # ClusterIssuer ìƒì„± (AWS Load Balancer Controller webhookìš©)
          echo "ğŸ” AWS Load Balancer Controllerìš© ClusterIssuer ìƒì„± ì¤‘..."
          cat <<'EOF' | kubectl apply -f -
        apiVersion: cert-manager.io/v1
        kind: ClusterIssuer
        metadata:
          name: aws-load-balancer-webhook-issuer
        spec:
          selfSigned: {}
        EOF
          
          # AWS Load Balancer Controller webhookìš© TLS ì¸ì¦ì„œ ë¯¸ë¦¬ ìƒì„±
          echo "ğŸ“œ AWS Load Balancer Controller webhook TLS ì¸ì¦ì„œ ë¯¸ë¦¬ ìƒì„± ì¤‘..."
          cat <<'EOF' | kubectl apply -f -
        apiVersion: cert-manager.io/v1
        kind: Certificate
        metadata:
          name: aws-load-balancer-webhook-tls
          namespace: kube-system
        spec:
          secretName: aws-load-balancer-webhook-tls
          issuerRef:
            name: aws-load-balancer-webhook-issuer
            kind: ClusterIssuer
          dnsNames:
          - aws-load-balancer-webhook-service
          - aws-load-balancer-webhook-service.kube-system
          - aws-load-balancer-webhook-service.kube-system.svc
          - aws-load-balancer-webhook-service.kube-system.svc.cluster.local
        EOF
          
          # ì¸ì¦ì„œ ìƒì„± ëŒ€ê¸°
          echo "â³ AWS Load Balancer Controller webhook TLS ì¸ì¦ì„œ ìƒì„± ëŒ€ê¸° ì¤‘..."
          kubectl wait --for=condition=ready certificate aws-load-balancer-webhook-tls -n kube-system --timeout=300s || {
            echo "âš ï¸ ì¸ì¦ì„œ ìƒì„± ì‹¤íŒ¨, ê³„ì† ì§„í–‰..."
          }
          
          # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ ì„¤ì¹˜
          for i in {1..3}; do
            echo "ì„¤ì¹˜ ì‹œë„ $i/3..."
            
            # í˜¹ì‹œ ë‚¨ì€ webhook ì •ë¦¬
            kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
            kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
            
            if helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set region=${{ secrets.AWS_REGION }} \
              --set vpcId=$VPC_ID \
              --set enableShield=false \
              --set enableWaf=false \
              --set enableWafv2=false \
              --set replicaCount=2 \
              --set resources.limits.cpu=200m \
              --set resources.limits.memory=500Mi \
              --set resources.requests.cpu=100m \
              --set resources.requests.memory=200Mi \
              --set webhook.port=9443 \
              --set webhook.certManager.enabled=true \
              --set webhook.certManager.issuerRef.name=aws-load-balancer-webhook-issuer \
              --set webhook.certManager.issuerRef.kind=ClusterIssuer \
              --timeout=600s \
              --wait; then
              echo "âœ… ì„¤ì¹˜ ì„±ê³µ"
              break
            else
              echo "âŒ ì„¤ì¹˜ ì‹¤íŒ¨ ($i/3)"
              if [ $i -lt 3 ]; then
                echo "60ì´ˆ í›„ ì¬ì‹œë„..."
                sleep 60
                # ì‹¤íŒ¨í•œ ì„¤ì¹˜ ì •ë¦¬
                helm uninstall aws-load-balancer-controller -n kube-system 2>/dev/null || true
                kubectl delete validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
                kubectl delete mutatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || true
                # ì¸ì¦ì„œ ì¬ìƒì„± ì‹œë„
                kubectl delete certificate aws-load-balancer-webhook-tls -n kube-system 2>/dev/null || true
                cat <<'EOF' | kubectl apply -f -
        apiVersion: cert-manager.io/v1
        kind: Certificate
        metadata:
          name: aws-load-balancer-webhook-tls
          namespace: kube-system
        spec:
          secretName: aws-load-balancer-webhook-tls
          issuerRef:
            name: aws-load-balancer-webhook-issuer
            kind: ClusterIssuer
          dnsNames:
          - aws-load-balancer-webhook-service
          - aws-load-balancer-webhook-service.kube-system
          - aws-load-balancer-webhook-service.kube-system.svc
          - aws-load-balancer-webhook-service.kube-system.svc.cluster.local
        EOF
                sleep 30
              else
                echo "âŒ ëª¨ë“  ì„¤ì¹˜ ì‹œë„ ì‹¤íŒ¨"
                exit 1
              fi
            fi
          done
        fi
        
        # Controller Pod ìƒíƒœ í™•ì¸
        echo "â³ AWS Load Balancer Controller Pod ì‹œì‘ ëŒ€ê¸° ì¤‘..."
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
        
        # ì›¹í›… ì„œë¹„ìŠ¤ ê¸°ë³¸ í™•ì¸
        echo "ğŸ” ì›¹í›… ì„œë¹„ìŠ¤ ê¸°ë³¸ ìƒíƒœ í™•ì¸..."
        if kubectl get service aws-load-balancer-webhook-service -n kube-system >/dev/null 2>&1; then
          echo "âœ… ì›¹í›… ì„œë¹„ìŠ¤ ì¡´ì¬"
          kubectl get endpoints aws-load-balancer-webhook-service -n kube-system
        else
          echo "âš ï¸ ì›¹í›… ì„œë¹„ìŠ¤ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ"
        fi
        
        # ValidatingAdmissionWebhook ìƒì„± ëŒ€ê¸° ë° í™•ì¸ ê°•í™”
        echo "ğŸ” ValidatingAdmissionWebhook ìƒì„± ëŒ€ê¸° ì¤‘..."
        WEBHOOK_READY=false
        
        # cert-managerê°€ ì¸ì¦ì„œë¥¼ ìƒì„±í•  ì‹œê°„ì„ ê¸°ë‹¤ë¦¼
        echo "â³ TLS ì¸ì¦ì„œ ìƒì„± ëŒ€ê¸° ì¤‘ (30ì´ˆ)..."
        sleep 30
        
        for i in {1..15}; do
          echo "ValidatingAdmissionWebhook í™•ì¸ ì‹œë„ ($i/15)..."
          
          # 1ë‹¨ê³„: ValidatingAdmissionWebhook ì¡´ì¬ í™•ì¸
          if kubectl get validatingadmissionwebhook aws-load-balancer-webhook >/dev/null 2>&1; then
            echo "âœ… ValidatingAdmissionWebhook ì¡´ì¬"
            
            # 2ë‹¨ê³„: Webhook ìƒì„¸ ì •ë³´ ì¶œë ¥
            echo "ì›¹í›… ìƒì„¸ ì •ë³´:"
            kubectl get validatingadmissionwebhook aws-load-balancer-webhook -o yaml | head -50
            
            # 3ë‹¨ê³„: ClientConfig ì„œë¹„ìŠ¤ í™•ì¸
            WEBHOOK_SERVICE=$(kubectl get validatingadmissionwebhook aws-load-balancer-webhook -o jsonpath='{.webhooks[0].clientConfig.service.name}' 2>/dev/null || echo "")
            WEBHOOK_NAMESPACE=$(kubectl get validatingadmissionwebhook aws-load-balancer-webhook -o jsonpath='{.webhooks[0].clientConfig.service.namespace}' 2>/dev/null || echo "")
            
            if [ -n "$WEBHOOK_SERVICE" ] && [ -n "$WEBHOOK_NAMESPACE" ]; then
              echo "ì›¹í›… ì„œë¹„ìŠ¤: $WEBHOOK_SERVICE (ë„¤ì„ìŠ¤í˜ì´ìŠ¤: $WEBHOOK_NAMESPACE)"
              
              # 4ë‹¨ê³„: ì„œë¹„ìŠ¤ ì¡´ì¬ ë° ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
              if kubectl get service "$WEBHOOK_SERVICE" -n "$WEBHOOK_NAMESPACE" >/dev/null 2>&1; then
                echo "âœ… ì›¹í›… ì„œë¹„ìŠ¤ ì¡´ì¬"
                
                # 5ë‹¨ê³„: ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
                ENDPOINT_IPS=$(kubectl get endpoints "$WEBHOOK_SERVICE" -n "$WEBHOOK_NAMESPACE" -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || echo "")
                if [ -n "$ENDPOINT_IPS" ]; then
                  echo "âœ… ì›¹í›… ì—”ë“œí¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ: $ENDPOINT_IPS"
                  
                  # 6ë‹¨ê³„: TLS ì¸ì¦ì„œ í™•ì¸
                  TLS_SECRET=$(kubectl get validatingadmissionwebhook aws-load-balancer-webhook -o jsonpath='{.webhooks[0].clientConfig.caBundle}' 2>/dev/null || echo "")
                  if [ -n "$TLS_SECRET" ]; then
                    echo "âœ… TLS ì¸ì¦ì„œ í™•ì¸ë¨"
                    WEBHOOK_READY=true
                    break
                  else
                    echo "âš ï¸ TLS ì¸ì¦ì„œ ëŒ€ê¸° ì¤‘..."
                  fi
                else
                  echo "âš ï¸ ì›¹í›… ì—”ë“œí¬ì¸íŠ¸ ëŒ€ê¸° ì¤‘..."
                fi
              else
                echo "âŒ ì›¹í›… ì„œë¹„ìŠ¤ ì—†ìŒ"
              fi
            else
              echo "âš ï¸ ì›¹í›… ClientConfig ì •ë³´ ë¶ˆì™„ì „"
            fi
          else
            echo "âš ï¸ ValidatingAdmissionWebhook ìƒì„± ëŒ€ê¸° ì¤‘..."
            
            # cert-managerì™€ AWS Load Balancer Controller ìƒíƒœ í™•ì¸
            echo "ğŸ” cert-manager ìƒíƒœ:"
            kubectl get pods -n cert-manager --no-headers | grep -E "(cert-manager|webhook|cainjector)" || echo "cert-manager ì •ë³´ ì—†ìŒ"
            
            echo "ğŸ” AWS Load Balancer Controller ìƒíƒœ:"
            kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --no-headers || echo "Controller ì •ë³´ ì—†ìŒ"
            
            # Certificate ë¦¬ì†ŒìŠ¤ í™•ì¸ (ìˆë‹¤ë©´)
            echo "ğŸ” Certificate ë¦¬ì†ŒìŠ¤ í™•ì¸:"
            kubectl get certificates -n kube-system 2>/dev/null || echo "Certificate ë¦¬ì†ŒìŠ¤ ì—†ìŒ"
          fi
          
          if [ $i -lt 15 ]; then
            echo "20ì´ˆ í›„ ì¬ì‹œë„..."
            sleep 20
          fi
        done
        
        if [ "$WEBHOOK_READY" = false ]; then
          echo "âŒ ValidatingAdmissionWebhookì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
          
          # ë” ìƒì„¸í•œ ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘
          echo ""
          echo "ğŸ” AWS Load Balancer Controller ì§„ë‹¨ ì •ë³´:"
          
          # Pod ìƒíƒœ í™•ì¸
          echo "Pod ìƒíƒœ:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide || true
          
          # Pod ì´ë²¤íŠ¸ í™•ì¸
          echo ""
          echo "Pod ì´ë²¤íŠ¸:"
          kubectl get events -n kube-system --field-selector involvedObject.kind=Pod --sort-by='.lastTimestamp' | grep aws-load-balancer-controller || true
          
          # Deployment ìƒíƒœ í™•ì¸
          echo ""
          echo "Deployment ìƒíƒœ:"
          kubectl describe deployment -n kube-system aws-load-balancer-controller || true
          
          # ReplicaSet ìƒíƒœ í™•ì¸
          echo ""
          echo "ReplicaSet ìƒíƒœ:"
          kubectl get replicasets -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
          
          # ServiceAccount í™•ì¸
          echo ""
          echo "ServiceAccount ìƒíƒœ:"
          kubectl get serviceaccount aws-load-balancer-controller -n kube-system -o yaml || true
          
          # ë¡œê·¸ ì¡°íšŒ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²•)
          echo ""
          echo "Controller Pod ë¡œê·¸ ì¡°íšŒ ì‹œë„:"
          
          # ë°©ë²• 1: ê°„ë‹¨í•œ ë¡œê·¸ ì¡°íšŒ
          if kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50 --since=10m 2>/dev/null; then
            echo "âœ… ë¡œê·¸ ì¡°íšŒ ì„±ê³µ"
          else
            echo "âš ï¸ ë°©ë²• 1 ì‹¤íŒ¨, ë‹¤ë¥¸ ë°©ë²• ì‹œë„ ì¤‘..."
            
            # ë°©ë²• 2: Pod ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¡°íšŒ
            POD_NAME=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$POD_NAME" ]; then
              echo "Pod ì´ë¦„: $POD_NAME"
              if kubectl logs -n kube-system "$POD_NAME" --tail=50 --since=10m 2>/dev/null; then
                echo "âœ… Podë³„ ë¡œê·¸ ì¡°íšŒ ì„±ê³µ"
              else
                echo "âš ï¸ Podë³„ ë¡œê·¸ ì¡°íšŒë„ ì‹¤íŒ¨"
                
                # ë°©ë²• 3: Pod ìƒì„¸ ì •ë³´
                echo "Pod ìƒì„¸ ì •ë³´:"
                kubectl describe pod -n kube-system "$POD_NAME" 2>/dev/null || echo "Pod ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨"
              fi
            else
              echo "âŒ Controller Podë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            fi
          fi
          
          # ValidatingAdmissionWebhook ìƒì„¸ í™•ì¸
          echo ""
          echo "ValidatingAdmissionWebhook ì§„ë‹¨:"
          if kubectl get validatingadmissionwebhook 2>/dev/null | grep aws-load-balancer; then
            kubectl describe validatingadmissionwebhook aws-load-balancer-webhook 2>/dev/null || echo "ì›¹í›… ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨"
          else
            echo "âŒ ValidatingAdmissionWebhookì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            echo "ì‚¬ìš© ê°€ëŠ¥í•œ ì›¹í›… ëª©ë¡:"
            kubectl get validatingadmissionwebhook 2>/dev/null || echo "ì›¹í›… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨"
          fi
          
          exit 1
        fi
        
        # ì›¹í›… ì¸ì¦ì„œ ìƒì„± ë° ìƒíƒœ í™•ì¸ ê°•í™”
        echo "ğŸ” ì›¹í›… ì¸ì¦ì„œ ìƒì„± ë° ìƒíƒœ í™•ì¸ ì¤‘..."
        
        # cert-managerê°€ ìƒì„±í•˜ëŠ” ë‹¤ì–‘í•œ ì¸ì¦ì„œ íŒ¨í„´ í™•ì¸
        TLS_SECRET_PATTERNS=(
          "aws-load-balancer-webhook-tls"
          "aws-load-balancer-webhook-ca"
          "aws-load-balancer-controller-webhook-tls"
        )
        
        FOUND_TLS_SECRET=""
        for TLS_SECRET_NAME in "${TLS_SECRET_PATTERNS[@]}"; do
          if kubectl get secret -n kube-system "$TLS_SECRET_NAME" >/dev/null 2>&1; then
            echo "âœ… TLS ì¸ì¦ì„œ ë°œê²¬: $TLS_SECRET_NAME"
            FOUND_TLS_SECRET="$TLS_SECRET_NAME"
            
            # ì¸ì¦ì„œ ìƒì„¸ ì •ë³´ í™•ì¸
            echo "ì¸ì¦ì„œ ìƒì„¸ ì •ë³´:"
            kubectl get secret -n kube-system "$TLS_SECRET_NAME" -o yaml | head -20
            
            # ì¸ì¦ì„œ ë§Œë£Œì¼ í™•ì¸
            CERT_DATA=$(kubectl get secret -n kube-system "$TLS_SECRET_NAME" -o jsonpath='{.data.tls\.crt}' 2>/dev/null || echo "")
            if [ -n "$CERT_DATA" ]; then
              echo "ì¸ì¦ì„œ ì •ë³´:"
              echo "$CERT_DATA" | base64 -d | openssl x509 -noout -subject -dates 2>/dev/null || echo "ì¸ì¦ì„œ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨"
            fi
            break
          fi
        done
        
        if [ -z "$FOUND_TLS_SECRET" ]; then
          echo "âš ï¸ ì›¹í›… TLS ì¸ì¦ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
          echo ""
          echo "ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ secrets ëª©ë¡:"
          kubectl get secrets -n kube-system | grep -E "(tls|cert|webhook)" || echo "ê´€ë ¨ secrets ì—†ìŒ"
          
          echo ""
          echo "ğŸ” cert-manager ì´ë²¤íŠ¸ í™•ì¸:"
          kubectl get events -n cert-manager --sort-by='.lastTimestamp' | tail -10 || echo "cert-manager ì´ë²¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨"
          
          echo ""
          echo "ğŸ” kube-system ì´ë²¤íŠ¸ í™•ì¸:"
          kubectl get events -n kube-system --sort-by='.lastTimestamp' | grep -i cert | tail -10 || echo "ì¸ì¦ì„œ ê´€ë ¨ ì´ë²¤íŠ¸ ì—†ìŒ"
        fi
        
        # ì›¹í›… ì—°ê²° í…ŒìŠ¤íŠ¸
        echo "ğŸ” ì›¹í›… ì—°ê²° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘..."
        WEBHOOK_PORT=$(kubectl get validatingadmissionwebhook aws-load-balancer-webhook -o jsonpath='{.webhooks[0].clientConfig.service.port}' 2>/dev/null || echo "443")
        if kubectl exec -n kube-system $(kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o jsonpath='{.items[0].metadata.name}') -- sh -c "nc -zv aws-load-balancer-webhook-service.kube-system.svc.cluster.local $WEBHOOK_PORT" 2>/dev/null; then
          echo "âœ… ì›¹í›… í¬íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ"
        else
          echo "âš ï¸ ì›¹í›… í¬íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ì¼ë°˜ì ì¸ í˜„ìƒì¼ ìˆ˜ ìˆìŒ)"
        fi
        
        # Controller ì•ˆì •í™” ëŒ€ê¸°
        echo "â³ Controller ì•ˆì •í™” ëŒ€ê¸° ì¤‘ (30ì´ˆ)..."
        sleep 30
        
        echo "âœ… AWS Load Balancer Controller ì„¤ì¹˜ ì™„ë£Œ"
        kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
        kubectl get service aws-load-balancer-webhook-service -n kube-system
        kubectl get endpoints aws-load-balancer-webhook-service -n kube-system

    # ===============================================
    # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    # ===============================================
    - name: Generate Kubernetes manifests
      run: |
        echo "ğŸ“ Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì¤‘..."
        
        # Namespace ìƒì„±
        cat <<EOF > namespace.yaml
        apiVersion: v1
        kind: Namespace
        metadata:
          name: ${{ env.PROJECT_NAME }}
          labels:
            name: ${{ env.PROJECT_NAME }}
        EOF
        
        # ConfigMap ìƒì„± (í™˜ê²½ë³€ìˆ˜)
        cat <<EOF > configmap.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: ${{ env.PROJECT_NAME }}-config
          namespace: ${{ env.PROJECT_NAME }}
        data:
          DB_HOST: "${{ env.RDS_ENDPOINT }}"
          DB_PORT: "5432"
          DB_NAME: "mydb"
          DB_USER: "dbadmin"
          AWS_REGION: "${{ secrets.AWS_REGION }}"
          AWS_S3_BUCKET: "walb-app-files"
          AWS_S3_REGION: "${{ secrets.AWS_REGION }}"
          STORAGE_TYPE: "s3"
          APP_ENV: "production"
          APP_DEBUG: "false"
          PHP_MEMORY_LIMIT: "256M"
          PHP_MAX_EXECUTION_TIME: "30"
          PHP_TIMEZONE: "Asia/Seoul"
          UPLOAD_MAX_SIZE: "10M"
          SESSION_LIFETIME: "7200"
        EOF
        
        # Secret ìƒì„± (DB íŒ¨ìŠ¤ì›Œë“œ)
        cat <<EOF > secret.yaml
        apiVersion: v1
        kind: Secret
        metadata:
          name: ${{ env.PROJECT_NAME }}-secret
          namespace: ${{ env.PROJECT_NAME }}
        type: Opaque
        data:
          DB_PASSWORD: $(echo -n "${{ secrets.DB_PASSWORD }}" | base64)
        EOF
        
        # Deployment ìƒì„±
        cat <<EOF > deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: ${{ env.PROJECT_NAME }}-app
          namespace: ${{ env.PROJECT_NAME }}
          labels:
            app: ${{ env.PROJECT_NAME }}-app
        spec:
          replicas: 2
          selector:
            matchLabels:
              app: ${{ env.PROJECT_NAME }}-app
          template:
            metadata:
              labels:
                app: ${{ env.PROJECT_NAME }}-app
            spec:
              serviceAccountName: ${{ env.PROJECT_NAME }}-service-account
              containers:
              - name: php-app
                image: ${{ steps.build-image.outputs.image }}
                ports:
                - containerPort: 80
                  name: http
                envFrom:
                - configMapRef:
                    name: ${{ env.PROJECT_NAME }}-config
                - secretRef:
                    name: ${{ env.PROJECT_NAME }}-secret
                livenessProbe:
                  httpGet:
                    path: /healthcheck.php
                    port: 80
                  initialDelaySeconds: 60
                  periodSeconds: 30
                  timeoutSeconds: 10
                readinessProbe:
                  httpGet:
                    path: /healthcheck.php
                    port: 80
                  initialDelaySeconds: 30
                  periodSeconds: 10
                  timeoutSeconds: 5
                resources:
                  requests:
                    memory: "256Mi"
                    cpu: "250m"
                  limits:
                    memory: "512Mi"
                    cpu: "500m"
                securityContext:
                  runAsNonRoot: false
                  allowPrivilegeEscalation: false
                  readOnlyRootFilesystem: false
        EOF
        
        # Service ìƒì„± (ClusterIPë¡œ ìœ ì§€ - Ingressê°€ ì‚¬ìš©)
        cat <<EOF > service.yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: ${{ env.PROJECT_NAME }}-service
          namespace: ${{ env.PROJECT_NAME }}
          labels:
            app: ${{ env.PROJECT_NAME }}-app
        spec:
          type: ClusterIP
          ports:
          - port: 80
            targetPort: 80
            protocol: TCP
            name: http
          selector:
            app: ${{ env.PROJECT_NAME }}-app
        EOF
        
        # VPC ì„œë¸Œë„· ì •ë³´ ì¡°íšŒ
        echo "ğŸ” VPC ì„œë¸Œë„· ì •ë³´ ì¡°íšŒ ì¤‘..."
        PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text)" \
                    "Name=tag:Name,Values=*public*" \
          --query "Subnets[*].SubnetId" --output text | tr '\t' ',')
        
        if [ -z "$PUBLIC_SUBNETS" ]; then
          # íƒœê·¸ ê¸°ë°˜ ì¡°íšŒê°€ ì‹¤íŒ¨í•˜ë©´ EKS ì„œë¸Œë„· ì‚¬ìš©
          PUBLIC_SUBNETS=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.subnetIds" --output text | tr '\t' ',')
        fi
        
        echo "ì‚¬ìš©í•  ì„œë¸Œë„·: $PUBLIC_SUBNETS"
        
        # Ingress ìƒì„± (AWS Load Balancer Controller ì‚¬ìš©) - ê°•í™”ëœ ì„¤ì •
        cat <<EOF > ingress.yaml
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: ${{ env.PROJECT_NAME }}-ingress
          namespace: ${{ env.PROJECT_NAME }}
          annotations:
            # ê¸°ë³¸ ALB ì„¤ì •
            kubernetes.io/ingress.class: alb
            alb.ingress.kubernetes.io/scheme: internet-facing
            alb.ingress.kubernetes.io/target-type: ip
            
            # Health Check ì„¤ì •
            alb.ingress.kubernetes.io/healthcheck-path: /healthcheck.php
            alb.ingress.kubernetes.io/healthcheck-interval-seconds: '30'
            alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
            alb.ingress.kubernetes.io/healthy-threshold-count: '2'
            alb.ingress.kubernetes.io/unhealthy-threshold-count: '3'
            
            # íŠ¸ë˜í”½ ì„¤ì •
            alb.ingress.kubernetes.io/target-group-attributes: stickiness.enabled=false,deregistration_delay.timeout_seconds=60
            
            # íƒœê·¸ ì„¤ì •
            alb.ingress.kubernetes.io/tags: |
              Environment=${{ env.PROJECT_NAME }},
              Project=${{ env.PROJECT_NAME }},
              ManagedBy=Kubernetes
              
            # ë¡œë“œë°¸ëŸ°ì„œ íƒ€ì…
            alb.ingress.kubernetes.io/load-balancer-name: ${{ env.PROJECT_NAME }}-alb
        spec:
          ingressClassName: alb
          rules:
          - http:
              paths:
              - path: /
                pathType: Prefix
                backend:
                  service:
                    name: ${{ env.PROJECT_NAME }}-service
                    port:
                      number: 80
        EOF
        
        # ServiceAccount ìƒì„± (IRSAìš©)
        cat <<EOF > serviceaccount.yaml
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: ${{ env.PROJECT_NAME }}-service-account
          namespace: ${{ env.PROJECT_NAME }}
          annotations:
            eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/walb-app-eks-app-role
        EOF
        
        echo "âœ… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ"
    
    # ===============================================
    # EKSì— ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ (main ë¸Œëœì¹˜ì¼ ë•Œë§Œ)
    # ===============================================
    - name: Deploy to EKS
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ğŸš€ EKSì— ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì¤‘..."
        
        # Namespace ë¨¼ì € ìƒì„±
        kubectl apply -f namespace.yaml
        
        # ë‚˜ë¨¸ì§€ ë¦¬ì†ŒìŠ¤ ë°°í¬
        kubectl apply -f serviceaccount.yaml
        kubectl apply -f configmap.yaml
        kubectl apply -f secret.yaml
        kubectl apply -f deployment.yaml
        kubectl apply -f service.yaml
        
        # Ingress ë°°í¬ ì „ ì›¹í›… ë¹ ë¥¸ í™•ì¸
        echo "ğŸ”— Ingress ë°°í¬ ì „ ì›¹í›… ìƒíƒœ í™•ì¸..."
        if ! kubectl get validatingadmissionwebhook aws-load-balancer-webhook >/dev/null 2>&1; then
          echo "âŒ ValidatingAdmissionWebhookì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - Ingress ë°°í¬ ì¤‘ë‹¨"
          kubectl get validatingadmissionwebhook 2>/dev/null || echo "ì›¹í›… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨"
          exit 1
        fi
        echo "âœ… ValidatingAdmissionWebhook í™•ì¸ë¨"
        
        # Ingress ë°°í¬ (ê°„ì†Œí™”ëœ ì¬ì‹œë„ ë¡œì§)
        echo "ğŸ”— Ingress ë¦¬ì†ŒìŠ¤ ë°°í¬ ì¤‘..."
        for i in {1..5}; do
          echo "Ingress ë°°í¬ ì‹œë„ ($i/5)..."
          
          if timeout 60s kubectl apply -f ingress.yaml; then
            echo "âœ… Ingress ë°°í¬ ì„±ê³µ"
            # ê°„ë‹¨í•œ ì¡´ì¬ í™•ì¸
            sleep 5
            if kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
              echo "âœ… Ingress ë¦¬ì†ŒìŠ¤ í™•ì¸ ì™„ë£Œ"
              break
            fi
          else
            echo "âš ï¸ Ingress ë°°í¬ ì‹¤íŒ¨ ($i/5)"
            if [ $i -lt 5 ]; then
              echo "30ì´ˆ í›„ ì¬ì‹œë„..."
              sleep 30
            else
              echo "âŒ Ingress ë°°í¬ ìµœì¢… ì‹¤íŒ¨"
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || true
              kubectl describe validatingadmissionwebhook aws-load-balancer-webhook || true
              exit 1
            fi
          fi
        done
        
        echo "â³ ë°°í¬ ì™„ë£Œ ëŒ€ê¸° ì¤‘..."
        kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s
    
    # ===============================================
    # ë°°í¬ ê²°ê³¼ í™•ì¸
    # ===============================================
    - name: Verify Deployment
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ğŸ” ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘..."
        echo "ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ${{ env.PROJECT_NAME }}"
        echo "ì•± ë¼ë²¨: ${{ env.PROJECT_NAME }}-app"
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¡´ì¬ í™•ì¸
        if ! kubectl get namespace ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
          echo "âŒ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '${{ env.PROJECT_NAME }}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
          kubectl get namespaces
          exit 1
        fi
        
        # ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
        echo "ğŸ“‹ Pod ìƒíƒœ:"
        kubectl get pods -n ${{ env.PROJECT_NAME }} -o wide || echo "Podë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        echo "ğŸ“‹ Service ìƒíƒœ:"
        kubectl get services -n ${{ env.PROJECT_NAME }} || echo "Serviceë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        echo "ğŸ“‹ Deployment ìƒíƒœ:"
        kubectl get deployments -n ${{ env.PROJECT_NAME }} || echo "Deploymentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        echo "ğŸ“‹ Ingress ìƒíƒœ:"
        kubectl get ingress -n ${{ env.PROJECT_NAME }} || echo "Ingressë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        echo "ğŸ“‹ AWS Load Balancer Controller ìƒíƒœ:"
        kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller || echo "AWS Load Balancer Controllerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # Deployment ì¡´ì¬ í™•ì¸ í›„ ëŒ€ê¸°
        if kubectl get deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} >/dev/null 2>&1; then
          echo "â³ Pod ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° ì¤‘..."
          kubectl wait --for=condition=ready pod -l app=${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || echo "Pod ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼"
          
          echo "ğŸ“ Deployment ìƒì„¸ ì •ë³´:"
          kubectl describe deployment ${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }}
          
        else
          echo "âŒ Deployment '${{ env.PROJECT_NAME }}-app'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "ì‚¬ìš© ê°€ëŠ¥í•œ Deployment ëª©ë¡:"
          kubectl get deployments -n ${{ env.PROJECT_NAME }}
          exit 1
        fi
    
    # ===============================================
    # LoadBalancer ì„¤ì • í™•ì¸ ë° ì ‘ì† URL ì œê³µ
    # ===============================================
    - name: Get Application URL
      if: success() && github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ğŸ”— ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† ì •ë³´ í™•ì¸ ì¤‘..."
        
        # Ingressì—ì„œ ALB DNS í™•ì¸ (ìµœëŒ€ 3ë¶„ ëŒ€ê¸°)
        echo "â³ Ingress ALB ìƒì„± ëŒ€ê¸° ì¤‘..."
        for i in {1..18}; do
          ALB_DNS=$(kubectl get ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$ALB_DNS" ]; then
            echo "âœ… ALB DNS í™•ì¸: $ALB_DNS"
            break
          fi
          echo "ëŒ€ê¸° ì¤‘... ($i/18)"
          sleep 10
        done
        
        if [ -n "$ALB_DNS" ]; then
          echo "ğŸŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† URL: http://$ALB_DNS"
          echo "ğŸ” Ingress ìƒíƒœ:"
          kubectl describe ingress ${{ env.PROJECT_NAME }}-ingress -n ${{ env.PROJECT_NAME }}
        else
          echo "âš ï¸ Ingress ALB DNSë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
          echo "Ingress ìƒíƒœ í™•ì¸:"
          kubectl get ingress -n ${{ env.PROJECT_NAME }}
          echo "ëŒ€ì²´ ì ‘ê·¼ ë°©ë²•:"
          echo "kubectl port-forward -n ${{ env.PROJECT_NAME }} svc/${{ env.PROJECT_NAME }}-service 8080:80"
        fi
    
    # ===============================================
    # ë°°í¬ ì™„ë£Œ ì•Œë¦¼
    # ===============================================
    - name: Application Deployment Notification
      if: success() && github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ğŸ‰ PHP ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ì™„ë£Œ!"
        echo "í”„ë¡œì íŠ¸: ${{ env.PROJECT_NAME }}"
        echo "ì´ë¯¸ì§€: ${{ steps.build-image.outputs.image }}"
        echo "í´ëŸ¬ìŠ¤í„°: ${{ env.EKS_CLUSTER_NAME }}"
        echo "ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ${{ env.PROJECT_NAME }}"
        echo "ë°ì´í„°ë² ì´ìŠ¤: ${{ env.RDS_ENDPOINT }}"
        echo "ì»¤ë°‹: ${{ github.sha }}"
        echo "ë°°í¬ ì‹œê°„: $(date)"
        
        # ë°°í¬ëœ ì„œë¹„ìŠ¤ ì •ë³´ ì¶œë ¥
        echo "ğŸ“‹ ë°°í¬ëœ ë¦¬ì†ŒìŠ¤ ëª©ë¡:"
        kubectl get all -n ${{ env.PROJECT_NAME }} || echo "ë¦¬ì†ŒìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"

    # ===============================================
    # ë°°í¬ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
    # ===============================================
    - name: Rollback on failure
      if: failure() && github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "âŒ ë°°í¬ ì‹¤íŒ¨ - ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°± ì¤‘..."
        kubectl rollout undo deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} || true
        kubectl rollout status deployment/${{ env.PROJECT_NAME }}-app -n ${{ env.PROJECT_NAME }} --timeout=300s || true
        echo "âœ… ë¡¤ë°± ì‹œë„ ì™„ë£Œ"